---
title: "Matching and Subclassification"
linktitle: "Matching and Subclassification"
date: "2022-11-23"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Toolbox
    weight: 7
type: docs
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = "center", fig.retina = 3, out.width = "75%")
set.seed(11)
options("digits" = 2, "width" = 150)
options(dplyr.summarise.inform = FALSE)

# custom ggplot theme
# colors from TUHH brand identitiy
tuhh_colors <- c("#D0D0CE", "#00C1D4", "#FF4F4F", "#5AFFC5",
                 "#FFDE36", "#143BFF", "#FF7E15", "#FFAEA2")

# initialise theme
cds_theme <- ggthemr::define_palette(
  swatch = tuhh_colors,
  gradient = c(lower = "#FFAEA2", upper = "#00C1D4"),
  background = "#0F2231",
  line = c("#FFFFFF", "#FFFFFF"),
  text = c("#FFFFFF", "#FFFFFF"),
  gridline = c(ggplot2::alpha("#D0D0CE", 0.2), 
               ggplot2::alpha("#D0D0CE", 0.4))
)

# set theme
ggthemr::ggthemr(cds_theme, type = "outer")

# source custom DAG theme
source("../../code/dag_theme.R")
```

# Problem

Almost always, the problem we are trying to solve in causal inference relates to the **fundamental problem of causal inference**, the fact that we cannot observe two states for one particular observation unit, e.g. we cannot see how a person's health status changes after taking a specific drug and after not taking the same drug. Consequently, we cannot know the individual treatment effect.

Thus, we said we can make use of averages and try to estimate the average treatment effect by taking the difference between a group of treated observation units and a group of untreated observation units. Written in a formula, we can present it as

$$
ATE = E[Y_1| D = 1] - E[Y_0 | D = 0] 
$$

Assuming that individuals (or other kinds of observation units) were randomly assigned for example to take the drug, then this result gives us exactly what we want. It compares the outcomes of groups under treatment with groups not under treatment (control).

But the estimate hinges on the assumption that both groups are comparable, formally

$$
E[Y_0|D=0] = E[Y_1|D=0]
$$

which we cannot test but in randomized settings we have good reason to believe it to be true. Under these circumstances, average treatment effect ($ATE$) and the average treatment effect on the treated ($ATT$) are equal.

However, **if there are underlying group differences** because the treatment assignment was not randomized and e.g. individuals were able to choose their treatment, **we are not measuring the estimate that we are interested in**. Then, the difference between $ATE$ and $ATT$ is what we call *selection bias*, an unmeasured factor representing systematic bias:

$$
ATE = ATT + \text{selection bias}
$$

Graphically, we can show an example of when the naive estimate would fail.

```{r}
#| echo: false
#| message: false
#| fig-cap: "Effect of D on Y is confounded by variation in Z. In other words, there are two paths from D to Y, one direct path and a backdoor path via Z."

library(tidyverse)
library(ggdag)
library(dagitty)

confounder <- dagify(
  D ~ Z,
  Y ~ Z,
  Y ~ D,
  coords = list(x = c(Y = 3, Z = 2, D = 1),
                y = c(Y = 0, Z = 1, D = 0))
)

ggdag(confounder) +
  theme_dag_cds() +
  geom_dag_point(color = ggthemr::swatch()[2]) +
  geom_dag_text(color = "white") +
  geom_dag_edges(edge_color = "white")
```

By now, you might recognize what kind of problem the DAG depicts: confounding. A variable $Z$ confounds the relationship between $X$ and $Y$ and to estimate the causal effect of $X$ on $Y$, we need to close the backdoor path of $X$ to $Y$ via $Z$.

# Idea of Matching

One of the options to close the backdoor is **matching**, which covers any method **attempting to equate or balance the distribution of covariates in treatment and control group**. Simply put, the **goal of matching is to compare apple to apples** and after treatment make treatment and control group as similar as possible (of course, except for the treatment value).

In a way, matching is an alternative to using regression to close backdoors and neither is better or worse, in fact, they can even be combined. But for now, let us focus on matching and understand what it really it is, what kind of matching methods are popular and how they can be applied in `R`.

Let us assume you would like to study a phenomena and only have observational data that looks like this. $Z$ (education) is a covariate, $Y$ is the outcome and the color of the data points shows if a unit has been treated or not. From the first glance you can already see that the data does not look as if you would expect in a randomized experiment. For values of $Z$ in the middle range, there are both treated and untreated cases, but for values at the lower and upper range there are only untreated cases. That is an indication that $Z$ confounds the relationship between $D$ and $Y$.

```{r}
#| echo: false

# Observations both treated and control (mid education)
matched_stuff <- tibble(education = rnorm(50, 20, 3)) %>% 
  mutate(outcome = 15 + education * rnorm(n(), -0.2, 0.05), 
         treatment = as.logical(rbinom(n(), 1, 0.5)),
         type = "Matched")
# Observations control (low education)
unmatched_stuff_low <- tibble(education = rnorm(20, 12, 2),
                          outcome = rnorm(20, 5, 2),
                          treatment = FALSE, 
                          type = "Unmatched")
# Observations control (high education)
unmatched_stuff_high <- tibble(education = rnorm(5, 28, 1),
                               outcome = rnorm(5, 5, 0.5),
                               treatment = FALSE,
                               type = "Unmatched")
# All observations
all_data <- bind_rows(matched_stuff, 
                      unmatched_stuff_low,
                      unmatched_stuff_high) %>%
  mutate(treatment = factor(treatment, 
                            labels = c("Untreated", "Treated")))
# Part of data that has matches (with factor treatment)
matched_stuff_real <- filter(all_data, type == "Matched")
# Wrong model
model_wrong <-
  lm(outcome ~ education + treatment, data = all_data) %>% 
  broom::tidy()
# Wrong model with square term
model_wrong1 <- lm(outcome ~ education + treatment + I(education^2), 
                   data = all_data)
model_wrong1_fitted <-
  expand_grid(education = seq(8, 30, 0.1),
              treatment = c("Treated", "Untreated")) %>% 
  broom::augment(model_wrong1, newdata = .)
# Better model
model_better <- lm(outcome ~ education + treatment, data = matched_stuff_real) %>% 
  broom::tidy()
# Better model with square term
model_better1 <- lm(outcome ~ education + treatment + I(education^2), data = matched_stuff_real)
model_better1_fitted <- expand_grid(education = seq(8, 30, 0.1),
                                    treatment = c("Treated", "Untreated")) %>% 
  broom::augment(model_better1, newdata = .)
```

Just imagine your measuring an arbitrary outcome and an arbitrary treatment confounded by years of education. When we plot the initial situation we see that for a low level of education and for a high level of education there are no treated observations. In causal inference lingo, treatment assignment is not independent from other factors, it is endogenous.

```{r}
#| echo: false

# Plot all data points
ggplot(all_data, aes(x = education, y = outcome, color = treatment)) +
  geom_point(size = 5, pch = 19, alpha = .8) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

When we plot a line through all blue and red points, respectively and check the difference between the two lines, we see that there is substantial difference between both groups. This difference is what we would get when we run a regression on all points. It indicates a positive treatment effect.

```{r}
#| echo: false

# Plot regression lines for both treatment and control (all data)
ggplot(all_data, aes(x = education, y = outcome, color = treatment)) +
  geom_point(size = 5, pch = 19, alpha = .8) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate,
              color = ggthemr::swatch()[2], linewidth = 0.75) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate +
                filter(model_wrong, term == "treatmentTreated")$estimate,
              color = ggthemr::swatch()[3], linewidth = 0.75) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

However, the lines clearly don't fit the data very well because there does not seem to be a linear relationship. To account for non-linearities, you can introduce e.g. square terms to the regression, here $Z^2$ or $education^2$, respectively. Then, the fit improves, in particular for the untreated data points. But what happened to the treatment effect? Now there is negative treatment effect smaller in magnitude than the previous positive treatment effect. We already see that the estimate is highly dependent on the choice of our particular model. This is why we should always put a lot of consideration into choosing the right model (which is not necessarily the one with the largest or most desired treatment effect).

```{r}
#| echo: false

# Plot regression lines for linear and square (all data)
ggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +
  geom_point(size = 5, pch = 19, alpha = .8) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate,
              color = ggthemr::swatch()[2], linewidth = 0.75) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate +
                filter(model_wrong, term == "treatmentTreated")$estimate,
              color = ggthemr::swatch()[3], linewidth = 0.75) +
  geom_line(data = model_wrong1_fitted, 
            aes(x = education, y = .fitted, color = treatment),
            linewidth = 0.75, show.legend = F) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

But we still have not really dealt with the fact that there are some regions where there are only units from one group. So let's think about what we would have to do to adjust for the confounder $Z$. Ideally, we would like to compare treated and control group in an area where both groups are present. So the lower and upper region should actually be left out. Only where there is overlap of both groups, we can compare apples with apples. Focusing only on this (somewhat arbitrary) data points and drawing the lines and curves for both specifications of the regression, we get another different result. Now it seems, that there is no effect at all.

```{r}
#| echo: false

# Plot regression line (matched data)
ggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +
  geom_point(aes(alpha = type), size = 5, pch = 21, color = "#0F2231") +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate,
              color = ggthemr::swatch()[2], linewidth = 0.75) +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate +
                filter(model_better, term == "treatmentTreated")$estimate,
              color = ggthemr::swatch()[3], linewidth = 0.75) +
  scale_alpha_manual(values = c(1, 0.2), guide = "none") +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

So, how do we know what units to select, especially when there is more than one dimension? We will explore some techniques in the following sections but the short answer is: we need to create a balance for vales of all confounding variables treatment and control group, i.e. treatment and control group should be as similar as possible. And of course, we also need to assume, that all confounding variables are observed.

# Subclassification Estimator

Although not often used in practice, the subclassification estimator is an intuitive way of thinking about matching comparable observations and thereby controlling for confounders.

Essentially, the subclassification estimator **estimates local treatment effects withing small groups that share the same values for (confounding) covariates**. Sometimes, values does not need to be the same but only similar, e.g. for continuous variables like age it is very unlikely to find another observation unit with the exact same birthday, so you would define e.g. a birthyear. By only comparing observations in a small subgroup as defined by the covariates, conditional independence is ensured as there is no variation in the confounding variable.

Imagine, that a treatment was not randomly assigned for women and men and sex also plays a role for the outcome as well. One imaginary example could be the effect of a trainee program on later salary that was advertised more heavily to women and therefore more women participated. And being a women might also have an impact on salary. Then, we have the situation of a confounding variable, sex, and to control for it, we estimate a treatment effect for women and a treatment effect for men and average the effects weighted by the respective subsample size.

To check how that affects the estimated treatment effect we can simulate some data according to the following relationships.

```{r}
# Number of observations
n <- 1e+3

# Variables
Z <- rbernoulli(n, 0.5) # 0 => Male, 1 => Female
D <- rbernoulli(n, p = if_else(Z, 0.65, 0.35)) # 1 => Treat, 0 => Control
Y <- 0.2*D - 0.2*Z + rnorm(n, 5, 0.1) # => Salary

# Create tibble
df <- tibble(
  Z = Z,
  D = D,
  Y = Y
)
```

$Z$ (= sex) is the confounder, $D$ (= participation in program) is the treatment and $Y$ (salary) is the outcome. As you can see both $D$ and $Y$ are related to $Z$ and $Y$ is influenced by both $D$ and $Z$. But we are only interested in the effect of $D$ on $Z$ and want to isolate it. From the commands, we know that the true causal effect of $D$ on $Y$ is 0.2.

Let's see what a naive comparison would return, where we would just compare average salary of the individuals who have participated in the trainee program are compared to the individuals who have not participated.

::: callout-note
## Accessing with base `R`

To access only certain elements of a table you can use `table[row,col]`. You can also use the `$` operator to access columns.
:::

```{r}
# Naive comparison
E_0 <- mean(df[df$D==F, ]$Y) # control group
E_1 <- mean(df[df$D==T, ]$Y) # treatment group
E_1 - E_0
```

A simpler way to compute it would be:

```{r}
# Regression of Y on D
summary(lm(Y ~ D, df))
```

Due to confounding, the naive comparison cannot reconstruct our implemented treatment effect (defined when we simulated the outcome variable) and is off. We should get closer if we account for confounding by taking averages treatment effects for men and women separately.

```{r}
# Subclassification estimator (subclasses: Z = 0 and Z = 1)
# E(Z, D)
E_00 <- mean(df[(df$Z==F & df$D==F), ]$Y) # control men
E_10 <- mean(df[(df$Z==T & df$D==F), ]$Y) # control women
E_01 <- mean(df[(df$Z==F & df$D==T), ]$Y) # treatment men
E_11 <- mean(df[(df$Z==T & df$D==T), ]$Y) # treatment women

# Weighted by K (proportion of female/male)
K <- mean(Z)

K*(E_11-E_10) + (1-K)*(E_01 - E_00)
```

And in fact, now we are really close to the true treatment effect. The remaining difference is caused by randomness in our sampling process.

This way, we have closed the backdoor and are able to retrieve the causal effect. However, note that the effect can only be causally interpreted, if sex is the only confounding variable. If there are more confounders, we have to build smaller groups that are defined by value combinations of all confounders, which will at some point lead groups that are extremely small or maybe even empty. This is called the curse of dimensionality and we'll deal with it in the following sections.

# Matching as a Concept

In the previous section, observations from treatment and control group were required to have equal covariates. But matching can also take other forms that are less strict. Generally, we are trying to select observations in the control group that are similar to those in the treated group. To do so, we need to define what similar means. By enforcing treatment and control group to have little variation in the matching variable, we close the backdoor. When the backdoor variable does not vary or varies only very little, it cannot induce changes in treatment and outcome. So, when we suppress this variation in the backdoor variable, we can interpret the effect from treatment to outcome as causal.

There are many ways and methods to conduct this and we will go through some of them. But let's first define matching conceptually.

In general, the **outcome of a matching** method are **weights for each observation** **based on one or several matching variables**. For some matching methods the weights are either 0 or 1 (in or out), but there are also a lot of methods that give weights between 0 and 1 (less and more important) to observations.

The **matching variables need to cover** the variables that can - when adjusted on - block **all backdoor paths** between our treatment variable and the outcome. Then, the treatment effect is computed as a weighted mean of the outcomes for the treatment and control group.

Matching methods can be classified into [two main approaches]{.underline}: matching **based on (1) distance** and matching **based on (2) propensity scores**.

1.  **Distance**: observations are similar if they have similar covariates (or more specific: similar values for the matching variables). This approach minimizes the distance between observations in treatment and control group based on their covariate values.

2.  **Propensity**: observations are similar if they are equally likely to be treated, i.e. matching variables are used to compute how likely an observation is to be in treatment group, regardless whether it actually is in the treatment or control group.

As you can also see in the plot, another difference across methods is whether observations are compared to matches, which classify other observations as either in our out or compared to matched weighted samples, where each observation obtains a different weight depending on how close they are. For matches, you have to decide how many to include or what the worst acceptable match is while for matched weighted samples you need to decide how weights decay with distance.

```{r}
#| echo: false
#| layout-nrow: 1
#| fig-asp: 1.1
#| fig-cap: 
#|   - "Initial situation: one treated observation and five potential matches from the control group."
#|   - "The closest match is selected to compute treatment effect."
#|   - "Several units are selected and weighted by their distance."

# Create tibble of potential matches with two variables, in/out and weight
df <- tibble(
x1 = c(1, 2, 3, 3, 3.2),
x2 = c(1.5, 2.2, 3.1, 2.7, 2.5),
m  = as.factor(c(0, 1, 0, 0, 0))
)

# Create focal point
treated <- tibble(
  x1 = 2.35,
  x2 = 2.2
)

# Initial situation
p1 <- ggplot(df, aes(x = x1, y = x2)) +
  geom_point(size = 4) +
    geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill= ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 4),
                  ylim = c(0, 4)) +
  theme(legend.position = "none") +
  labs(title = "Initial situation")

# In/Out matching
p2 <- ggplot(filter(df, m==1), aes(x = x1, y = x2)) +
  geom_point(data = filter(df, m == 0), color = "grey", alpha = .42, size = 4) +
  geom_point(size = 7, color = ggthemr::swatch()[4]) +
    geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill = ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 4),
                  ylim = c(0, 4)) +
  theme(legend.position = "none") +
  labs(title = "In or Out")

# Compute distance based on Euclidean distance
euclidean <- function(a, b) sqrt(sum((a - b)^2))
distance <- lapply(1:nrow(df), function(row) {
  vec_df <- c(df$x1[row], df$x2[row])
  vec_tr <- as_vector(treated)
  
  euclidean(vec_df, vec_tr)
}) %>% unlist()
df$w <- 1/distance

# Weighted by distance
p3 <- ggplot(df, aes(x = x1, y = x2, size = w)) +
  geom_point(color = ggthemr::swatch()[4]) +
    geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill= ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 4),
                  ylim = c(0, 4)) +
  theme(legend.position = "none") +
  labs(title = "Weighted by distance")

# plot all
p1
p2
p3
```

We can now outline a structure for the [**general process of matching**]{.underline} that applies to all methods.

1.  First, it starts with some kind of preprocessing, which is a "manual" part, where you use all information about the causal mechanisms that you have. Essentially, it forces you to look at the DAG you can draw based on all your theoretical knowledge and thoughts, and particularly what you know about the treatment assignment. In observational studies, this will leave you with a sample that is different from your raw sample. In randomized studies, it might be the same as you do not have to for example close a backdoor path.
2.  Then, using this preprocessed data you can build your model to estimate the treatment effect.

Now, let's look at some methods in detail and afterwards get more application-oriented.

## Single Matching Variable

Probably the most simple method, though rarely applied in practice, is matching on a single variable. Its rare use in practice is due to the fact that matching on a single variable is only applicable if there is only one backdoor path that can be closed by this matching variable.

But for ease of explanation, we'll have a look at it.

```{r}
#| echo: false
#| fig-cap: "Generic form of confounder and example. By fixing the level of confounder, backdoor is closed and valid estimation of causal treatment effect for that specific group at level of confounder. Example scenario: Effect of being late on settling credict card debt in the past on being late again. Confounded by size of credit card debt. It could be suspected that customers with higher debts in the past are unlikely to be financially recovered by now."
#| fig-subcap: 
#|   - "Generic DAG with matching"
#|   - "Applied to credit card example"
#| layout-ncol: 2

# Generic confounder
confounder <- dagify(
  D ~ Z,
  Y ~ Z,
  Y ~ D,
  coords = list(x = c(Y = 3, Z = 2, D = 1),
                y = c(Y = 0, Z = 1, D = 0))
)

ggdag(confounder) +
  annotate("text", x = 1.5, y = 1, label = "Z fixed at constant level") +
  theme_dag_cds() +
  geom_dag_point(color = ggthemr::swatch()[2]) +
  geom_dag_text(color = "white") +
  geom_dag_edges(edge_color = "white")

# Example confounder
example_confounder <- dagify(
  D ~ Z,
  Y ~ Z,
  Y ~ D,
  coords = list(x = c(Y = 3, Z = 2, D = 1),
                y = c(Y = 0, Z = 1, D = 0)),
  labels = list(D = "Being late previously", Y = "Being late next month", Z = "")
)

ggdag(example_confounder, use_labels = "label") +
  annotate("text", x = 1.5, y = 1, label = "Size of previous bill\n equal to 5000$") +
  theme_dag_cds() +
  geom_dag_point(color = ggthemr::swatch()[2]) +
  geom_dag_text(color = "white") +
  geom_dag_edges(edge_color = "white")
```

## (Coarsened) Exact Matching

When you think of the previous credit card example, two things could come up in your mind:

-   How likely is it that when fixing a continuous variable such as previous bill, we can find matching treatment and control observations?

-   How do we control for more than one variable?

To address the first issue we can coarsen continuous variables. That means, we create different levels/bins for the size of the previous bill, which could be for example: 1-50€, 51-250€, 251-1000€ and so on. This increases the likelihood of finding matches substantially. However, we also lose a bit of precision. In general: the wider the bins, the more matches but the less precision.

If there is more than one backdoor, we can also match on more than one variable. Matched observations need to correspond on all variables that are selected as matching variables.

Especially if matching on a single variable, there might be more than one match for a particular observation. In this case, you would weight the matches. On the other hand, especially if trying to find (coarsened) exact matches on multiple variables, some observations might not be matched at all. That's a situation where nearest-neighbor matching comes in handy.

## Nearest-Neighbor Matching

It is less strict than exact matching as it does not require observations to match exactly in all (coarsened) variables. Instead, the distance between observations is computed using distance measures such as the distance or the Euclidean distance.

```{r}
#| echo: false
#| layout-nrow: 1
#| fig-asp: 1.1
#| fig-cap: 
#|   - "Initial situation: six potential matches."
#|   - "Four best matches are selected."
#|   - "Matches are only accepted if they fall into specified vicinity."
#| message: false
#| warning: false

# Create tibble of potential matches with two variables, in/out and weight
df <- tibble(
x1 = c(0.5, 2.2, 2, 4.5, 4.2, 1),
x2 = c(3.4, 3, 4.5, 2.1, 0.8, 1),
m  = as.factor(c(1, 1, 1, 1, 0, 0))
)

# Create focal point
treated <- tibble(
  x1 = 2.2,
  x2 = 4.1
)

# Initial situation
p1 <- ggplot(df, aes(x = x1, y = x2)) +
  geom_point(size = 4) +
    geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill= ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 6),
                  ylim = c(0, 6)) +
  theme(legend.position = "none") +
  labs(title = "Initial situation")

# In/Out matching
p2 <- ggplot(filter(df, m==1), aes(x = x1, y = x2)) +
  geom_point(data = filter(df, m == 0), color = "grey", alpha = .42, size = 4) +
  geom_point(size = 7, color = ggthemr::swatch()[3]) +
    geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill = ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 6),
                  ylim = c(0, 6)) +
  theme(legend.position = "none") +
  labs(title = "Nearest-Neighbor: k = 4") 

# Compute distance based on Euclidean distance
euclidean <- function(a, b) sqrt(sum((a - b)^2))
distance <- lapply(1:nrow(df), function(row) {
  vec_df <- c(df$x1[row], df$x2[row])
  vec_tr <- as_vector(treated)
  
  euclidean(vec_df, vec_tr)
}) %>% unlist()
df$w <- 1/distance

# Weighted by distance
p3 <- ggplot(df, aes(x = x1, y = x2, size = w, color = m)) +
  geom_point(color = c(rep(ggthemr::swatch()[3], 3), rep("grey", 3))) +
  geom_point(aes(x = treated$x1, treated$x2),
             pch = 23, size = 7, color = "white", fill= ggthemr::swatch()[5]) +
  coord_cartesian(xlim = c(0, 6),
                  ylim = c(0, 6)) +
  ggforce::geom_circle(aes(x0 = 2.2, y0 = 4.1, r = 1.85),
                       color = alpha("white", 0.7), linetype = "solid", lwd = 0.4) + 
  theme(legend.position = "none") +
  labs(title = "Using kernel weight")

# plot all
p1
p2
p3
```

For two observations, a scalar is returned calculated from the distances of all matching variables. Based on that value, $k$ matching observations are selected to build the control group.

If $k \neq 1$, you also have to decide whether to weight the control observations on their distance. Weighting the observations makes the approach less sensitive to the choice of $k$ because less importance observations are down-weighted. Eventually the weights goes to zero.

## Propensity score

With an increasing number of matching variables nearest-neighbors matching becomes unfeasible, as well. To reduce the dimensionality onto one dimension, for each observation a propensity score can be computed. The propensity score expresses the estimated probability of treatment. In absence of selection bias, propensity scores should be very similar across treatment and control group. Thinking back to our probability chapter, the propensity score is:

$$
P(D_i = 1|X_i)
$$

We'll discuss how to use it later in detail but essentially, we exploit that there are units in the treatment group that were unlikely to be treated and vice versa. The most recommended method that uses propensity scores as matching input is called **inverse probability weighting** and weights each observation is weighted by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight.

# Application

## Multiple Matching Variables

Let us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.

Now you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age. Let's assume for demonstration purposes that these are the only confounding factors. In practice, there might be more, however.

We can use a DAG to think about the correct identification strategy. Using `dagitty` and `ggdag` we see that we need to close two backdoor paths: initial health status and age.

```{r}
#| fig-cap: "DAG showing what needs to be accounted for."

# Load packages
library(dagitty)
library(ggdag)

# Define DAG
dag_model <- 'dag {
  bb="0,0,1,1"
  "Health Program" [exposure,pos="0.25,0.2"]
  "Initial Health Status" [pos="0.35,0.25"]
  "Sick Days" [outcome,pos="0.35,0.2"]
  Age [pos="0.25,0.25"]
  "Initial Health Status" -> "Health Program"
  "Initial Health Status" -> "Sick Days"
  Age -> "Health Program"
  Age -> "Initial Health Status"
  Age -> "Sick Days"
}'

# DAG with adjustment sets (and custom layout)
ggdag_adjustment_set(dag_model, shadow = T, use_labels = "name", text = F) +
  guides(color = "none") +  # Turn off legend
  theme_dag_cds() +
  geom_dag_point(color = ggthemr::swatch()[2]) +
  geom_dag_text(color = NA) +
  geom_dag_edges(edge_color = "white")
```

```{r}
#| include: false

# [3] Generate synthetic data ----
# Number of observations
n <- 1e+4
# Z_1: age
age <- scales::rescale(
  rbeta(n, shape1 = 3, shape2 = 2), 
  to = c(18, 65))
# Z_2: initial health status (sick days before)
sick_days_before <- round(rexp(n, 25/age) + rnorm(n, 2, 0.5))
# D: participation in health program
d_cont <- 0.04*age + sick_days_before + rnorm(n)
health_program <- if_else(
  d_cont > median(d_cont),
  rbernoulli(n, 0.65),
  rbernoulli(n, 0.35)
  )
mean(health_program)
        
# Y: sick days (after)
sick_days <- round(
  0.04*age + sick_days_before + 0.5*health_program + rnorm(n, 2, 1)
  )
# Add variables to tibble
df <- tibble(
  age              = age,
  sick_days_before = sick_days_before,
  health_program   = health_program,
  sick_days        = sick_days
)

# Save
saveRDS(df, "../../datasets/health_program.rds")
```

Let's load the data (you probably have to change the path) and have a glance at it.

```{r}
#| eval: false

# Read data
df <- readRDS("../../datasets/health_program.rds")

# Show data 
df
```

::: callout-important
## True Treatment Effect

Because the data is simulated, we know the true treatment effect: **0.5**

Compare it to the estimates in the following sections!
:::

A naive estimate would be obtained by just regressing `sick_days` on `health_program`.

```{r}
# Naive estimation (not accounting for backdoors)
model_naive <- lm(sick_days ~ health_program, data = df)
summary(model_naive)
```

But we suspect that treatment assignment was not random, so let's see if we can improve the validity of our estimation using matching.

## (Coarsened) Exact Matching

Again, in case of exact matching, only observations that share the same values in (coarsened) matching variables are matched in. To perform Coarsened Exact Matching (CEM) you can use the `MatchIt` package in R. If you do not specify how to coarsen the data, it will be done automatically based on an algorithm. Other than that, we provide a formula containing our treatment dependent on the matching variables, the data, what method to use (`'cem'` = Coarsened Exact Matching) and what estimate we are interested in.

```{r}
# Load 'MatchIt' library
library(MatchIt)

# Without specifying coarsening
# (1) Matching
cem <- matchit(health_program ~ age + sick_days_before,
               data = df, 
               method = 'cem', 
               estimand = 'ATE')
```

Using the `summary()` function we can check how well balanced the covariates are compared to before. In this case, they are almost perfectly balanced.

```{r}
# Covariate balance
summary(cem)
```

Now, we can use the matched data and see how the coefficient changes. Actually, it changes quite a lot. Even when at first glance, the covariates were not too different before matching.

```{r}
# Use matched data
df_cem <- match.data(cem)

# (2) Estimation
model_cem <- lm(sick_days ~ health_program, data = df_cem, weights = weights)
summary(model_cem)
```

Instead of letting the algorithm decide how to coarsen the data, we can also provide custom cut-points. Let's do that and check if we can create matched data as balanced as by the algorithm.

We'll see that we are able to decrease the imbalance, but not to the same degree as the algorithm did it.

```{r}
# Custom coarsening
# (1) Matching
cutpoints <- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))
cem_coars <- matchit(health_program ~ age + sick_days_before,
                     data = df, 
                     method = 'cem', 
                     estimand = 'ATE',
                     cutpoints = cutpoints)

# Covariate balance
summary(cem_coars)
```

We can also visualize the subsamples and see how data points are weighted. Weights depend on how many treated and control units there are in a specific subsample. You can see that in the top-right corner, for example. From the plot we can also see that the cut-points are too broad as matches could be way closer.

```{r}
#| fig-cap: "Coarsening: each subsample should contain only similar units."

# Use matched data
df_cem_coars <- match.data(cem_coars)

# Plot grid
ggplot(df_cem_coars, aes(x = age, y = sick_days_before,
                         size = weights, color = as.factor(health_program))) +
  geom_point(alpha = .2) +
  geom_abline(data.frame(y = cutpoints$sick_days_before),
              mapping = aes(intercept = y, slope = 0), 
              linewidth = 1.5, color = ggthemr::swatch()[5]) +
  geom_vline(data.frame(y = cutpoints$age),
              mapping = aes(xintercept = y),
             linewidth = 1.5, color = ggthemr::swatch()[5]) +
  theme(legend.position = "none")
```

With custom coarsening, we again get another coefficient. It could indicate that this way, backdoors are not properly closed.

```{r}
# (2) Estimation
model_cem_coars <- lm(sick_days ~ health_program, data = df_cem_coars, 
                      weights = weights)
summary(model_cem_coars)
```

## Nearest-Neighbor matching

For nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular distance measures used to find so called *nearest neighbors* is the Mahalanobis distance.

Again, we use `MatchIt` to conduct the matching process. We just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching. The result differs from (coarsened) exact matching but again, we have almost perfect balance.

```{r}
# (1) Matching
# replace: one-to-one or one-to-many matching
nn <- matchit(health_program ~ age + sick_days_before,
              data = df,
              method = "nearest",
              distance = "mahalanobis",
              replace = T)

# Covariate Balance
summary(nn)
```

And also the estimated average treatment effect is very similar to the one obtained by (default) CEM.

```{r}
# Use matched data
df_nn <- match.data(nn)

# (2) Estimation
model_nn <- lm(sick_days ~ health_program, data = df_nn, weights = weights)
summary(model_nn)
```

::: callout-important
## Curse of dimensionality

With exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don't find any matches at all.

Regarding exact matching, consider for example the situation with two covariates with each five different values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates \~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations.
:::

## Inverse Probability weighting

One way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let's go into more detail.

### Estimating Propensity Score

We start by understanding what **probability** in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as **propensity score**.

Here, we will use a logistic regression for prediction. A logistic regression, opposed to a linear regression, is designed for outcomes that are between 0 and 1, such as probabilities. The coefficients are a bit more difficult to interpret, so we'll leave that for now. But what we see is, that `age` and `sick_days_before` are relevant and positive predictors for the probability of treatment.

```{r}
# (1) Propensity scores
model_prop <- glm(health_program ~ age + sick_days_before,
                  data = df,
                  family = binomial(link = "logit"))
summary(model_prop)
```

For each observation we can compute a probability and add it to a table. It is important to specify `type = "response"` in the `predict()` command to obtain probabilities.

```{r}
# Add propensities to table
df_aug <- df %>% mutate(propensity = predict(model_prop, type = "response"))
```

Having obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as **propensity score matching**. However, there are several reasons why this is not a good identification strategy[^1], mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that same covariate values indeed imply the same propensity score.

[^1]: <https://gking.harvard.edu/publications/why-propensity-scores-should-not-be-used-formatching>

### Weighting by Propensity Score

Instead **inverse probability weighting (IPW)** has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use the propensity score of an observation unit to in- or decrease its weights and thereby make some observations more important than others. The weight obtains as

$$
w_i = \frac{D_i}{\pi_i} + \frac{(1-D_i)}{(1-\pi_i)}
$$

where only one of the terms is always active as $D_i$ is either one or zero. Now we should better understand what "inverse probability weighting" actually means. It weights each observation by its inverse of its treatment probability. Let's compute it for our data.

```{r}
# Extend data by IPW scores
df_ipw <- df_aug %>% mutate(
  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))

# Look at data with IPW scores
df_ipw %>% 
  select(health_program, age, sick_days_before, propensity, ipw)
```

Imagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. $D_i=1$. Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let's assume it was 0.8, for demonstration. Then his/her weight would equal $w_i = \frac{1}{0.8} = 1.25$.

Compared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let's say his/her probability of participating would be 0.2. Then, his/her weight would be $w_i = \frac{1}{0.2} = 5$. So we see, he/she would obtain a significantly higher weight.

In general, IPW upweights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group.

Running a linear regression with weights as provided by IPW yields a coefficient not as precise as the ones obtained by previous matching procedures.

```{r}
# (2) Estimation
model_ipw <- lm(sick_days ~ health_program,
                data = df_ipw, 
                weights = ipw)
summary(model_ipw)
```

A reason for that could be that there are some extreme and very atypical observations with either very high or low probabilities. Those observations than get a very high weight.

```{r}
#| fig-cap: "Most predicted treatment probabilities are between 0.4 and 0.6. Only a few outliers."

# Plot histogram of estimated propensities
ggplot(df_aug, aes(x = propensity)) +
  geom_histogram(alpha = .8, color = "white")
```

```{r}
# Looking for observations with highest weights
df_ipw %>% 
  select(health_program, age, sick_days_before, propensity, ipw) %>% 
  arrange(desc(ipw))
```

A rule of thumb is to filter out all observations with a propensity score less than 0.15 and higher than 0.85. Doing that,the coefficient is very close to the true treatment effect.

```{r}
# Run with high weights excluded
model_ipw_trim <- lm(sick_days ~ health_program,
                data = df_ipw %>% filter(propensity %>% between(0.15, 0.85)),
                weights = ipw)
summary(model_ipw_trim)
```

Opposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the probability model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders.

## Comparison

Comparing the methods, we see that some of them were yield results very close to the true treatment effect. The naive estimation, however, was far off. Also, using custom cut-points (that were too wide) for coarsening also reduces the accuracy.

```{r}
# Summary of naive and matching methods
modelsummary::modelsummary(list("Naive" = model_naive,
                                "CEM1"  = model_cem,
                                "CEM2"  = model_cem_coars,
                                "NN"    = model_nn,
                                "IPW1"  = model_ipw,
                                "IPW2"  = model_ipw_trim))
```

# Assumptions

To obtain valid estimates with any matching estimator, we still need to fulfill some assumptions. Most important are the following:

-   **Conditional Independence**: all backdoors needs to be closed so it has to be ensured that all confounders are in the set of matching variables.

-   **Common Support: for** treated units, there need to exist appropriate control units. Without a substantial overlap in the distribution of matching variables or the propensity score, estimating causal effects is impossible.

Parts of these assumptions can be checked by looking at the balance of covariates, e.g. with `summary()` and the `MatchIt` package or with base `R` functions. If the degree balance is not satisfactory, you can change parameters of your matching method and do a new iteration until you are confident to have a good balance between treatment and control.

Having fulfilled the necessary assumptions allows the valid estimation of a treatment. It is interesting to note that, depending on what method you chose, a different kind of treatment effect is estimated. Most of the methods start with having a treated unit and search for matches (selecting or weighting) across the control units, which will return the average treatment effect on the treated, $ATT$. By the same logic, these methods can also yield the average treatment effect on the untreated, $ATU$.

Then, the average treatment effect can be estimated by a weighted estimate of the group treatment effects.

$$
ATE = p*ATT + (1-p)*ATU
$$

In case of IPW, $ATE$ is computed directly, as it directly changes the weights of units from both groups.

::: callout-important
## Matching vs. Regression

Matching methods are a very active research field and even thinking about the correct way to choose parameters in those methods offers a wide range of questions that there is no definite answer to. To some degree, choosing matching parameters is arbitrary. A benefit, however, is that after matching model dependence is reduced when using simple techniques like weighted means.

But how does it compare to using a simple linear regression? In one of the previous chapters, we said that closing back doors can also be achieved by including confounders into the regression equation. One difference is that when using regression, the $ATE$ points in direction where covariates have more variance, i.e. when there is a higher treatment variance for older people for example, they obtain a higher weight as compared to the matching approach. Another difference is that a regression hinges on the assumption of linearity, which we do not need to assume in matching approaches.
:::

# Assignment

Imagine, the following situation. You are running an online store and you one year ago, you introduced a plus membership to bind customers to your store and increase revenue. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. But of course, there are potentially confounding variables such as `age`, `sex` or `pre_avg_purch` (previous average purchases).

Load the data `membership.rds`. Then,

1.  Check the relationships between the variables and draw a DAG as you understand the relations.
2.  Compute a naive estimate of the average treatment effect.
3.  Use the following matching methods to obtain more precise estimates:
    1.  (Coarsened) Exact Matching.

    2.  Nearest-Neighbor Matching.

    3.  Inverse Probability Weighting.
