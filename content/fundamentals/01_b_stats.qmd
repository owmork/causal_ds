---
title: "Statistical Concepts"
linktitle: "Statistical Concepts"
date: "2022-11-23"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Fundamentals
    weight: 2
type: docs
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = "center", fig.retina = 3, out.width = "75%")
set.seed(11)
options("digits" = 2, "width" = 150)
options(dplyr.summarise.inform = FALSE)

# custom ggplot theme
# colors from TUHH brand identitiy
tuhh_colors <- c("#D0D0CE", "#00C1D4", "#FF4F4F", "#5AFFC5",
                 "#FFDE36", "#143BFF", "#FF7E15", "#FFAEA2")

# initialise theme
cds_theme <- ggthemr::define_palette(
  swatch = tuhh_colors,
  gradient = c(lower = "#FFAEA2", upper = "#00C1D4"),
  background = "#0F2231",
  line = c("#FFFFFF", "#FFFFFF"),
  text = c("#FFFFFF", "#FFFFFF"),
  gridline = c(ggplot2::alpha("#D0D0CE", 0.2), 
               ggplot2::alpha("#D0D0CE", 0.4))
)

# set theme
ggthemr::ggthemr(cds_theme, type = "outer")

# source custom DAG theme
source("../../code/dag_theme.R")
```

Now we will talk about some statistical concepts, that are the foundation for modeling processes in both statistical and causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to show [here](https://seeing-theory.brown.edu/basic-probability/index.html).

# Random Variable

For starters, let's again define what a random variable is. Often represented by letters such as $X$, a random variable has a set of values, also called sample space, of which any could be the outcome if we draw a random sample from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die) or continuous values (e.g. average height of individuals).

# Expected value

A random variable is a real-valued function and can with more than one possible outcomes. This is why we cannot represent it as a scalar. The expected value of random variable, however, is a scalar and represents something like a "summary" of the random variable. Knowing the possible values (from the sample space) and the probability distribution, we can compute the expected value.

::: callout-tip
The summation operator $\sum$, denoted by the Greek capital Sigma is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, $x_1, x_2, …, x_n$ to a shorter and more readable form

$$
\sum_{i=1}^nx_i \equiv x_1+x_2+\ldots+x_n
$$

with the arbitrary index of summation $i$ being the lower limit and $n$ the upper limit.

By basic math rules, the following simplifications are possible, where $c$ is a constant:

$$
\sum_{i=1}^nc=nc
$$

and

$$
\sum_{i=1}^ncx_i=c\sum_{i=1}^nx_i  
$$
:::

::: callout-important
The *expected value*, also called *population mean*, of a variable $X$ is defined as the weighted average of possible values the random variable can take, where the weight is equal to the probability of the random variable taking on a specific value.
:::

If we want to compute it for a discrete random variable, we make use of the summation operator. Considering a finite list of potential values $x_1, x_2, …, x_k$ with probabilities $p_1, p_2, …, p_k$, the expectation of $X$ can be computed by

$$
E(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \sum_{j}^{k} x_i*p_i
$$

As an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:

$$
E(X) = \frac{1}{6}*1 + \frac{1}{6}*2 + ... + \frac{1}{6}*6 = \frac{1}{6} \sum (1 + 2 + ... + 6)
$$

Probabilities could also be different, as long as they sum to 1. For continuous random variables, we need a function returning the probabilities for each value. We leave that out here as we are only interested in understanding the intuition behind the expected value. However, it is very similar.

Let's type in the probabilities and outcomes of rolling a die and see what the expected value is.

::: callout-note
| To replicate values and create a vector `rep()` can be used. It takes the value and number of times it should be replicated.
:::

```{r}
# Vector of probabilities (all equal)
p <- rep(1/6, 6)

# Vector of possible outcomes
x <- 1:6

# Expected value
sum(p*x)
```

As you might have expected, it's 3.5.

::: callout-tip
Additional rules regarding the calculation of expected values that can be useful are:

$$
E(aW+b) = aE(W)+b\ \text{for any constants $a$, $b$} \\
E(W+H) = E(W)+E(H) \\E\Big(W - E(W)\Big) = 0
$$
:::

Knowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.

# Conditional Expected Value

The conditional expected value is the expected value conditioned on some other value. Given the value $x$ of $X$, the expected value for $Y$ obtains as

$$
E[Y|X = x]
$$

and is a function of $x$. In other words, the conditional expected value is the best guess for $Y$ knowing only that $X=x$.

As a simple example, consider we take a representative random sample from the world population and want to compute the expected value for $height$. Denoting height with $Y$, the expected value for the whole population is the expected value $E[Y]$ over all individuals in your sample.

The conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.

$$
E[Y] \neq E[Y|age < 10] \neq E[Y|age >= 10]
$$

# **Variance**

Before we define variance, let's see why it is important to know. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.

::: callout-note
| `seq(from, to, by)` or `seq(from, to, length.out)` returns a vectors with a sequence as specified by the arguments.
:::

::: callout-note
| `rnorm(n, mean, sd)` samples values from the normal distribution. `n` specifies the number of values, `mean` and `sd` define the parameters of the normal distribution.
:::

::: callout-note
| `map()` is a very useful function when you want to apply a function to each element of a list or a vector.
:::

::: callout-note
| `pivot_longer()` changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need `pivot_wider()`.
:::

```{r}
#| code-fold: true
#| warnings: false
#| message: false

# Load tidyverse package
library(tidyverse)

# 100 step-wise values from 1 to 5
X <- seq(1, 5, length.out = 100)

# For each value x_i of X sample from normal distribution with mean equal to x_i
# Note the different values for standard deviation
Y_lv <- map(X, function(i) rnorm(1, i, 0.25)) %>% unlist()
Y_hv <- map(X, function(i) rnorm(1, i, 1.5)) %>% unlist()

# Create tibble with X and YY values for low and high variance
df <- tibble(X = X, 
             Y_low_variance = Y_lv,
             Y_high_variance = Y_hv) %>%
  pivot_longer(cols = c(Y_low_variance, Y_high_variance))

# Plot both X~Y relations as scatter plot
ggplot(df, aes(X, value)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  facet_wrap(~name)
```

Mathematically, the *variance* is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The *sample variance* indicates how far a set of observed values spread out from their average value and is an estimate of the full *population variance*, that in most cases cannot be directly observed due to lack of data of the whole population.

Mathematically, the *population variance* is defined as

$$
Var(W)=\sigma^2=E\Big[\big(W-E(W)\big)^2\Big]\
$$

and the *sample variance* results as

$$
\widehat{\sigma}^2=(n-1)^{-1}\sum_{i=1}^n(x_i - \overline{x})^2
$$

You might have noticed the term $(n-1)^{-1}$ is different from what you probably expected ($n^{-1}$). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.

A related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.

```{r}
# High variance
var(Y_hv)
```

```{r}
# Low variance
var(Y_lv)
```

The **standard deviation** obtains as the square root of the variance:

$$
\sigma = \sqrt{\sigma^2}
$$

```{r}
# High standard deviation
sd(Y_hv)
```

```{r}
# Low standard deviation
# High standard deviation
sd(Y_lv)
```

::: callout-tip
A useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of $a$, then the variance will increase by $a^2$.

$$
Var(aX+b)=a^2V(X)
$$

You can also conveniently compute a variance for the sum of two random variables

$$
Var(X+Y)=Var(X)+Var(Y)+2\Big(E(XY) - E(X)E(Y)\Big)
$$

which in case of independence reduces to the sum of the individual variances due to the fact that $E(XY) = E(X)E(Y)$.
:::

# **Covariance**

Covariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.

::: callout-note
| `as_tibble()` or `as.data.frame()` can be used to change e.g. matrices or lists into tables.
:::

::: callout-note
| `rbind()` or `bind_rows()` are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are `cbind()` and `bind_cols()`.
:::

::: callout-note
| To rename columns, you can use `rename(new_name = old_name)`.
:::

If you're interested in how to create correlated variable, you can open the following code chunk.

```{r}
#| code-fold: true
#| warning: false

# Create the variance covariance matrix
sigma <- rbind(c(1, 0.8, 0), c(0.8, 1, -0.5), c(0, -0.5, 1))
# Create the mean vector
mu <- c(0, 0, 0) 

# Generate the multivariate normal distribution
df <- as_tibble(MASS::mvrnorm(n = 1e+3, mu = mu, Sigma = sigma)) %>% 
  rename(X1 = V1, X2 = V2, X3 = V3)
```

We defined the following covariance matrix:

```{r}
# Show variance-covariance matrix
sigma
```

From left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.

```{r}
#| code-fold: true
#| layout-nrow: 1 

ggplot(df, aes(x = X1, y = X2)) +
  geom_point() +
  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +
  labs(title = "Positive correlation")

ggplot(df, aes(x = X2, y = X3)) +
  geom_point() +
  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +
  labs(title = "Negative correlation")

ggplot(df, aes(x = X1, y = X3)) +
  geom_point() +
  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +
  labs(title = "No correlation")
```

If the equality $E(XY) = E(X)E(Y)$ holds, then it implies a *covariance* of 0 between the variables $X$ and $Y$. *Covariance* is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.

As a matter of form, the formula for the covariance of the random variables $X$ and $Y$ is

$$
Cov(X,Y) = E(XY) - E(X)E(Y)
$$

But, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the *correlation* which can be derived from the covariance if the individual variances are known.

$$
\text{Corr}(X,Y) = \dfrac{C(X,Y)}{\sqrt{V(X)V(Y)}}
$$

The *correlation* is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.

::: callout-note
| To compute correlation, variance and covariance, respectively, you can use `cor()`, `var()` and `cov()`.
:::

# Conclusion

Many of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.

# Assignments

From the [data folder](https://cloud.tuhh.de/index.php/s/owg5GLR9FoY9Xgm), load the table `random_vars.rds`. It contains draws from two random variables, $age$ and $income$.

1.  For each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.
    1.  expected value

    2.  variance

    3.  standard deviation
2.  Explain, if it makes sense to compare the standard deviations.
3.  Then, examine the relationship between both variables and compute:
    1.  covariance

    2.  correlation
4.  What measure is easier to interpret? Please discuss your interpretation.
5.  Compute the conditional expected value for:
    1.  $E[income|age <= 18]$

    2.  $E[income|age \in [18, 65)]$

    3.  $E[income|age >= 65]$

::: callout-warning
## How to submit your solutions!

Please see [here](https://owmork.github.io/causal_ds/submission/submission.html) how you have to sucessfully submit your solutions. I would recommend you to solve the assignments first in `.R` scripts and in the end convert them to the required format as explained in the submission instructions.
:::
