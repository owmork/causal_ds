{
  "hash": "1e69315fc4e927cc3f67bbe301337016",
  "result": {
    "markdown": "---\ntitle: \"Regression Discontinuity\"\nlinktitle: \"Regression Discontinuity\"\ndate: \"2022-10-01\"\noutput:\n  blogdown::html_page:\n    toc: true\nmenu:\n  example:\n    parent: Toolbox\n    weight: 11\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Introduction\n\nThe next tool we introduce is called **regression discontinuity design (RDD)**. Fist used in 1960 (Campbell), it did not play a large role until 1999 and since then has experienced growing acceptance due to the advance of more rigorous requirements regarding credibility and causality in social sciences. Another factor that made many researchers use RDDs is the increased availability of digitized administrative data that is often linked to arbitrary rules that can be exploited to capture \"as-if\" randomization processes for treatment assignment.\n\nAssuming a data-generating process, where we have a variable $X$ that is a confounder as it has an impact on treatment assignment $D$ and the outcome $Y$. Additionally, we could have an unobserved confounders between $X$ and $Y$.\n\nThen, as can be seen in the second DAG, the regression discontinuity design exploits the fact that $X$ determines $D$ and data is filtered such that there are only observations that were close to a cut-off value determining their treatment status. This way, treated and untreated units are very similar and comparable and RDD is able to eliminate selection bias for that subpopulation. Note that the treatment effect you calculate using this method is an average treatment effect for a subgroup rather than for the whole population.\n\n$X$ is called the **running variable** and is a continuous variable assigning units to treatment $D$ based on a cut-off score $c_0$. Because it has an impact on $Y$ as well, it is a confounder and opens a backdoor path. Now, the problem is that due to the cut-off determining the treatment $D$, the backdoor cannot be closed with regular ways like e.g. matching as there is no overlap, i.e. there are no treated and untreated units for all levels of $X$.\n\n!!! Summarizing:\n\n-   Running/forcing variable: ...\n\n-   Cut-off/threshold: ...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n✔ tibble  3.1.6     ✔ dplyr   1.0.8\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggdag)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttache Paket: 'ggdag'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDas folgende Objekt ist maskiert 'package:stats':\n\n    filter\n```\n:::\n\n```{.r .cell-code}\nlibrary(dagitty)\n\n# Initial situation\ndag_model_1 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> X\nU -> Y\nX -> D\nX -> Y\n}\n')\n \n# Directed Acyclic Graph\ndag_1 <- ggdag_status(dag_model_1) +\n  guides(color = \"none\") +\n  theme_dag()\n\n# Exploiting cut-off\ndag_model_2 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX_c [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> Y\nX_c -> D\n}\n')\n \n# Directed Acyclic Graph\ndag_2 <- ggdag_status(dag_model_2) +\n  guides(color = \"none\") +\n  theme_dag()\n\ngridExtra::grid.arrange(dag_1, dag_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nTherefore, as the second graph shows, the causal effect is identified by analyzing only observations that are in close neighborhood to $c_0$. The identified treatment effect is the **local average treatment effect (LATE)**.\n\n$$\nLATE_{RDD} = E[Y_1 - Y_0| X \\rightarrow c_0] \n$$\n\n# Continuity assumption\n\nRDD relies heavily on the continuity assumption, which implies that the cut-off value is exogenous, i.e. there is no logic why the cut-off has to be at that particular level of $X$. In other words, the rule was chosen arbitrary. Only if that is fulfilled, expected potential outcomes are almost the same for units just below and just above the cut-off (the only units that are analyzed). Even across the cut-off threshold, both $E[Y_0|X \\rightarrow c_0]$ and $E[Y_1|X \\rightarrow c_0]$ are **continuous functions** of $X$.\n\nSmooth and continuous functions of potential outcomes at the cut-off also implies that there are no other interventions or variables that occur at the cut-off and rules out omitted variable bias.\n\nGraphically, we have illustrated the continuity assumption by removing the arrow from $X$ to $Y$. As all units have about the same value for $X$, no effect from $X$ on $Y$ can be expected from those specific units. For $X \\rightarrow c_0$, there is no direct effect from $X$ to $Y$.\n\nThere are two different designs of RDD. Both have in common that the probability of treatment changes **discontinuously** at the threshold, but how strict the probability changes differs.\n\n-   **Sharp design:** at the cut-off, probability changes from 0 to 1. All units below cut-off belong to control group, while all units above cut-off belong to treatment group (or the other way around).\n\n-   **Fuzzy design:** at the cut-off, probability discontinuously increases. Most units below cut-off belong to control group, while most units above cut-off belong to control group (or the other way around). Here, cut-off is used as an instrumental variable.\n\nThe continuity assumption is crucial for conducting a valid RDD and that is why you have to ensure it is fulfilled. Violations can potentially happen when:\n\n-   units know assignment rule in advance\n\n-   units are interested in adjusting behavior according to assignment rule\n\n-   units have enough time to adjust\n\n-   other factors cause potential outcomes to shift at cut-off\n\n-   nonrandom heaping along running variable\n\n![](images/paste-3209D5B2.png)\n\n# Estimation and Inference\n\n## Research Design\n\nRDDs are quite intuitive and very graphical. For this reason, we will go through estimation and inference using an application and explain at each step what has to be considered.\n\nIn the application, we want to analyze the effect of being graded \"very good\" instead of \"good\" on a restaurant review site. Customers leave reviews at this site for restaurants they have visited and the resulting average score (from 1 to 10) is phrased with a label. Restaurants with a score $[8,9)$ will receive the label \"good\", while restaurants with a score of $[9, 10]$ receive the label \"very good\".We will only focus on restaurants with a score larger than 8.\n\nYou might already see how we can use RDD in this application to identify the effect of a \"very good\" label on revenue. We can exploit the rule that there is a sharp cut-off for restaurants having a score just above or below 9. That means our cut-off value is $c0 = 9$.\n\n\n\n\n\nLet's have a look at what the data looks like.\n\n!!! INCLUDE COVARIATES?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"user_rating\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"user_rating_ct\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"rating_label\"],\"name\":[3],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"management\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"size\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"revenue\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"9.1\",\"2\":\"0.12\",\"3\":\"TRUE\",\"4\":\"1\",\"5\":\"65\",\"6\":\"119\"},{\"1\":\"8.6\",\"2\":\"-0.37\",\"3\":\"FALSE\",\"4\":\"0\",\"5\":\"23\",\"6\":\"52\"},{\"1\":\"8.3\",\"2\":\"-0.75\",\"3\":\"FALSE\",\"4\":\"0\",\"5\":\"15\",\"6\":\"33\"},{\"1\":\"8.7\",\"2\":\"-0.30\",\"3\":\"FALSE\",\"4\":\"1\",\"5\":\"90\",\"6\":\"166\"},{\"1\":\"9.7\",\"2\":\"0.72\",\"3\":\"TRUE\",\"4\":\"1\",\"5\":\"84\",\"6\":\"173\"},{\"1\":\"9.4\",\"2\":\"0.42\",\"3\":\"TRUE\",\"4\":\"1\",\"5\":\"81\",\"6\":\"151\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n!!! DAG?\n\nAt first glance, it looks like all restaurants below the cut-off don't have a \"very good\" label (indicated by FALSE) and the restaurants above cut-off do have it. We can visualize if that applies to all restaurants. As we have expected, to the left of the cut-off and to the right of the cut-off, there is always just one label type. It means, we are dealing with a sharp cut-off.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [2] Visualization ----\n# [2.1] Compliance ----\n# As expected, perfect \"compliance\" and sharp cutoff. All \n# restaurants below the cutoff get a \"good\" rating, while all restaurants above\n# the cutoff get a \"very good\" rating.\ncompl <- \n  ggplot(df, aes(x = user_rating, y = rating_label, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[4]) +\n  geom_point(alpha = 1000/n, position = position_jitter(width = NULL, height = 0.05)) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Good\", \"Very Good\"))+\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"\") +\n  theme(legend.title = element_blank())\ncompl\n```\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Random assignment\n\nAs already mentioned, for RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\n\nWe can see that there is no decline or incline at the cut-off and therefore can assume that the continuity assumption holds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [2.2] Random assignment test ----\n# identifying assumption: random assignment to either side of cut-off\n# Manual plot\nggplot(df, aes(x = user_rating, fill = label)) +\n  geom_histogram(binwidth = .1, color = \"white\", boundary = c0, alpha = .6) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5], size = 2, linetype = \"solid\")\n```\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nTo check the continuity assumption more thoroughly, we can also use functions of the `rddensity` package. It relies on a test to check the assumption. as the p-value is large, we can reject the null hypothesis that the number of units at either side are different.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd <- rddensity(df$user_rating, c = c0)\nsummary(rddd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       5000\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 9                 Left of c           Right of c          \nNumber of obs         2488                2512                \nEff. Number of obs    709                 840                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.178               0.199               \n\nMethod                T                   P > |T|             \nRobust                0.5829              0.5599              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          <c     >=c    P>|T|\n0.003                       8      12    0.5034\n0.006                      18      22    0.6358\n0.008                      27      35    0.3742\n0.011                      37      46    0.3800\n0.014                      50      58    0.5008\n0.017                      60      72    0.3384\n0.019                      79      85    0.6963\n0.022                      89      96    0.6592\n0.025                     100     111    0.4913\n0.028                     118     122    0.8465\n```\n:::\n:::\n\n\nIt can also be shown graphically, where you can see that the confidence intervals overlap. If they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to get valid results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Visually check continuity at running variable\nrdd_plot <- rdplotdensity(rddd, df$user_rating, plotN = 100)\n```\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Plot Treatment Effect\n\nHaving checked potential violations of the continuity assumptions, we can move on and estimate the treatment effect. We start with selecting a bandwidth, i.e. we select what and how many observations should be compared. The larger the bandwidth, the more observations are taken into consideration but it also reduces the comparability. On the other hand, choosing a small bandwidth results in a lower number of observations while ensuring similarity and comparability.\n\nThere is no safe rule how to best select the bandwidth, although there are algorithms attempting to look for the optimal bandwidth. For now, we just use common sense and select a bandwidth of 0.15, resulting in an analysis window $[8.85, 9.15]$, which still leaves us with about 1000 observations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [3] Dependent variable ----\n# [3.1] Average Treatment Effect ----\n# Plot regression lines for full and specified bandwidth.\n# Specify bandwidth\nbw <- c0 + c(-0.15, 0.15)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(user_rating %>% between(bw[1], c0))\ndf_bw_above <- df %>% filter(user_rating %>% between(c0, bw[2]))\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1268    6\n```\n:::\n:::\n\n\nTo illustrate the difference between using only a small window and all data, we plot the resulting regression lines. You can see that both approaches would lead to different results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Plot dependent variable vs running variable\ndep_var <-\n  ggplot(df, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_vline(xintercept = bw[1], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_vline(xintercept = bw[2], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_point(alpha = 1000/n, size = 0.5) +\n  # add lines for the full range\n  geom_smooth(data = filter(df, user_rating <= c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(df, user_rating > c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  # add lines for specified bandwidth\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nFrom the plot above, it is hard to see what the difference between observations close to the cut-off is. So what we can do is to compute to regressions, one for the observations in $[8.85, 9)$ and another one for the observations in $[9,9.15]$.\n\nThen, using the resulting coefficients, we compute what both models predict for the cut-off value $c0$ and take the difference. The difference is the local average treatment effect (LATE).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [3.2] Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(revenue ~ user_rating, df_bw_below)\nmodel_bw_above <- lm(revenue ~ user_rating, df_bw_above)\n\ny0 <- predict(model_bw_below, tibble(user_rating = c0))\ny1 <- predict(model_bw_above, tibble(user_rating = c0))\n\nlate <- y1 - y0\nlate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 1 \n25 \n```\n:::\n:::\n\n\nIt's a bit messy when we plot all observations, so let's zoom in to see if we can detect the local average treatment effect graphically. Not surprisingly, it is equal to what we have just computed.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Minimum and maximum for y-axis limits\nmin_y <- min(df_bw$revenue)\nmax_y <- max(df_bw$revenue)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw <- \n  ggplot(df_bw, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.2, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  theme(legend.position = \"bottom\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y0, yend = y0),\n             linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  annotate(\"text\", x = c0 - 0.05, y = mean(c(y1, y0)),\n           label = sprintf(\"Difference: %.2f\", late), fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Estimate Treatment Effect\n\n### Parametric Estimation\n\nWhat you will see most in studies is a regression to compute the LATE. Here, we prefer to use `user_rating_ct`, which is the raw `user_rating` variable centered, i.e. subtracted by the cut-off value $c0$. That simplifies the interpretation, however, it does not change the coefficient of interest, the LATE.\n\nThe coefficient we are most interested in is the one for `rating_labelTRUE`. It is equal to the effect in the plot above, but the regression summary also yields additional statistical information. We see that the LATE is statistically significant.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [4] Estimation ----\n# [4.1] Parametric ----\n# Compute coefficients for specified bandwidth.\nparam_bw <- lm(revenue ~ user_rating_ct + rating_label, df_bw)\nsummary(param_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = revenue ~ user_rating_ct + rating_label, data = df_bw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-106.67  -28.59    0.19   28.25  103.00 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         90.00       2.36   38.18  < 2e-16 ***\nuser_rating_ct       2.84      25.62    0.11     0.91    \nrating_labelTRUE    24.96       4.31    5.80  8.5e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38 on 1265 degrees of freedom\nMultiple R-squared:   0.1,\tAdjusted R-squared:  0.0987 \nF-statistic: 70.4 on 2 and 1265 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n### Non-parametric Estimation\n\nYou can also use non-parametric estimation techniques, i.e. we do not fit a line through the data but instead a curve. The `R` package `rdrobust` is a good resource that is easy to use. We just have to provide the dependent variable, the running variable and the cut-off value.\n\nBy default, the `rdrobust()` function automatically selects a bandwidth and uses a kernel weighting function (triangular kernel).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [4.2] Non-parametric ----\nlibrary(rdrobust)\nnparam_bw <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, c = c0)\nsummary(nparam_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             214          207\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.048        0.048\nBW bias (b)                   0.076        0.076\nrho (h/b)                     0.634        0.634\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    29.892     8.322     3.592     0.000    [13.582 , 46.203]    \n        Robust         -         -     3.111     0.002    [11.487 , 50.600]    \n=============================================================================\n```\n:::\n:::\n\n\nTo check how the curve goes through the data, we can use `rdplot()`, which splits the data into bins and for each bin shows an average data point. We see that the curve fits the data very well.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrdplot(y = df$revenue, x = df$user_rating, c = c0)\n```\n\n::: {.cell-output-display}\n![](09_rdd_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nThere are a lot of options with the `rdrobust()` function, e.g. we can change the kernel and see how it affects the result.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use other kernel\nnparam_bw_kernel <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, \n                             c = c0, kernel = \"epanechnikov\")\nsummary(nparam_bw_kernel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Epanechnikov\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             204          199\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.046        0.046\nBW bias (b)                   0.075        0.075\nrho (h/b)                     0.606        0.606\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    30.892     8.286     3.728     0.000    [14.652 , 47.132]    \n        Robust         -         -     3.257     0.001    [12.846 , 51.681]    \n=============================================================================\n```\n:::\n:::\n\n\n# Bandwidth robustness test or sensitivity\n\n# Kernel robustness test\n\n# Assignment\n\n1)  There are a lot of parameters that can be changed when using RDD. Change the bandwidth (for example only half the bandwidth or double the bandwidth) and compare how sensitive the original estimate is to those changes.\n\n2)  Load this data set and check for potential violations of the continuity assumption. Find possible explanations for violations.\n",
    "supporting": [
      "09_rdd_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}