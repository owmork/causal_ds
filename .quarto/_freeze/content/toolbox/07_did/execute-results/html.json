{
  "hash": "bb048756ab9a73bc014eae17afa8bb80",
  "result": {
    "markdown": "---\ntitle: \"Difference-in-Differences\"\nlinktitle: \"Difference-in-Differences\"\ndate: \"2022-11-23\"\noutput:\n  blogdown::html_page:\n    toc: true\nmenu:\n  example:\n    parent: Toolbox\n    weight: 9\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Introduction\n\nAnother quasi-experimental method from our toolbox is the **difference-in-differences (DiD)** approach. It is the most popular research design in quantitative and social sciences. As the name implies, the method **captures differences by observing a treatment and a control group over time** to estimate causal average effects.\n\nIn its simplest form, DiD compares **two groups** (control and treatment) **at two points in time** (before treatment and after) by observing if and how different both groups change. It is important to note, that both groups do not need to be equal before the treatment.\n\nBy taking two differences, **two different kind of biases can be avoided**. First, by comparing both groups at both points in time, any external effect that affects the outcome through time plays no role as it affects both groups. Secondly, taking only the difference of change in consideration, we can disregard selection bias. Potential outcomes can differ:\n\n$$\nE[Y_0|D = 0] \\gtreqqless E[Y_0|D = 1] \n$$\n\nWe don't care whether initially treatment and control group are different. We only assume that they **behave similarly in absence of treatment**.\n\nAs can be seen in the table, the difference in outcome for the treatment group before and after treatment is $D + T$, while for the control group it is only $T$. The difference of these two differences then reduces to only $D$, which is the treatment effect we want to estimate.\n\n| Group           | Time | Outcome                   | 1st Difference | DiD |\n|-----------------|------|---------------------------|----------------|-----|\n| Treatment (D=1) | 0    | $Y= Y_{T=0, D=1}$         |                |     |\n|                 | 1    | $Y = Y_{T=0,D=1} + T + D$ | $T +D$         |     |\n|                 |      |                           |                | $D$ |\n| Control (D=0)   | 0    | $Y = Y_{T=0, D=0}$        |                |     |\n|                 | 1    | $Y = Y_{T=0, D=0} + T$    | $T$            |     |\n\nWe can also break it down in our known notation:\n\n$$\nD = ATE = \\bigg(E[Y_{D=1}|T=1] - E[Y_{D=1}|T=0] \\bigg)- \\bigg(E[Y_{D=0}|T=1] - E[Y_{D=0}|T=0]\\bigg)\n$$\n\nBecause there are a lot of subscripts, it can also help to write down the formula in pseudo-math:\n\n$$\nATE = (Y_{Treatment, After} - Y_{Treatment, before}) - (Y_{Control, After} - Y_{Control, Before})\n$$\n\nAgain, opposed to methods where we just know one outcome - the \"after\" outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential outcomes $E[Y_0|D=1] = E[Y_0|D=1]$ are equal. That is a big difference, because do not have to assume that observation units are similar in all their characteristics.\n\nInstead DiD hinges on a different assumption, the **parallel trends assumption.** It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but only the change of outcomes from before to after. It implies that there is no factor that has only an impact on just one of the groups. If units differ in characteristics, they are only allowed to have a constant effect. If the effect varies with time, the parallel trends assumption is violated.\n\n# Application\n\nIllustrating the parallel trends assumption is very helpful. By going through two scenarios, we will look at an example where parallel trends are fulfilled and another one where it is violated.\n\n-   **Scenario A**: parallel trends assumption fulfilled\n\n-   **Scenario B**: parallel trends assumption violated\n\nLet's imagine, you are manager of a company that has two stores, one of them being in City 1 and the other being in City 2. You want to test the effectiveness of a local ad campaign on sales. Therefore, you will run a campaign at the store in City 1 but not in City 2 and keep track of sales in the periods before and after the treatment.\n\nYou don't need to read follow all steps how to generate data for both scenarios, but I'll leave it in for those who are interested.\n\n::: {.callout-tip collapse=\"true\"}\n## Data Simulation\n\n$Y$ - sales\n\n$D$ true treatment effect\n\n$P$ is periods\n\n$S$ store index\n\n$X$ covariate, e.g. ecosystem\n\nWith the function *generate_data()* we generate data with arbitrary values for the number of stores, number of observed periods, the size of the true treatment effect, the timing of treatment and level of noise in the data generating process. Data is generated for both scenarios as can be seen at the suffixes.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function to simulate DiD data\"}\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Function to simulate data\ngenerate_data <- function(\n    S = 2, # number of groups\n    P = 10, # number of periods\n    D_size = 1, # effect of treatment\n    D_time = NULL, # time of treatment\n    y_0 = 50, # base value for y\n    sd = 1, # standard deviations for randomly generated sequences\n    scenario = c(\"A\", \"B\")\n){\n  # create group period dyads\n  s <- rep(0:1, each = P)\n  p <- rep(1:P, S)\n  \n  # timing and size of treatment (effect)\n  delta <- D_size\n  if (missing(D_time)) D_time <- P/2\n  after <- as.numeric(ifelse(p > D_time, 1, 0))\n  \n  # create relation between independent variables and treatment (actually\n  # other way round, but easier to simulate this way)\n  x1 <- rnorm(S*P, s, sd)\n  \n  # create dependent variable ...\n  # ... for scenario (A)\n  y_a <- y_0 + delta*s*after + 1/5*p + x1 + rnorm(S*P, 0, sd)\n  \n  # ... for scenario (B)\n  y_b <- y_0 + delta*s*after + 1/5*p + x1 + s*x1 + 1/3*p*x1 + rnorm(S*P, 0, sd)\n  \n  # add variables to table\n  df <- tibble(\n    treat   = s,\n    period  = p,\n    after   = after,\n    x1      = x1,\n    sales   = if (scenario == \"A\") {y_a} else {y_b}\n  )\n  \n  # return table\n  return(df)\n}\n```\n:::\n\n\nWe choose to generate a data set with a true treatment effect of 1 and 10 observed periods. For now, we just have one store as a treated unit and one store as a non-treated unit. Later, we will extend it to a larger number of stores per group, but for demonstration purpose, we restrict it to two stores, initially. We have 10 periods with 5 periods before and 5 after treatment. Lastly, we add a little bit of noise.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate one sample for each scenario\nP     <- 10 # number of periods\ndelta <- 1  # true treatment effect\n\n# Scenario A\ndf_A <- generate_data(P = P, D_size = delta, sd = 0.01, scenario = \"A\")\n\n# Scenario B\ndf_B <- generate_data(P = P, D_size = delta, sd = 0.01, scenario = \"B\")\n```\n:::\n\n\n## Parallel trends\n\n### Scenario A\n\nTo compute an estimated treatment effect, we filter the data to the two periods just around treatment and implement the formulas as in the introduction. Not surprisingly, we get an estimate that is very close to our true treatment effect.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [1.1.1] (A) Fulfillment ----\n# Scenario (A)\n# Only show last data point before and first data point after treatment.\ndf_A_zoom_in  <- df_A %>% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_A <- df_A_zoom_in %>%\n  filter(treat == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_A <- df_A_zoom_in %>%\n  filter(treat == 1, after == 0) %>% \n  pull(sales)\n\ndiff_before_A <- before_treatment_A - before_control_A\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_A <- df_A_zoom_in %>%\n  filter(treat == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_A <- df_A_zoom_in %>%\n  filter(treat == 1, after == 1) %>% \n  pull(sales)\n\ndiff_after_A <- after_treatment_A - after_control_A\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate may vary from \n# true treatment effect, as we also include some noise in the data generating \n# process.\ndiff_diff_A <- diff_after_A - diff_before_A\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_A, delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Estimate: 1.05, True Effect: 1.00\"\n```\n:::\n:::\n\n\nLooking at the last period before and the first period after treatment, the impact of treatment can clearly be seen. The dashed line represents the counterfactual value for the treated group, i.e. the value it would have if it had not been treated. This value is not observed, but by the parallel trends assumptions, it would have developed like the value for the untreated group.\n\n::: callout-tip\n## Plots in this chapter\n\nGenerating the plots for this chapter is a bit tricky as they contain a lot of annotations and other extensions. I left the code for those who want to replicate it. But you do not worry if you cannot reproduce them.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot parallel trends assumption\"}\n# Compute counterfactual sales for treated group\ncf_treat_A <- df_A[!df_A$treat == 1, \"sales\"] + diff_before_A\ndf_A[df_A$treat == 1, \"sales_cf\"] <- cf_treat_A\n\n# Add to zoomed in table\ndf_A_zoom_in <- df_A_zoom_in %>% left_join(df_A)\n\n# Plot DiD with parallel trends assumption\nggplot(df_A_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A_zoom_in %>% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_A - (diff_diff_A / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_A + 1.1*diff_before_A + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A:\\nParallel Trends Assumption\")\n```\n\n::: {.cell-output-display}\n![If parallel trends assumption can be assumed, treatment effect is valid. Counterfactual line shows how outcome would have been evolved in absence of treatment.](07_did_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n### Scenario B\n\nRepeating the steps for scenario B yields an unexpected result. The estimated treatment effect is different from what we would have expected.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [1.1.2] (B) Violation ----\n# Scenario (B)\n# Only show last data point before and first data point after treatment.\ndf_B_zoom_in  <- df_B %>% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_B <- df_B_zoom_in %>%\n  filter(treat == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_B <- df_B_zoom_in %>%\n  filter(treat == 1, after == 0) %>% \n  pull(sales)\n\ndiff_before_B <- before_treatment_B - before_control_B\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_B <- df_B_zoom_in %>%\n  filter(treat == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_B <- df_B_zoom_in %>%\n  filter(treat == 1, after == 1) %>% \n  pull(sales)\n\ndiff_after_B <- after_treatment_B - after_control_B\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate varies from \n# true treatment effect due to confounding and added noise.\ndiff_diff_B <- diff_after_B - diff_before_B\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_B, delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Estimate: 1.23, True Effect: 1.00\"\n```\n:::\n:::\n\n\nAgain, the picture is very similar. Having only four data points, treatment before and after and control before and after, there is no way to test the parallel trends assumption which leaves room for doubt. So how can we check whether we made a mistake or the parallel trends assumption is violated?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot parallel trends assumption\"}\n# Compute counterfactual sales for treated group\ncf_treat_B <- df_B[!df_B$treat == 1, \"sales\"] + diff_before_B\ndf_B[df_B$treat == 1, \"sales_cf\"] <- cf_treat_B\n\n# Add to zoomed in table\ndf_B_zoom_in <- df_B_zoom_in %>% left_join(df_B)\n\n# Plot DiD with parallel trends assumption\nggplot(df_B_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B_zoom_in %>% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_B - (diff_diff_B / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_B + 1.1*diff_before_B + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B:\\nParallel Trends Assumption\")\n```\n\n::: {.cell-output-display}\n![If parallel trends assumption can be assumed, treatment effect is valid. Counterfactual line shows how outcome would have been evolved in absence of treatment.](07_did_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Event Study\n\nMany researchers therefore try to increase the validity of their results by providing an event study. Comparing trends before treatment across treatment and control group, it should show that there was no difference. **Because if there was no difference before treatment, why should there be difference after the treatment (if not for the treatment?**\n\nHowever, event studies cannot provide full certainty about the parallel trends assumption. There still might be other unobserved factors that could affect the treatment. But still, it is a good way to argue that treatment and control group are comparable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Event Study A\"}\n# [1.2] Event study ----\n# To provide evidence of the credibility in assuming parallel trends, \n# researchers often perform an event study, if possible. Instead of only\n# looking at the last period before and the first period after treatment,\n# further periods are included to examine the validity of the parallel trends\n# assumption and the treatment effect estimate.\n\n# [1.2.1] (A) Fulfillment ----\n# Zoom out and show that parallel trend assumption is fulfilled in scenario (a)\n\n# Compute difference in control group\ndiff_control_A <- after_control_A - before_control_A\n\n# Plot event study\nev_stdy_A <- ggplot(df_A, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A %>% filter(treat == 1, period >= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(breaks = 1:P) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A\\nEvent Study\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Event Study B\"}\n# [1.2.2] (B) Violation----\n# Zoom out and show that parallel trend assumption is violated in scenario (b)\n\n# Compute difference in treatment group\n# Before treatment\ndiff_control_B <- after_control_B - before_control_B\n\n# Increase from t0 to before treatment\ninit_treatment_B <- df_B %>%\n  filter(treat == 1, period == 1) %>%\n  pull(sales)\ndiff_treatment_B <-  (before_treatment_B - init_treatment_B) / (P/2)\n\n# Plot event study\nev_stdy_B <- ggplot(df_B, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B %>% filter(treat == 1, period >= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[2], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_B,\n           yend = before_treatment_B + (P/2)*(diff_treatment_B),\n           linetype = \"dashed\", color = ggthemr::swatch()[3]) +\n  # Estimated treatment effect\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(name   = \"Period\", breaks = 1:P) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B\\nEvent Study\")\n```\n:::\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![For treatment and control group, we see the same trend over time. This lends credibility to the parallel trends assumption and consequently, to the validity of the causal treatment effect.](07_did_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=75%}\n:::\n\n::: {.cell-output-display}\n![Other than in scenario A, the parallel trends assumption does not seem to hold. The estimated treatment effect is larger than the actual treatment effect. This is due to different trends in both groups. The treatment group has a more positive trend even without treatment and the groups would have further diverged after treatment (see dashed red line). The difference between the dashed red and dashed blue line is attributable to this trend and should not be part of the treatment effect.](07_did_files/figure-html/unnamed-chunk-9-2.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Modeling\n\nA more typical situation is usually that there is more than one unit in the treatment and control group. You could e.g. imagine that you are managing more than two stores and are implementing an ad campaign in a specific region.\n\nTo simulate such a scenario, we generate data for 3'000 stores that are split evenly into two regions. In one region, the ad campaign will be run (treatment region) and in the other there will be no campaign (control region). The variable relationships as defined in the previous section still hold.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [1.4] Linear regression ----\n# Now assume that there are more than two stores and treatment is performed\n# e.g. in a specific region which are, depending on scenario (A) and (B) \n# different.\n# Generate a bunch of samples and combine in one table. Here, we choose a higher\n# standard deviation.\nn_stores <- 3e+3\n\n# Scenario A\ndf_A_lm  <- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"A\")}) %>%\n    bind_rows() %>%\n  filter(period %in% (P/2):(P/2+1))\n\n# Scenario B\ndf_B_lm  <- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"B\")}) %>%\n  bind_rows() %>%\n  filter(period %in% (P/2):(P/2+1))\n```\n:::\n\n\n### Scenario A\n\nSo how do we compute the average treatment effect? Previously in this chapter, we just used basic math calculations (particularly subtraction). But there is an easier way: we can use regression again. This is because the average treatment effect is the coefficient of the interaction of group and time.\n\n$$\ny_i = \\beta_0 + \\beta_1 * Period_i + \\beta_2 * Treatment_i + \\beta_3 * (Time_i * Treatment_i) + \\epsilon_i\n$$\n\n$Time$ indicates whether the period is before or after the treatment and $Treatment$ whether an observation was treated or not. Then, the coefficient we are interested in is $\\beta_3$, because its term is only active for the treated group after treatment.\n\n!!! x1: maybe purchase power in region\n\nFor scenario A, we can see that there is no need to adjust for the covariate $x1$. If you check the formulas again, your will notice that $x1$ has a constant and time-invariant effect on sales and therefore it does not violate the paralell trends assumption.\n\nIncluding or leaving out $x1$ in the regression yields the a similar unbiased estimate (close to defined true size) for our variable of interest $store:after$, the parameter of interest.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [1.4.1] (A) ----\n# (a): Due to the construction of the data set, we expect interaction\n# coefficient to be significant as well as the covariate and period. However, as\n# the covariate does not have a time-varying effect, it is not a confounder and\n# interaction coefficient should be unbiased even if not adjusting for the\n# covariate.\nsummary(lm(sales ~ treat*after , data = df_A_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ treat * after, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.176 -0.952  0.002  0.956  6.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.9610     0.0257 1981.43   <2e-16 ***\ntreat         1.0372     0.0364   28.52   <2e-16 ***\nafter         0.2642     0.0364    7.26    4e-13 ***\ntreat:after   0.9813     0.0514   19.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 11996 degrees of freedom\nMultiple R-squared:  0.284,\tAdjusted R-squared:  0.284 \nF-statistic: 1.58e+03 on 3 and 11996 DF,  p-value: <2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(sales ~ treat*after + x1, data = df_A_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ treat * after + x1, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.947 -0.680  0.000  0.673  4.204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.98128    0.01821 2800.25   <2e-16 ***\ntreat        0.03914    0.02732    1.43     0.15    \nafter        0.22876    0.02575    8.88   <2e-16 ***\nx1           0.98721    0.00903  109.31   <2e-16 ***\ntreat:after  0.97808    0.03641   26.86   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11995 degrees of freedom\nMultiple R-squared:  0.641,\tAdjusted R-squared:  0.641 \nF-statistic: 5.36e+03 on 4 and 11995 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n### Scenario B\n\nIn scenario B, the effect of *x1* is different because it has a time-varying effect. Therefore it violates the parallel trends assumption, leading to a biased estimate if *x1* is not included (e.g. because it is unobserved).\n\nBecause we constructed the data set ourselves, we are able to see that the bias in fact is quite large and the treatment effect seems to include the actual treatment effect plus the effect of *x1*. Even with including $x1$ and as a main effect and moderator, we cannot fully reconstruct the true treatment effect.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# [1.4.2] (B) ----\n# (b): Due to the construction of the data set, we expect interaction coefficient\n# to be significant and accurate only when adjusting for the time-varying effect\n# of the covariate and main effects for period and covariate.\nsummary(lm(sales ~ treat*after, data = df_B_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ treat * after, data = df_B_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.025  -2.331   0.008   2.293  16.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  51.0835     0.0643  794.08   <2e-16 ***\ntreat         3.6321     0.0910   39.92   <2e-16 ***\nafter         0.0682     0.0910    0.75     0.45    \ntreat:after   1.4315     0.1287   11.13   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 11996 degrees of freedom\nMultiple R-squared:  0.287,\tAdjusted R-squared:  0.287 \nF-statistic: 1.61e+03 on 3 and 11996 DF,  p-value: <2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(sales ~ treat*after + after*x1 + treat*x1, data = df_B_lm)) # best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ treat * after + after * x1 + treat * x1, \n    data = df_B_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.084 -0.682  0.001  0.679  4.348 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 51.00199    0.01818 2804.68  < 2e-16 ***\ntreat        0.00336    0.03012    0.11     0.91    \nafter        0.16597    0.02571    6.45  1.1e-10 ***\nx1           2.63804    0.01591  165.81  < 2e-16 ***\ntreat:after  1.00018    0.04063   24.62  < 2e-16 ***\nafter:x1     0.36083    0.01823   19.80  < 2e-16 ***\ntreat:x1     1.03128    0.01823   56.57  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11993 degrees of freedom\nMultiple R-squared:  0.943,\tAdjusted R-squared:  0.943 \nF-statistic: 3.31e+04 on 6 and 11993 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n# Conclusion\n\nDiD is a useful quasi-experimental method that relies on the parallel trends assumption which is untestable. We can't prove it but try to justify it by for example showing prior trends. If both groups were evolving similarly before the treatment, that supports the plausibility and appropriateness of using DiD.\n\nAdditionally, there are many extensions to the simple DiD approach we have not discussed here like the synthetic control method, that is able to deal with one treated and multiple untreated groups. By matching and weighting the untreated groups, a synthetic group is composed, that is similar in the lead up to the treatment period. It shares similarities with what we have done in the matching chapter.\n\n# Assignment\n\nImagine, you are manager of a large health provider that manages many hospitals and you want to test how a new admission procedure affects patient satisfaction. You randomly selected 18 hospitals that introduced the new admission procedure and compare them to 28 other hospitals that did not introduce the method. For both groups of hospitals you collected data from before and after the introduction. The data you have collected is from patient surveys where they were asked how satisfied they are.\n\nLoad the data from the file `hospdd.rds`[^1]. Then, perform a difference-in-differences analysis by\n\n[^1]: Data and example from: <https://www.stata.com/new-in-stata/difference-in-differences-DID-DDD/>\n\n1.  Manually computing the mean satisfaction for treated and control hospitals before and after the treatment. Helpful functions could be `filter()`, `pull()` and basic arithmetic operations.\n\n2.  Using a linear regression to compute the estimate. Also, include group and time fixed effects in the regression, i.e. one regressor for each month and one regressor for each hospital: Consider, whether you want to include them as\n\n    -   `month + hospital` or as\n\n    -   `as.factor(month) + as.factor(hospital)`\n\n    and explain what the difference is.\n\n::: callout-warning\n## How to submit your solutions!\n\nPlease see [here](https://owmork.github.io/causal_ds/submission/submission.html) how you have to successfully submit your solutions. I would recommend you to solve the assignments first in `.R` scripts and in the end convert them to the required format as explained in the submission instructions.\n:::\n",
    "supporting": [
      "07_did_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}