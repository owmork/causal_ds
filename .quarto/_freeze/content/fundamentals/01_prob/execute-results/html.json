{
  "hash": "c6d54eeb1e64c89a91d30001934133f7",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Statistics\"\nlinktitle: \"Probability Theory and Statistics\"\ndate: \"2022-11-23\"\noutput:\n  blogdown::html_page:\n    toc: true\nmenu:\n  example:\n    parent: Fundamentals\n    weight: 2\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Introduction\n\nBefore we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part.\n\n# Probability\n\nFirst, we will review some basic concepts of probability theory.\n\n## **Basic rules of probability**\n\nConsider the most simple example: flipping coins. We define the flip of a coin as a **random variable** as we don't know the outcome. To express our uncertainty, we make us of probability theory.\n\nFlipping the coin, we will see how the coin has landed and our random variable can take on of the **two possible events** $\\{H, T\\} \\subseteq \\Omega$. It will be either *Head* or *Tail*.\n\nSo we have already defined two terms: random variable and events. Now what is a probability? A **probability is always linked to an event** typically denoted by a capital letter, here either $H$ and $T$, and expresses **how likely this event is to happen**. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\n$$\nP(H) = P(T) = 0.5\n$$\n\n[Extreme cases]{.underline}: If an event $A$ is impossible, its probability is $P(A) = 0$ and if it is certain to occur, it is $P(A)=1)$.\n\n::: callout-important\n**Axiom 1:** Probability is a real number greater or equal to 0.\n:::\n\nWe can also introduce the **compliment** $\\overline{A}$, which is what happens when $A$ does not happen and consequently, $P(A) + P(\\overline{A}) = 1$. $A$ and $\\overline{A}$ are mutually exclusive, by definition. But there could also be two events $A$ and $B$ that are mutually exclusive, i.e. only one of those events can happen, then $P(A \\cup B) = P(A) + P(B)$, where $\\cup$ represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\n$$\nP(H \\cup T) = P(H) + P(T) = 1\n$$\n\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n::: callout-important\n**Axiom 2:** Total probability is equal to 1.\n:::\n\n::: callout-important\n**Axiom 3:** Probability of mutually exclusive events is the sum of the probabilities.\n:::\n\nTo understand what not mutually exclusive events are, consider events $studying$ and $working$. For a random person, we don't know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\n\nThen, the probability of both events happening is calculated by\n\n$$\nP(A \\cup B) = P(A) + P(B) + P(A \\cap B)\n$$\n\nwith $P(A \\cap B)$ being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the **addition rule**.\n\n!!! CHANGE\n\n![](/images/01_exclus.png){fig-align=\"center\" width=\"480\"}\n\nThe aforementioned intersection $P(A \\cap B)$ can be calculated by the **multiplication rule**,\n\n$$\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n$$\n\nwhere $P(A|B)$ denotes the probability of $A$ happening given that $B$ has happened. It is called a **conditional probability** and is defined by:\n\n$$\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n$$\n\nIt can be thought of as the probability of an event $A$ after you know that $B$ is true. Essentially, it computes the possibility of event $A$ and $B$, normalized by the probability of $B$ occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\n\nUsing the example with workers and students: without knowing exact numbers, we can almost safely assume that students are less likely to work than individuals who are not studying.\n\n$$\nP(working|studying) > P(working|\\overline{studying})\n$$\n\nEssentially, we are looking at probabilities restricted to a subset of the sample.\n\nAnother important concept when dealing with probabilities of events is **stochastic independence**. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways:\n\n$$\nP(A \\mid B) = P(A)\n$$\n\nA typical example of independence is to roll a die twice. The second roll does not depend on the first one and each outcome is as likely independent of the first roll. This also results in\n\n$$\nP(A \\cap B) = P(A) \\ P(B)\n$$\n\n## **Probability Tree**\n\nAn intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to $1$ in probability as one (and only one) of the events has to happen. The probability of two consecutive events is obtained by multiplying the probabilities.\n\nConsider the following example: you are project manager and based on your are interested in th probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Based on your historical data about past projects, you come up with the following tree.\n\n![Probability tree](/images/01_tree.png){fig-align=\"center\" width=\"320\"}\n\nhttps://www.mashupmath.com/blog/probability-tree-diagrams\n\nhttps://en.wikipedia.org/wiki/Tree_diagram\\_%28probability_theory%29#/media/File:Probability_tree_diagram.svg\n\n*https://eight2late.wordpress.com/2010/03/11/bayes-theorem-for-project-managers/*\n\n## Assignment 1\n\nDefine being on time as event $T$, being not on time as $\\overline{T}$, having a change in scope as $S$ and having no change in scope as $\\overline{S}$. Then, compute the following probabilities:\n\n-   $P(T \\cap S)$\n\n-   $P(T \\cap \\overline{S})$\n\n-   $P(\\overline{T} \\cap S)$\n\n-   $P(\\overline{T} \\cap \\overline{S})$\n\nWhat is the sum of all four probabilities?\n\n## **Set Theory**\n\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams building on set theory. A very simple one we actually already used above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\n\nLet's use an example to understand the other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices, smartphones, tables and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\n\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current distribution.\n\n::: callout-note\n| `library()` loads external packages/libraries containing functions that are not built in base `R`.\n:::\n\n::: callout-note\n| `tibble()` is the most convenient way to create tables. You specify column name and content and assign your `tibble` to an object to store it.\n:::\n\n::: callout-note\n| `ifelse(test, yes, no)` is a short function for if...else statements. The first argument is a condition that is either `TRUE` or `FALSE` and determines whether the second or third argument is returned.\n:::\n\n::: callout-note\n| `rbinom(n, size, prob)` samples `n` values from a binomial distribution of a given `size` and with given probabilities `prob`.\n:::\n\n::: callout-note\n| `mutate()` is one of the most important functions for data manipulation in tables. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, `mutate(table, new_variable = existing_var / 100)`, which is equivalent to `table %>% mutate(new_variable = existing_var / 100)`.\n:::\n\n!!! exclude creation and move R explanations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load tidyverse package\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\n# Nmber of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  rowwise() %>% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n```\n:::\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer.\n\n::: callout-note\n| To see the first lines of a table (for example a `tibble()` or a `data.frame()`, you can use the `head(table, n)` function, where `n` specifies how many rows you want to see.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(app_usage, 10)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"user_id\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"smartphone\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tablet\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"computer\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"3\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"4\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"5\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"6\",\"2\":\"1\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"7\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"8\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"9\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"10\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n::: callout-note\n| Summing all values by column is done by `colSums(table)`. For rows, you would use `rowSums(table)`.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncolSums(app_usage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n```\n:::\n:::\n\n\nSumming the $user\\_id$ does not make any sense. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n::: callout-note\n| To access only specified columns, you can provide the location or names in square brackets or you can use the `select()` function.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# equivalent commands\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsmartphone     tablet   computer \n       589        389        226 \n```\n:::\n:::\n\n\nNow let's see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n::: callout-note\n| `which()` checks a condition and returns the indices.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\nsets_all <- list(set_phon, set_tabl, set_comp)\n\nlibrary(ggVennDiagram)\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\")) +\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n```\n\n::: {.cell-output-display}\n![](01_prob_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n![Generic Venn diagram](/images/01_venn_diag.png){fig-align=\"center\" width=\"350\"}\n\nUsing the Venn diagram, we are able to answer questions like the following:\n\n-   What is the percentage of customers using all three devices?\n\n-   What is the percentage of customers using at least two devices?\n\n-   What is the percentage of customers using only one device?\n\nTry to answer these questions yourself.\n\nWe can also use the example to go through the basic probability rules defined above.\n\n**Addition rule**:\n\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\n$P(T \\cup S) = P(T) + P(S) - T \\cap S)$\n\n**Multiplication rule**:\n\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\n$P(T|C) = \\frac{P(T \\cap C)}{P(C)}$\n\n**Total probability rule**:\n\nWhat is the fraction of customers using a computer?\n\n$P(C) = P(C \\cap T) + P(C \\cap \\overline{T})$\n\n## **Bayes Theorem**\n\n### Math\n\nA very important theorem in probability theory is Bayes theorem that we get by reformulating the multiplication rule\n\n$$\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n$$\n\nUsing the equality of $P(A ∩ B)$ and $P(B ∩ A)$ we arrive at\n\n$$\nP(B|A)*P(A) = P(A|B)*P(B)\n$$\n\nand finally at the Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\nBayes theorem states the mathematical formulation for determining a conditional probability, exemplary the likelihood of $A$ occurring conditioned on $B$ having happened before.\n\nYou will often hear Bayes theorem in connection with the terms *updating beliefs*. You start with a prior probability $P(A)$ and collecting evidence $P(B)$ and the likelihood $P(B|A)$, you update your prior probability to get a posterior probability $P(A|B)$. That is in fact the foundation of Bayesian inference. Look it up if you want, but you won't need Bayesian inference for this course.\n\n$$\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n$$\n\n### **Application** \n\nTo understand how useful Bayes theorem is, let's use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\n\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\n1.  What is the probability that when the alarm is triggered the product is found to be flawless?\n\n2.  What is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\n\nWe should start by defining the events and event sets:\n\n$A$: product is faulty vs. $\\overline{A}$: product is flawless\n\n$B$: alarm is triggered vs. $\\overline{B}$: no alarm\n\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\n$P(B|A) = 0.97$ and consequently $P(\\overline{B}|A) = 0.03$\n\n$P(B|\\overline{A}) = 0.01$ and consequently $P(\\overline{B}|\\overline{A}) = 0.99$\n\n$P(A) = 0.04$ and consequently $P(\\overline{A}) = 0.96$\n\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is $P(\\overline{A}|B$ (1) and $P(A|B)$ (2) and we will need Bayes theorem to obtain those probabilities.\n\nLet's recall Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\nNow we can see, that we have all the information we need to compute the desired probabilities. $P(A|B)$ (2) is calculated by\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(0.97 * 0.04) / ((0.97 * 0.04) + (0.01 * 0.96))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8\n```\n:::\n:::\n\n\nand $P(\\overline{A}|B)$ (1) by\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(0.01 * 0.96) / ((0.01 * 0.96) + (0.97 * 0.04))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2\n```\n:::\n:::\n\n\nThese results show that in case the alarm is triggered, there is still a possibility of about 20% that the product is flawless and in only 80% of cases the product is faulty. Depending on your current method of quality assurance, these numbers might make you think more about buying the tool. And also note, how these numbers give a different perspective compared to what the manufacturer said.\n\n# Statistics\n\nNow we will talk about some statistical concepts, that are the foundation for modeling processes in both statistical and causal inference.\n\n## Random Variable\n\nFor starters, let's define what a random variable is. Often represented by letters such as $X$, a random variable has a set of values, also called sample space, of which any could be the outcome if we draw from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die( or continuous values (e.g. average height of random people).\n\n## Expected value\n\nBecause a random variable can take on different values, we cannot represent it as a scalar. However, the expected value of random variable is a scalar and represents something like a \"summary\" of the random variable with its values and its probability distribution.\n\nBefore we define the expected value, we need to introduce the summation operator $\\sum$, denoted by the Greek capital Sigma.\n\nIn general, it is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, $x_1, x_2, …, x_n$ to a shorter and more readable form\n\n$$\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n$$\n\nwith the arbitrary index of summation $i$ being the lower limit and $n$ the upper limit.\n\nBy basic math rules, the following simplifications are possible, where $c$ is a constant:\n\n$$\n\\sum_{i=1}^nc=nc\n$$\n\nand\n\n$$\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n$$\n\nA statement, that you will see a lot in many applications and formulas is the average of a sequence of numbers, typically denoted by a line over a variable. Some equivalent forms of writing down the calculation for the average are shown here:\n\n$$\n\\overline{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i \\\\ = \\sum_{i=1}^n \\dfrac{1}{n} x_i \\\\ = \\sum_{i=1}^n \\dfrac{x_i}{n} \\\\ =\\dfrac{x_1+x_2+\\dots+x_n}{n} \\\\ = n^{-1} \\sum_{i=1}^{n} x_i\n$$\n\nThis brings us to the first important basic concept we will need throughout the course, the *expected value*, also called *population mean*. The expected value of a random variable $X$ is defined as the weighted average of possible values the random variable can take. The weight is equal to the probability of the random variable taking a specific value.\n\nConsidering a finite list of potential values $x_1, x_2, …, x_k$ with probabilities $p_1, p_2, …, p_k$, the expectation of $X$ can be computed by\n\n$$\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n$$\n\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\n$$\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n$$\n\n::: callout-note\n| To replicate values and create a vector `rep()` can be used. It takes the value and number of times it should be replicated.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Vector of probabilities (all equal)\np <- rep(1/6, 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.5\n```\n:::\n:::\n\n\nAdditional rules regarding the calculation of expected values that can be useful are:\n\n$$\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n$$\n\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.\n\n## Conditional Expected Value\n\nThe conditional expected value is the expected value conditioned on some other value. Given the value $x$ of $X$, the expected value for $Y$ obtains as\n\n$$\nE[Y|X = x]\n$$\n\nand is a function of $x$. In other words, the conditional expected value is the best guess for $Y$ knowing only that $X=x$.\n\nAs a simple example, consider we take a representative random sample from the world population and want to compute the expected value for $height$. Denoting height with $Y$, the expected value for the whole population is the expected value $E[Y]$ over all individuals in your sample.\n\nThe conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.\n\n$$\nE[Y] \\neq E[Y|age < 10] \\neq E[Y|age >= 10]\n$$\n\n## **Variance**\n\nBefore we define variance, let's see why it is important to know it. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.\n\n::: callout-note\n| `seq(from, to, by)` or `seq(from, to, length.out)` returns a vectors with a sequence as specified by the arguments.\n:::\n\n::: callout-note\n| `rnorm(n, mean, sd)` samples values from the normal distribution. `n` specifies the number of values, `mean` and `sd` define the parameters of the normal distribution.\n:::\n\n::: callout-note\n| `map()` is a very useful function when you want to apply a function to each element of a list or a vector.\n:::\n\n::: callout-note\n| `pivot_longer()` changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need `pivot_wider()`.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](01_prob_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nMathematically, the *variance* is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The *sample variance* indicates how far a set of observed values spread out from their average value and is an estimate of the full *population variance*, that in most cases cannot be directly observed due to lack of data of the whole population.\n\nMathematically, the *population variance* is defined as\n\n$$\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n$$\n\nand the *sample variance* results as\n\n$$\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n$$\n\nYou might have noticed the term $(n-1)^{-1}$ is different from what you probably expected ($n^{-1}$). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\n\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.\n\nThe standard deviation obtains as the square root of the variance:\n\n$$\n\\sigma = \\sqrt{\\sigma^2}\n$$\n\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of $a$, then the variance will increase by $a^2$.\n\n$$\nVar(aX+b)=a^2V(X)\n$$\n\nYou can also conveniently compute a variance for the sum of two random variables\n\n$$\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n$$\n\nwhich in case of independence reduces to the sum of the individual variances due to the fact that $E(XY) = E(X)E(Y)$.\n\n## **Covariance**\n\nCovariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.\n\n::: callout-note\n| `as_tibble()` or `as.data.frame()` can be used to change e.g. matrices or lists into tables.\n:::\n\n::: callout-note\n| `rbind()` or `bind_rows()` are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are `cbind()` and `bind_cols()`.\n:::\n\n::: callout-note\n| To rename columns, you can use `rename(new_name = old_name)`.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  1.0  0.8  0.0\n[2,]  0.8  1.0 -0.5\n[3,]  0.0 -0.5  1.0\n```\n:::\n:::\n\n\nFrom left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_prob_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nIf the equality $E(XY) = E(X)E(Y)$ holds, then it implies a *covariance* of 0 between the variables $X$ and $Y$. *Covariance* is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.\n\nAs a matter of form, the formula for the covariance of the random variables $X$ and $Y$ is\n\n$$\nCov(X,Y) = E(XY) - E(X)E(Y)\n$$\n\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the *correlation* which can be derived from the covariance if the individual variances are known.\n\n$$\n\\text{Corr}(X,Y) = \\dfrac{C(X,Y)}{\\sqrt{V(X)V(Y)}}\n$$\n\nThe *correlation* is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.\n\n# Conclusion\n\nMany of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.\n\n# Assignments\n\n1)  Load data with the two variables $age$ and $income$.\n\n<!-- -->\n\na)  Compute\n\n-   expected value\n\n-   variance\n\n-   standard deviation\n\nfor both variables.\n\nb)  Then, examine the relationship between the two variables and compute covariance and correlation.\n\nc)  Also, compute the following expected means:\n\n-   $E[income|age <= 18]$\n\n-   $E[income|age \\in [18, 65)]$\n\n-   $E[income|age >= 65]$\n",
    "supporting": [
      "01_prob_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}