{
  "hash": "392f054bad1c75b9e41c5cbfd959c9bd",
  "result": {
    "markdown": "---\ntitle: \"Directed Acyclic Graphs\"\nlinktitle: \"Directed Acyclic Graphs\"\ndate: \"2022-10-01\"\noutput:\n  blogdown::html_page:\n    toc: true\nmenu:\n  example:\n    parent: Fundamentals\n    weight: 5\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Introduction\n\nWe have already learned that correlation and causation can easily be confused. Now, we will define concepts and acquire tools that help us in developing credible identification strategies to separate correlation from causation. One essential part is **graphically modeling your theoretical knowledge** about the data-generating process.\n\nIn causal inference, **directed acylic graphs** (**DAGs)** do the graphic modeling part. They are the foundation of any analysis strategy and moreover communicate your research plan.\n\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come things like:\n\n-   Domain knowledge\n\n-   State-of-the art theory\n\n-   Plausible assumptions and hypotheses\n\n-   Observations and experiences\n\n-   Conversations with experts\n\nA DAG should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\n-   Between what variables do you think is **a causal relationship**?\n\n-   Between what variables there is **no causal relationship**?\n\nBesides being helpful in guiding your analysis and identification strategy, DAGs also show your research design to your audience.\n\nA simple example of a DAG could be the effect of having an university degree on future salary: at first, it might be intuitive to say that future salary increases when you get an university degree.\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=75%}\n:::\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-2-2.png){fig-align='center' width=75%}\n:::\n:::\n\n\nBut zooming out and thinking about why a university degree is correlated with higher salaries could lead you to the idea that both university degree and salary are influenced by individuals' ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree.\n\nIt is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit.\n\n# Notation\n\nLet's have a look at a generic DAG to dive into the notation.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nFirst of all, a DAG presents a chain of causal effects captured by nodes and arrows. **Nodes depict random variables**, such as the treatment variable, control variables and the outcome variable and **arrows represent the flow of a causal effect** between two variables.\n\nIn a [**directed**]{.underline} acyclic graph, causal relationships run in one direction and they run forward in time. The direction is represented by an arrow: there is a an effect of $Z$ on $X$ and $Y$ and there is an effect of $X$ on $Y$.\n\nAlso, directed [**acylic**]{.underline} graphs are only useful in scenarios where the flow of causation runs in one direction. Graphs do no involve things as reverse causality or simultaneity. (Then, you could help yourself using time indices for the variables, but in general DAGs are used only for acyclic models.)\n\nNodes are ancestors or parents of another node when there is an arrow directing from the parent node to the child node: $Z$ is the parent node of its child node $X$ and so on. Instead of child node, you can also use the term descendant.\n\n# Three Types of Association\n\nMost of the time, DAGs are more complex than the example above. Even in that example, there are many more variables that could be included, like social environment, gender etc. But although in practice DAGs are more complex, they can be disassembled in building blocks that are easier to analyze.\n\nEffectively, there are only three different types of association we need to focus on:\n\n1.  **Chain**: $X \\rightarrow Z \\rightarrow Y$\n\n2.  **Confounder**: $X \\leftarrow Z \\rightarrow Y$\n\n3.  **Collider**: $X \\rightarrow Z \\leftarrow Y$\n\nKnowing their characteristics and idiosyncrasies allows us to identify valid strategies to estimate causal effects. Then, you know which variables you have to include in your analysis and which ones you have to leave out. Because in case you include or exclude the wrong variables, you will end up with a biased results and you are not able to interpret your estimate causally.\n\nIt is important to understand (conditional) (in-)dependencies between $X$, $Y$ and $Z$ with particular focus on **conditional independence**. What it exactly is, how it is defined, we will explain now going through the types of association mentioned above.\n\n## Chain\n\nOne element is a chain of random variables where the causal effect flows in one direction.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nAn example of such a causal mechanism (page 37, Pearl) could be the effect of work hours on training and training on race time. In the DAG, the variables would be:\n\n-   $X$: work hours\n\n-   $Z$: training\n\n-   $Y$: race time\n\nThis mechanism is also sometimes called mediation, because $Z$ mediates the effect of $X$ on $Y$.\n\nIn terms of dependencies,\n\n-   $X$ and $Z$: dependent, as indicated by the arrow.\n\n-   $Z$ and $Y$: dependent, as indicated by the arrow.\n\n-   $X$ and $Y$: dependent, as indicated by the arrow (going through $Z$).\n\n-   $X$ and $Y$ conditional on $Z$: independent, because when we condition on the training amount, that means we hold training amount fixed at a particular level, then there is no effect from work hours to race time as there is no direct effect, but only an effect through the amount of training. In other words, for individuals that differ in the hours they work but still have the same amount of training, there is no association between working hours and race time.\n\n**Rule:** *Two variables,* $X$ *and* $Y$*, are conditionally independent given* $Z$*, if there is only one unidirectional path between* $X$ *and* $Y$ *and* $Z$ *is any set of variables that intercepts that path.*\n\n## Fork\n\nAnother mechanism is the fork, also called common cause.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nThe reason it is called common cause is that, as depicted above, both $X$ and $Y$ are caused by $Z$.\n\nTo illustrate it, consider the following scenario: $Z$ represents the temperature in a particular town and $X$ and $Y$ represent ice cream sales and number of crimes in that same town, respectively.\n\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\n\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\n\nAgain, let's check in term of dependencies:\n\n-   $Z$ and $X$: dependent, as indicated by arrow.\n\n-   $Z$ and $Y$: dependent, as indicated by arrow.\n\n-   $X$ and $Y$: dependent, as both are influenced by $Z$. $X$ and $Y$ change both with variation in $Z$.\n\n-   $X$ and $Y$ conditional on $Z$: independent, as for a fixed level of temperature, there is no association anymore.\n\n**Rule:** *If variable* $Z$ *is a common cause of variables* $X$ *and* $Y$, and there is only one path between $X$ *and* $Y$, then $X$ *and* $Y$*are independent conditional on X.*\n\n## Collision\n\nThe last mechanism is the collision, which is also called common effect.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nIt is the reflection of the fork and both $X$ and $Y$ have a common effect on the collision node $Z$.\n\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n-   $X$ and $Z$: dependent, as indicated by arrow.\n\n-   $Y$ and $Z$: dependent, as indicated by arrow.\n\n-   $X$ and $Y$: independent, there is no path between $X$ and $Y$.\n\n-   $X$ and $Y$ conditional on $Z$: dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson's paradox.\n\nFor example, imagine the variables to be:\n\n-   $X$: attractiveness\n\n-   $Y$: talent\n\n-   $Z$: celebrity\n\nFirst of all, in the general population, there is no correlation between attractiveness and talent (3rd dependency). Second, being either attractive or having a talent will help you to become a celebrity (1st and 2nd dependency).\n\nBut what about the last dependency? Why are attractiveness and talent suddenly correlated when conditioned on e.g. being a celebrity? That is because when you know someone is a celebrity and has no talent, the likelihood that he/she is attractive increases because otherwise he/she would likely not be a celebrity. Vice versa, if you know someone is a celebrity and is not attractive, he/she is probably talented in some form.\n\n**Rule:** *If a variable* $Z$ *is the collision node between two variables* $X$ *and* $Y$, and there is only one path between $X$ *and* $Y$, then $X$ *and* $Y$ *are unconditionally independent but are dependent conditional on* $Z$ *(and any descendants of* $Z$)*.*\n\n# Causal Identification\n\nYou might wonder why we put such an emphasis on the dependencies in the previous sections. In fact, to isolate causal effects it is crucial to know how to interpret DAGs based on the dependence structure. Understanding what paths between what variables can bias effects is maybe the most important concept of causal inference.\n\nAgain, to understand the theory we will rely on simple examples and you will see a lot of similarities to the section above.\n\n## Confounding\n\nA very common problem when trying to prove causal effects is confounding.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nIt is the same DAG as in the introduction. So just recall and imagine $X$ was university degree, $Y$ future salary and $Z$ ability.\n\nThere are two paths from $X$ to $Y$:\n\n1.  **direct** path $X \\rightarrow Y$\n\n2.  **backdoor** path $X \\leftarrow Z \\rightarrow Y$\n\nFirst of all, it is important to clarify what effect we are actually **interested in** and that is the **direct causal effect** of university degree on future salary, $X$ on $Y$.\n\nThe **indirect effect** is not causal but is only **spurious correlation** induced by $Z$ which we are generally not interested in. It is called *backdoor path.*\n\nSo how do we proceed to extract that effect? We need to somehow remove the association between $X$ and $Y$ that is only due to variation in $Z$. And from the previous section we know how to do that. $X$ and $Y$ need to be **independent conditional** on $Z$ and **block the path** from $X$ to $Y$ over $Z$.\n\nBut what does blocking the path mean? It means that we have to condition on $Z$, to keep it at a fixed level. Then, the variations in $X$ that cause $Y$ to vary are not due to $Z$ because it does not vary at all and cannot have an impact on either $X$ or $Y$. Doing that we **closed the backdoor** and are able to retrieve the causal effect.\n\nNot blocking the path would falsify our results and is what is called the omitted variable bias. That is why $Z$ is called **confounder**, because it **confounds the ability to measure the causal effect**.\n\nHowever, the main problem is that in many cases you might not be able to block the path for two different reasons:\n\n-   You are aware of the confounder, but you did not collect data for it\n\n-   You are not aware of the confounder (and probably did not collect data for it)\n\nThis stresses the importance of theoretical knowledge about the phenomenon you are researching. Without it, it is very unlikely that you can prove truly causal effects. The risk of not paying attention to confounders gets clear when we look at graphs visualizing an example of Simpson's paradox: accounting for a third variable reverses the sign of correlation.\n\nWe can illustrate it with an imaginary example. Let's assume you want to measure how a specific characteristic affects salary. So you start to collect data about both variables, throw them into a regression and your result tells you that there is a positive correlation. But what happens if you take the education level into account? You can see how the lines show a positive correlation on the left and a negative correlation on the right. When you include a third variable, the relationship reverses.\n\nIt is neither wrong or right to always include or exclude variables, but it depends on the application and question you want to answer. This is why causal reasoning is so important.\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Simpson's paradox ----\nn <- 1e+03\neducation <- rbinom(n, 2, 0.5)\ncharacter <- rnorm(n) + education\nsalary <- education * 2 + rnorm(n) - character * 0.3\n\nsalary <- sample(10000:11000,1) + scales::rescale(salary, to = c(0, 100000))\ncharacter <- scales::rescale(character, to = c(0, 7))\neducation <- factor(education, labels = c(\"Low\", \"Medium\", \"High\"))\n\ndf <- tibble(\n  salary,\n  character,\n  education\n)\n\n# Not conditioning on education\nsimps_not_cond <- ggplot(df, aes(x = character, y = salary)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on education  \nsimps_cond <- ggplot(df, aes(x = character, y = salary, color = education)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsimps_not_cond\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=75%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nsimps_cond\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-14-2.png){fig-align='center' width=75%}\n:::\n:::\n\n\n## Collider\n\nYou probably noticed that the confounding example was related to the common cause in the previous section. The next example is related to the common effect mechanism.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nNow consider the following example: a research group wants to examine if there is a causal relationship between grade point averages ($X$) and musical talent ($Y$). They can use data from their own university where both is measured for every student as scoring high in one of these characteristics substantially increases your chance of being accepted.\n\nNow the researchers, who hypothesized a positive or no correlation between $X$ and $Y$, perform a simple analysis and, to their surprise, find out that there is a strong negative correlation. This strange result can be explained by collider bias. Implicitly, as they only used data from their own university students, they also conditioned on a collider $Z$, which is \"accepted to university\" and is fixed on a constant level of actually being accepted to the university.\n\nAgain, we have two paths from $X$ to $Y$, but one thing is different: the **backdoor path is already closed** because $Z$ is a collider, both arrows point toward it.\n\nTo correct their results they would need to have data from the whole population and not only students from a university that puts focus on either grade point averages or musical talent. Having a full population as a sample would probably lead to the result that there is no correlation between $X$ and $Y$.\n\nThis case (collider) is different to the case before (confounding), where conditioning was the correct solution. But **if you have a collider in you DAG, make sure not to condition on it** as it creates a dependence between $X$ and $Y$. You \"open\" a backdoor path that was closed before, just due to the presence of the collider.\n\nCollider bias often arises when your sample is not very representative of the population you are making claims about. How that could change your result can be seen in an example Berkson's paradox. While there is no correlation for the whole population, for smaller subgroups there are. And therefore, it is crucial that you clearly state what effect you are interested in.\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\n# Berkson's paradox ----\nn <- 1e+03\nability     <- rnorm(n)\nmotivation  <- rnorm(n)\naptitude    <- 1/2 * ability + 1/2 * motivation + rnorm(n, 0, .1)\n\ndf <- tibble(\n  ability    = ability,\n  motivation = motivation,\n  aptitude   = aptitude,\n  student    = ifelse(aptitude > 0, \"student\", \"no_student\")\n)\n\n# Not conditioning on student\nberk_not_cond <- ggplot(df, aes(x = motivation, y = ability)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on student  \nberk_cond <- ggplot(df, aes(x = motivation, y = ability,\n                            color = student, \n                            alpha = student)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_color_manual(values = c(\"student\" = ggthemr::swatch()[4],\n                                \"no_student\" = ggthemr::swatch()[5])) +\n  scale_alpha_manual(values = c(\"student\" = 1, \"no_student\" = 0.2)) +\n  theme(legend.position = \"right\")\n\n# Plot both\nberk_not_cond\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=75%}\n:::\n\n```{.r .cell-code}\nberk_cond\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-18-2.png){fig-align='center' width=75%}\n:::\n:::\n\n\n# Simulating interventions\n\nAs you could see in the previous sections, DAGs help us to find out, in a non-technical but graphical way, how we are able to isolate the causal estimate we are interested in. And not explicitly stating our understanding of the data-generating process can easily lead us to wrong conclusions.\n\n## d-separation\n\nOne concept, that we have not named yet but implicitly used, is **d-separation**. If an effect of $X$ on $Y$ is d-separated, there is no statistical association that can flow between $X$ and $Y$ except for the direct effect. In fact, d-separation determines conditional independence.\n\nD-separation formalizes what we have already learned when going through the tree types of association.\n\nPractically (<http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html>) , it is easier to define the opposite, **d-connection**:\n\n-   **Rule 1:** unconditional separation: $X$ and $Y$ are d-connected if there is an unblocked path between them. (As an example, imagine a confounder, that is not conditioned on.)\n\n-   **Rule 2:** blocking by conditioning: $X$ and $Y$ are d-connected, conditioned on a set of $Z$ nodes, if there is a collider-free path between $X$ and $Y$ that traverses no member. (Think of a mediated effect that takes away parts from the direct effect.)\n\n-   **Rule 3**: conditioning on colliders: If a collider is a member of conditioning set $Z$, or has a descendant in $Z$, then it no longer blocks any path that traces this collider. (Image the collider example in the previous section.)\n\n!!! MAYBE CHANGE TO [https://de.wikipedia.org/wiki/D-Separation](https://de.wikipedia.org/wiki/D-Separation#:~:text=d%2DSeparation%20ist%20ein%20Begriff,die%20Separation%20in%20ungerichteten%20Graphen.)\n\nKnowing these rules and having mapped our assumptions into the DAG allows us to treat observational data like experimental data and simulate interventions as we would have conducted an experiment. However, it is not a silver bullet as in many cases data availability will stop you from isolating the causal effect. For example, if you do not observer a confounder, you cannot control for it. All empirical work requires theory and with observational data we need to be extra careful to make sure to actually extract the effects we are interested in.\n\n## Backdoor/Frontdoor Criterion\n\nSpecial cases that are derived from d-separation rules are the backdoor and frontdoor criterion/adjustment.\n\nTo satisfy the backdoor criterion, we have to make sure all backdoors are closed, which, as already mentioned, differs for confounders and colliders.\n\n![](/images/04_conf_coll_bias.png){fig-align=\"center\"}\n\nAgain, we want to block all other paths between $X$ (treatment) and $Y$ (outcome). So depending on the structure of the DAG, the following can block a path:\n\n-   a chain or a fork whose middle **node is in** $Z$\n\n-   a **collider** that is not conditioned on, which means it is **not in** $Z$\n\nThe frontdoor criterion, which is actually a consecutive application of the backdoor criterion, is a bit more complicated and we will leave it out for now, but in a the section about instrumental variables we will deal with it extensively.\n\n## Algorithms to identify causally valid estimates\n\nIf DAGs become more complex because there are a lot of variables that are somehow related, we can make use of algorithms to check all the rules for us.\n\nOne application to help in such cases is [http://](http://dagitty.net/dags.html#){.uri}[dagitty](http://dagitty.net/dags.html# \"dagitty\"){.uri}[.net/dags.html](http://dagitty.net/dags.html#){.uri}, where you can draw your DAG, define what is the treatment and outcome, which variables are observed and unobserved and many other things. Then it will show you what kind of adjustment is necessary to estimate the causal effect of interest.\n\n![](/images/04_dagitty_example.png){fig-align=\"center\" width=\"350\"}\n\n**Try to build this graph on dagitty.net and look what useful information you can get from the site.**\n\n`dagitty` is also implemented in R and combined with `ggdag` you are also able to plot your DAGs in a easy manner and obtain information needed for designing your research.\n\nFirst, let's just plot the DAG. When we define exposure, which is a different term for intervention or treatment, and the outcome, they are highlighted in another color.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(dagitty)\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model <- 'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nD -> Y\nD -> Z3\nZ1 -> D\nZ1 -> Y\nZ2 -> Y\nZ2 -> Z3\nZ3 -> Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n\nLet's check what paths there are from treatment $D$ to $Y$. Of course, there is a direct path (the causal path) and there are two other paths, of which one is closed.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# find all paths\npaths(dag_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$paths\n[1] \"D -> Y\"             \"D -> Z3 <- Z2 -> Y\" \"D <- Z1 -> Y\"      \n\n$open\n[1]  TRUE FALSE  TRUE\n```\n:::\n:::\n\n\nWe can also plot the open paths. One path is already blocked by a collider (remember: we do not want to open that path).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot paths\nggdag_paths(dag_model) +\n  theme_dag_cds()\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nTo see what we have to adjust for to isolate the causal effect, we use `adjustmentsSets()`. As you might have figured out already, it is $Z1$ that needs to be conditioned on.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ Z1 }\n```\n:::\n:::\n\n\nA very concise summary plot is returned by the function `ggdag_adjustment_set()`, which shows what needs to be adjusted, the open paths and the whole DAG.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot adjustment sets\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n```\n\n::: {.cell-output-display}\n![](04_dag_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nBut what does it actually mean in practice? How do we block a path or condition on a variable?\n\nIf you use a linear regression, including a variable as an independent variable is the sames as conditioning on it. If you use other models, you might have to use subsets and average them or do some kind of matching where you only compare units that have the same value for the variables that has to be conditioned on.\n\nIn the following chapters, we will deal with a large variety of techniques and figure out clever ways to isolate causal effects.\n\n# Conclusion\n\nWe have learned that when we want to talk about causality, atheoretical approaches are likely to fail and assumptions have to be made and shown. DAGs are very powerful in depicting the relationships according to our theory and help to develop a valid research design.\n\nImplicitly, DAGs always base on some form of counterfactual reasoning as we presuppose that we could change the values for the treatment variable by simulating an intervention. In that sense, DAGs give an answer to the fundamental problem of causal inference.\n\n# Assignment\n\n1)  Think about example from previous chapter (parking spots) and draw DAG\n\n2)  Load the data and compute what the click-through-rate (CTR, clicks / impressions) is for male and female. Then, compute the CTR for male and female on country level. Then, check what is surprising about the CTRs and explain how that could happen.\n",
    "supporting": [
      "04_dag_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}