{"title":"Regression and Statistical Inference","markdown":{"yaml":{"title":"Regression and Statistical Inference","linktitle":"Regression and Statistical Inference","date":"2022-11-23","output":{"blogdown::html_page":{"toc":true}},"menu":{"example":{"parent":"Fundamentals","weight":3}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"headingText":"custom ggplot theme","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\n# **Introduction**\n\n**Statistical inference** aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores and a lot of other business-related units.\n\nIn practice, it is often impossible to collect data about the whole population, which is why we **draw** (ideally random) **samples from the whole population and use statistical inference to draw conclusions about the whole population** using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter.\n\n# Linear Regression\n\nIt is undisputed that the **most important method in statistical inference** is the **linear regression**, which has been used extensively in practical and scientific applications due to its desirable mathematical properties and its convenient interpretation.\n\nIn this course, we will use linear regression in combination with other methods to explain and interpret relationships and effects between random variables, in general: how does changing one variable affect another variable? There are also other purposes that linear regression could be used for, namely as prediction method. However, prediction is not the focus of this course and in fact, there are actually many other methods that surpass linear regression for prediction tasks. On the other hand, for explanation and interpretation purpose, which are fundamental for causal reasoning, linear regression is still one of the sophisticated methods.\n\nIt builds upon many of the statistical concepts that were explained in the previous chapter, such as conditional expectations, variance, covariance, correlation etc. Formally, we want to explain an outcome $Y$ (also called response or dependent variable) using an explanatory variable $X$ (also called predictor or independent variable). A popular example, that has been studied extensively, is to explain the effect of smoking on lung cancer. Here, smoking is the explanatory variable $X$ and lung cancer $Y$ is the outcome.\n\n![Cause and Effect in Regression](/images/02_xy.png){fig-align=\"center\" width=\"470\"}\n\nLet's use a simple example to show how linear regression works and how we can implement it in R. First, we consider a model with one explanatory variable:\n\n$$\ny=\\beta_0+\\beta_1x+u\n$$\n\nThis equation represents the so called population model and the coefficients are never actually observed. We usually only have a small sample of the population and having this sample and making a few assumptions we will try to estimate values for the parameters of interest, $\\beta_0$ and $\\beta_1$. When dealing with causal models, $y$ is thought of as effect and $x$ as cause. $\\beta_0$ is called the intercept while $\\beta_1$ is known as the coefficient of the slope.\n\nAlso note, that the model does not rule out other causes by including an error term $u$, that contains all (unobserved) factors potentially affecting $y$. As we will visualize below, in the whole population, the expected value of this term is assumed to be zero (it does not mean it is zero for everyone).\n\n$$\nE(u) = 0\n$$\n\nHowever, this is a rather trivial assumption as it is always possible for the error term to be zero by balancing the terms $u$ and the intercept $\\beta_0$. An assumption that is way more important in a statistical sense and in order to interpret parameters as causal parameters in the following sections, is the mean independence of $u$ from $x$, which states that\n\n$$\nE(u\\mid x)=E(u)\\ \\text{for all values $x$}\n$$\n\nIn many cases, this assumption is violated, as unobserved factors contained in $u$ are correlated with observed factors in $x$. However, as the first assumption is always fulfilled by definition of linear regression algorithms, there is a high risk of not detecting the failure of second assumption, which in turn leads to biased results. Therefore, it is crucial to always argue why the assumption holds in a particular application.\n\nBoth assumptions can be combined into one assumption, known as the zero conditional mean assumption\n\n$$\nE(u\\mid x)=0,\\ \\text{for all values $x$}\n$$\n\nyielding the conditional expectation function of $y$ being a linear function of $x$\n\n$$\nE(y\\mid x)=\\beta_0+\\beta_1x\n$$\n\nwith $u$ being left out as it is zero on average.\n\n# Estimation\n\n## R\n\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in `R`. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\n\nBefore we start, we load the `R` package `tidyverse`, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\n```{r warning=FALSE, message=FALSE}\n# Load tidyerse package\nlibrary(tidyverse)\n```\n\nWe create a tibble, which is a table containing data, with two columns, $x$ and $y$. For $x$, we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable $y$, we make dependent from $x$, i.e. for each unit $i$, we create a $y_i$ as\n\n$$\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n$$\n\nwhere $\\epsilon_i$ is random noise.\n\nLet's have a look at how the data is presented. We have a table containing 100 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n```{r}\n# Simluate data\n\n# number of observations\nn <- 10\n\n# Create tibble\nlm_dat <- tibble(\n  # draw from normal distribution\n  x = rnorm(n, mean = 3, sd = 1),\n  # y depends on x and noise from normal distribution\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\n# Show data\nlm_dat\n```\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n```{r}\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8)\n```\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between $X$ and $Y$. However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\n\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\n\nLet's have a look at some random lines.\n\n```{r}\n#| echo: false\n\n# Function to create a random line\nrline <- function(){\n  tibble(x = c(min(lm_dat$x), max(lm_dat$x)),\n         y = c(runif(1, -0.3, 0.3) + min(lm_dat$y),\n               runif(1, -0.3, 0.3) + max(lm_dat$y)))\n}\n\n# Create 10 random lines\nrlines <- map(1:10, ~rline())\n\n# Change line color\nline_color <- ggthemr::swatch()[1]\n\n# Scatter plot of x and y with random lines\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  # LM line\n  geom_line(stat = \"smooth\",\n            method='lm',\n            formula= y~x,\n            alpha = .5,\n            se = F, \n            color = line_color,\n            linewidth = 1) +\n  # Random lines\n  geom_line(data = rlines[[1]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[2]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[3]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[4]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[5]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[6]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[7]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[8]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[9]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[10]], color = line_color, alpha = .5, linewidth = 1)\n```\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the residuals are zero.\n\n```{r}\n#| echo: false\n\n# Scatter plot of x and y with random lines\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  # LM line\n  geom_line(stat = \"smooth\",\n            method='lm',\n            formula= y~x,\n            alpha = .9,\n            se = F, \n            linewidth = 3) +\n  # Random lines\n  geom_line(data = rlines[[1]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[2]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[3]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[4]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[5]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[6]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[7]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[8]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[9]], color = line_color, alpha = .5, linewidth = 1) +\n  geom_line(data = rlines[[10]], color = line_color, alpha = .5, linewidth = 1)\n```\n\nThe resulting line is highlighted in blue.\n\nIf we want to mathematically compute the line in R, we have to use the `lm()` function and provide data and the assumed functional relationship as arguments. `lm()` is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: `lm_mod`), which we can interpret best when using `summary()` to show the resulting coefficients and other statistical information.\n\n::: callout-note\n| `lm()` is a function that fits a linear model. You have to provide data and a regression equation in the form of for example `outcome ~ regressor_1 + regressor_2` or `outcome ~ .`, if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call `summary()`.\n:::\n\nLooking at the regression summary, we see that the line is modeled by $y = -0.1918 + 0.3354*x$. It means that for the fitted model, an increase of one unit in $x$ is related to an 0.3354 increase in $y$. That is relatively close to what we simulated (0.3) and deviates due to the added random noise.\n\n```{r}\n# Fit model and print summary\nlm_mod <- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n```\n\nNow, let's check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations $y_i$ and predictions $\\hat{y_i}$ for each $i$ or plotting the residuals $r_i = y_i - \\hat{y_i}$ and comparing it to the $x$-axis.\n\n```{r}\n#| layout-nrow: 1\n\n# Add fitted values and residuals to data\nlm_dat_fit <- lm_dat %>% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")\n```\n\n## Math\n\nMathematically, the best line is found by the ordinary least squares (OLS) method.\n\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\n\nGiven $n$ samples of observed pairs of dependent and independent variables $\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}$, we plug any of them into the equation\n\n$$\ny_i=\\beta_0+\\beta_1x_i+u_i\n$$\n\nand together with our assumptions $E(u) = 0$ and $E(u|x)=0$ we obtain the equations to be solved to retrieve estimates for $\\beta_0$ and $\\beta_1$.\n\nFrom the independence of $x$ and $u$ and our understanding of probabilities and expectations, we also know that the expected value of the product of $x$ and $u$ has to be zero: $E(xu)=0$. Substituting $u$ with $y-\\beta_0-\\beta_1$, we obtain the two conditions that when being solved give us the optimal estimates for our $\\beta$ parameters.\n\n$$\n\\begin{align}\nE(y-\\beta_0-\\beta_1x) = E\\Big(x[y-\\beta_0-\\beta_1x]\\Big) = 0\n\\end{align}\n$$\n\nTranslated into its sample counterpart:\n\n$$\n\\begin{align}\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\end{align}\n$$\n\nLooking at the sample equations, we know our sample size $n$, our sampled values $y_i$ and $x_i$. The coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\n\nSkipping a few transformation steps, we obtain\n\n$$\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n$$\n\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our $\\beta_1$ is defined as the covariance of $X$ and $Y$ divided by the variance of $X$. It also shows that the variance of $X$ has to be greater than zero, which means that not all values of $x_i$ can be the same. You need to observe different values to be able to estimate how $y_i$ reacts to $x_i$.\n\n$\\beta_0$ follows directly by plugging $\\beta_1$ into $\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta}1\\overline{x}$*.* A bar above a variable always represents the sample value of that particular variable. Thus,$\\beta_0$ is (as expected) constant and linear in $\\beta_1$.\n\nKnowing the equation for the regression line, we can compute fitted values $y_i$ for all $i$\n\n$$\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n$$\n\nIn almost all cases however, $\\widehat{y}_i$ won't be equal to $y_i$ but there will be a prediction error, commonly referred to as residual $\\widehat{u}_i$. Make sure that you don't mix it up with $u$, the error term, which is always unobserved.\n\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\n\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\n\n::: callout-tip\n## Info\n\nSquares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n:::\n\n$$\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n$$\n\nAgain, most of the residuals won't be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average.\n\n# Application\n\nNow let's use a more complex model to learn more about how results of a linear regression can be interpreted and how statistically significant results are defined.\n\nIn this application, imagine you are working at a health insurance company and your task is to estimate the annual expected cost of a (potential) customer based on his/her characteristics. Based on historical data, you want to see what characteristics affect the cost.\n\nLet's create a synthetic data set for our application. In practice, you obviously would not need to create the data first but we have to due to the lack of real data which is generally kept under lock by insurance companies. However, the advantage of using synthetic data is that we know the data-generating process and check if our estimates are correct or if they are are biased.\n\nYou don't need to worry about how to create a synthetic data set at this point. But for those who are interested, the package `fabricatr` has some very useful functions and a very well written documentation.\n\nWe simulate a couple of factors, of which some do and some does not affect the expected cost of a customer. The factors we simulate are sex, age, BMI, children, smoking, region and income.\n\n```{r}\n#| code-fold: true\n\n# [0] Data ----\n# Simulate data for a multivariate linear regression. Introduction, therefore\n# disregarding causality of effects.\n# Application: Health insurance company wants to estimate expected annual cost \n# of potential customer based on various characteristics such as sex, age, BMI,\n# children, smoking status, region, salary\n\n# empirical distributions to draw from\nincome_sample <- 1e+4 * rbeta(1e+6, 2, 5)\nbmi_sample <- 12 + 50 * rbeta(1e+6, 2, 5)\n\n# custom function to generate correlated variables based on given variable\n# and empirical sample\ndraw_custom <- function(data, quantile_y) {\n  quantile(data, quantile_y)\n}\n\nlibrary(fabricatr)\n# sex, age, BMI, children, smoking status, region, salary\ndf <- fabricate(\n  N = 1e+3,\n  # randomly assign a region\n  region = draw_categorical(prob = rep(0.25, 4), \n                            N = N,\n                            category_labels = c(\"N\", \"E\", \"S\", \"W\")),\n  # randomly assign sex\n  sex = draw_binary(0.5, N = N),\n  # randomly assign smoking (yes/no)\n  smoking = draw_binary(prob = 0.25, N = N),\n  # age distribution (18-70): younger or older people would not sign up for an\n  # insurance as they are either already insured or insured via their parents\n  age = (100*rbeta(1000, 1.4, 1)) %>% scales::rescale(to = c(18, 70)),\n  # generate income based on age and income sample distribution\n  income = correlate(draw_custom,\n                      data = income_sample,\n                      given = age,\n                      rho = 0.4),\n  # generate BMI based on age and BMI sample distribution\n  bmi = correlate(draw_custom,\n                  data = bmi_sample,\n                  given = age,\n                  rho = 0.3),\n  # generate children (yes/no) based on age\n  children = draw_ordered(x = rnorm(N,age, sd = 20), breaks = c(35)) - 1,\n  \n  # compute expected cost based on income, smoking, bmi and age\n  expected_cost = 100 + -0.05*income + 200*smoking + 2*((bmi-30) + (bmi-30)^2) +\n    10*age + rnorm(N, 100, sd = 100)\n  ) %>%\n  as_tibble() %>%\n  select(-ID)\n```\n\nLet's also save the data.\n\n```{r}\n#| eval: false\n\n# Save data set\nsaveRDS(df, \"../../datasets/health_ins.rds\")\n```\n\nIf you don't want to create the data yourself, you could instead download it from [here](https://cloud.tuhh.de/index.php/s/owg5GLR9FoY9Xgm). All data sets that you are going to need for replicating examples and solving the assignments is stored there.\n\nLoading data is generally referred to as \"reading\" the data and can be done for a variety of formats. One format, that is particularly convenient in `R` is the `.rds` format. An `.rds` file can be read with `readRDS(path)` and saved with `saveRDS(df, path)`. Note that the `path` argument depends on where you want to save an object or where you want to load it from. It is probably different from the path I used (by the way, `..` moves you a one folder upwards).\n\n::: callout-note\nReading/saving commands for other file formats are for example\n\n-   `read_csv()` and `write_csv()` from the `readr` package\n\n-   `read_delim()` and `write_delim()` from the `readr` package\n:::\n\n```{r}\n#| eval: false\n\n# Read data set. Check what folder/path your data is in. \ndf <- readRDS(\"../../datasets/health_ins.rds\")\n```\n\nOften, you will deal with large data sets that do not fit on a single page and thus are not easy comprehensible. To still get a good understanding, there are some functions to get an idea of how your data looks like.\n\nMost data scientists start by using the `head()` command, which returns the first part of the table and shows you the data structure. Also, if something is wrong with your data, because you did not load it correctly or headers do not fit the data, you were able to see it here already.\n\nHere, we see that our table consists of different column types, such as integers, doubles and factors.\n\n```{r}\n# Show first lines\nhead(df)\n```\n\nAn option not available in RStudio, but in documents like this one here, is to view a paged table by just typing in the name of the object. In RStudio, the best way to look at a table is `View()` or `view()`, which opens the table in a readable format.\n\n```{r}\n# Show table\ndf\n```\n\nIf you have too many columns to fit on one page, you could also use `glimpse()`, which lists columns and their first values row by row.\n\n```{r}\n# Get glimpse of data\nglimpse(df)\n```\n\nSimilar to `head()`, it returns the general structure and data types.\n\nA command yielding a more concise description of the data is `summary()`, that besides data types also returns summary statistics. For factor (= categorical) variables, it counts the occurrences and for numeric variables (discrete, continuous) variables, it returns minimum/maximum values, median/mean and quartiles. For binary variables, the only value of interest is generally the mean, which is equal to the proportion.\n\nWhen we take a look at a summary of our data set, we see that it includes a variety of data types: Variables are continuous, discrete, binary/logical or even categorical/factors. Continuous variables or discrete variables have numeric values, which we have already seen in the previous example, are easy to work with when doing the needed calculus. But what about the other types?\n\nLogical variables, which take either the value `TRUE` or `FALSE` are in fact discrete, or to be more precise binary values. So you can change the value of `TRUE` to 1 and `FALSE` to 0. Then, they behave like discrete variables.\n\nFor categorical variables, in R they are called factors, it is a bit more complicated. If they follow a natural order e.g. like *low, medium, high*, they could be coded as discrete variables as well with e.g. the values $0,1,2$.\n\n```{r}\n# Show summary statistics: various types of variables (continuous, discrete,\n# binary/logical, categorical/factor)\nsummary(df, digits = 3)\n```\n\nLet's also check how the variables are correlated with each other. In case variables were fully linearly dependent, we could already see it here. Then, we were not able to run a linear regression or at least it would be meaningless.\n\n::: callout-note\n| To compute correlation, variance and covariance, respectively, you can use `cor()`, `var()` and `cov()`.\n:::\n\n```{r}\n# Show correlation\ndf %>%\n  select(-region) %>% \n  cor() %>% \n  round(2) %>% \n  Matrix::tril()\n```\n\nOther than that, compared to our minimalist example from the previous section, there is not much we need to consider when fitting a multiple linear regression model. In fact, using the most popular `lm()` command for regression, we don't even need to transform categorical variables ourselves. It is done automatically.\n\n## Interpretation\n\nSo let's run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. We will see in later chapters why that might be dangerous.\n\n```{r}\n# Include all potential regressors\nlm_all <- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n```\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\n\nThe estimate is what we know as regression coefficient from before, typically denoted by $\\widehat{\\beta}_i$ or other Greek letters. As it is the estimated version, it has a hat. The estimated for $\\beta_i$ It tells you by how much the dependent variable $\\widehat{y}$ varies when a particular variable is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\n\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect of $x_i$ **ceteris paribus**, Latin for \"the others equal\". It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as **controlling for the other variables**.\n\n## Statistical Significance\n\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate.\n\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error.\n\nMost widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data. The null hypothesis states that there is no correlation between the dependent variable $y$ and the independent variable $x_1$. The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis. Here, the alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable.\n\nStatistical significance can directly be derived from the p-value and an arbitrary significance level $\\alpha$. However, the most widely used level of $\\alpha$ is $0.05$. Less often used are levels of $.1$, $.01$ or $.001$.\n\nAn estimate with a p-value less than $\\alpha$ is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level $\\alpha$. Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that $age$, $income$, $bmi$, and $smoking$ are statistically significant (at different levels though, indicated by the number of stars).\n\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A $(1-\\alpha)$ confidence interval has a probability of $(1-\\alpha)*100 \\%$ to contain the true value of our estimated coefficient. That means, if we would sample $100$ times, $\\beta_i$ would be contained in the sample $(1-\\alpha)*100$ times.\n\n```{r}\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n```\n\n```{r}\n# alpha = 0.9\nconfint(lm_all, level = 0.90)\n```\n\nAn estimate whose interval is either completely positive or completely negative is *different from zero* and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the positive coefficient.\n\n## Model selection\n\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\n\nLooking at the spread of residuals ($\\widehat{u}_i = y_i - \\widehat{y}_i$) we want them to be spread evenly around zero.\n\n```{r}\n# Plot histogram of residuals\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n```\n\nWe can see that the residuals are in fact almost normally distributed.\n\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit is $R^2$. It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\n$$\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n$$\n\nConveniently, $R^2$ is always between 0 and 1 and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high $R^2$ can even point to a biased model while a model with a low $R^2$ can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of $R^2$ does also not change the interpretation of the regression coefficients.\n\nA problem with $R^2$ is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the $\\text{Adjusted} \\, R^2$. It includes a term for the number of independent variables used.\n\n$$\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n$$\n\nwhere $n$ is the sample size and $p$ the number of independent variables. This way, you can compare models and account for their scarcity.\n\nLet's build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n```{r}\n# Include only significant regressors\nlm_imp <- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n```\n\nExcept for $bmi$, coefficients are very similar. We'll look into that in just a second. But first let us compare both models with regard to $\\text{(Adjusted)} \\, R^2$.\n\n| To elegantly print variables in a specified format, you can use `sprintf()`.\n\n```{r}\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n```\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike's Information Criteria (AIC) and Bayesian Information Criteria (BIC). Both AIC and BIC penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n```{r}\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n```\n\nFor both criteria, the model with the lowest value is preferred.\n\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\n\nFor example, is it correct to assume a linear relationship between $bmi$ and the outcome $expected\\_cost$? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\n\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n```{r}\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n```\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term \"linear\", so what can we do about it?\n\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. $bmi^2$.\n\n```{r}\n# Include quadratic term for BMI\nlm_sq <- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n```\n\nFrom the summary, we can see that including the square term significantly improves the model fit.Check for yourself the metrics $R^2$ and $AIC$ and plot the histogram of residuals.\n\nWe can take from it, that it is extremely important to rely on theoretical considerations when building models.\n\n## Assumptions\n\n1.  **Linearity**: Relationship between $X$ and $Y$ is linear.\n\n2.  **Homoscedasticity**: Variance of residual is the same for any value of $X$.\n\n3.  **Independence**: Observations are independent of each other. Residuals are independent of each other.\n\n4.  **Normality**: For any fixed value of $X$, $Y$ is normally distributed. Residuals of the model are normally distributed.\n\n# Assignment\n\nIn the [data folder](https://cloud.tuhh.de/index.php/s/owg5GLR9FoY9Xgm) you find a data set `car_prices.rds`. It is slightly modified of the data set you could also find on [Kaggle](https://www.kaggle.com/datasets/hellbuoy/car-price-prediction?datasetId=383055), the largest data science community worldwide hosting published code and data for a whole variety of applications. Check it out if you are interested!\n\nAgain, open a new `.R` script/file and name it `02_reg.R`. Then, you should accomplish the following tasks.\n\n::: callout-tip\nYou can copy parts of the code from the example. Also keep in mind, that not all functions are built in base `R` but you might have to load a package.\n:::\n\n1.  Read the data and check the dimensions. How many rows and how many columns does the data have? You could use e.g. the `dim()` command.\n\n2.  Use appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\n3.  Run a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\n4.  Choose one regressor and\n\n    1.  explain what data type it is and what values it can take on\n\n    2.  what effect is has on the price and what changing the value would have as a result\n\n    3.  whether its effect is statistically significant.\n\n5.  Add a variable $seatheating$ to the data and assign a value `TRUE` for all observations. You can use e.g. `df %>% mutate(new_variable = value)`. Assign it to a new object and run a regression. What coefficient do you get for the new variable $seatheating$ and how can you explain it?\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"blogdown::html_page":{"toc":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"02_reg.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":["darkly","../../theme-darkly.scss"],"mainfont":"arial","linestretch":1.7,"title":"Regression and Statistical Inference","linktitle":"Regression and Statistical Inference","date":"2022-11-23","menu":{"example":{"parent":"Fundamentals","weight":3}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}}}