{"title":"Matching and Subclassification","markdown":{"yaml":{"title":"Matching and Subclassification","linktitle":"Matching and Subclassification","date":"2022-11-23","output":{"blogdown::html_page":{"toc":true}},"menu":{"example":{"parent":"Toolbox","weight":7}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"headingText":"custom ggplot theme","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\n# Problem\n\nAlmost always, the problem we are trying to solve in causal inference relates to the **fundamental problem of causal inference**, the fact that we cannot observe two states for one particular observation unit, e.g. we cannot see how a person's health status changes after taking a specific drug and after not taking the same drug. Consequently, we cannot know the individual treatment effect.\n\nThus, we said we can make use of averages and try to estimate the average treatment effect by taking the difference between a group of treated observation units and a group of untreated observation units. Written in a formula, we can present it as\n\n$$\nATE = E[Y_1| D = 1] - E[Y_0 | D = 0] \n$$\n\nAssuming that individuals (or other kinds of observation units) were randomly assigned for example to take the drug, then this result gives us exactly what we want. It compares the outcomes of groups under treatment with groups not under treatment (control).\n\nBut the estimate hinges on the assumption that both groups are comparable, formally\n\n$$\nE[Y_0|D=0] = E[Y_1|D=0]\n$$\n\nwhich we cannot test but in randomized settings we have good reason to believe it to be true. Under these circumstances, average treatment effect ($ATE$) and the average treatment effect on the treated ($ATT$) are equal.\n\nHowever, **if there are underlying group differences** because the treatment assignment was not randomized and e.g. individuals were able to choose their treatment, **we are not measuring the estimate that we are interested in**. Then, the difference between $ATE$ and $ATT$ is what we call *selection bias*, an unmeasured factor representing systematic bias:\n\n$$\nATE = ATT + \\text{selection bias}\n$$\n\nGraphically, we can show an example of when the naive estimate would fail.\n\n```{r}\n#| echo: false\n#| message: false\n#| fig-cap: \"Effect of D on Y is confounded by variation in Z. In other words, there are two paths from D to Y, one direct path and a backdoor path via Z.\"\n\nlibrary(tidyverse)\nlibrary(ggdag)\nlibrary(dagitty)\n\nconfounder <- dagify(\n  D ~ Z,\n  Y ~ Z,\n  Y ~ D,\n  coords = list(x = c(Y = 3, Z = 2, D = 1),\n                y = c(Y = 0, Z = 1, D = 0))\n)\n\nggdag(confounder) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n```\n\nBy now, you might recognize what kind of problem the DAG depicts: confounding. A variable $Z$ confounds the relationship between $X$ and $Y$ and to estimate the causal effect of $X$ on $Y$, we need to close the backdoor path of $X$ to $Y$ via $Z$.\n\n# Idea of Matching\n\nOne of the options to close the backdoor is **matching**, which covers any method **attempting to equate or balance the distribution of covariates in treatment and control group**. Simply put, the **goal of matching is to compare apple to apples** and after treatment make treatment and control group as similar as possible (of course, except for the treatment value).\n\nIn a way, matching is an alternative to using regression to close backdoors and neither is better or worse, in fact, they can even be combined. But for now, let us focus on matching and understand what it really it is, what kind of matching methods are popular and how they can be applied in `R`.\n\nLet us assume you would like to study a phenomena and only have observational data that looks like this. $Z$ (education) is a covariate, $Y$ is the outcome and the color of the data points shows if a unit has been treated or not. From the first glance you can already see that the data does not look as if you would expect in a randomized experiment. For values of $Z$ in the middle range, there are both treated and untreated cases, but for values at the lower and upper range there are only untreated cases. That is an indication that $Z$ confounds the relationship between $D$ and $Y$.\n\n```{r}\n#| echo: false\n\n# Observations both treated and control (mid education)\nmatched_stuff <- tibble(education = rnorm(50, 20, 3)) %>% \n  mutate(outcome = 15 + education * rnorm(n(), -0.2, 0.05), \n         treatment = as.logical(rbinom(n(), 1, 0.5)),\n         type = \"Matched\")\n# Observations control (low education)\nunmatched_stuff_low <- tibble(education = rnorm(20, 12, 2),\n                          outcome = rnorm(20, 5, 2),\n                          treatment = FALSE, \n                          type = \"Unmatched\")\n# Observations control (high education)\nunmatched_stuff_high <- tibble(education = rnorm(5, 28, 1),\n                               outcome = rnorm(5, 5, 0.5),\n                               treatment = FALSE,\n                               type = \"Unmatched\")\n# All observations\nall_data <- bind_rows(matched_stuff, \n                      unmatched_stuff_low,\n                      unmatched_stuff_high) %>%\n  mutate(treatment = factor(treatment, \n                            labels = c(\"Untreated\", \"Treated\")))\n# Part of data that has matches (with factor treatment)\nmatched_stuff_real <- filter(all_data, type == \"Matched\")\n# Wrong model\nmodel_wrong <-\n  lm(outcome ~ education + treatment, data = all_data) %>% \n  broom::tidy()\n# Wrong model with square term\nmodel_wrong1 <- lm(outcome ~ education + treatment + I(education^2), \n                   data = all_data)\nmodel_wrong1_fitted <-\n  expand_grid(education = seq(8, 30, 0.1),\n              treatment = c(\"Treated\", \"Untreated\")) %>% \n  broom::augment(model_wrong1, newdata = .)\n# Better model\nmodel_better <- lm(outcome ~ education + treatment, data = matched_stuff_real) %>% \n  broom::tidy()\n# Better model with square term\nmodel_better1 <- lm(outcome ~ education + treatment + I(education^2), data = matched_stuff_real)\nmodel_better1_fitted <- expand_grid(education = seq(8, 30, 0.1),\n                                    treatment = c(\"Treated\", \"Untreated\")) %>% \n  broom::augment(model_better1, newdata = .)\n```\n\nJust imagine your measuring an arbitrary outcome and an arbitrary treatment confounded by years of education. When we plot the initial situation we see that for a low level of education and for a high level of education there are no treated observations. In causal inference lingo, treatment assignment is not independent from other factors, it is endogenous.\n\n```{r}\n#| echo: false\n\n# Plot all data points\nggplot(all_data, aes(x = education, y = outcome, color = treatment)) +\n  geom_point(size = 5, pch = 19, alpha = .8) +\n  labs(x = \"Education\", y = \"Outcome\") +\n  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 0, b = 0))\n```\n\nWhen we plot a line through all blue and red points, respectively and check the difference between the two lines, we see that there is substantial difference between both groups. This difference is what we would get when we run a regression on all points. It indicates a positive treatment effect.\n\n```{r}\n#| echo: false\n\n# Plot regression lines for both treatment and control (all data)\nggplot(all_data, aes(x = education, y = outcome, color = treatment)) +\n  geom_point(size = 5, pch = 19, alpha = .8) +\n  geom_abline(slope = filter(model_wrong, term == \"education\")$estimate, \n              intercept = filter(model_wrong, term == \"(Intercept)\")$estimate,\n              color = ggthemr::swatch()[2], linewidth = 0.75) +\n  geom_abline(slope = filter(model_wrong, term == \"education\")$estimate, \n              intercept = filter(model_wrong, term == \"(Intercept)\")$estimate +\n                filter(model_wrong, term == \"treatmentTreated\")$estimate,\n              color = ggthemr::swatch()[3], linewidth = 0.75) +\n  labs(x = \"Education\", y = \"Outcome\") +\n  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 0, b = 0))\n```\n\nHowever, the lines clearly don't fit the data very well because there does not seem to be a linear relationship. To account for non-linearities, you can introduce e.g. square terms to the regression, here $Z^2$ or $education^2$, respectively. Then, the fit improves, in particular for the untreated data points. But what happened to the treatment effect? Now there is negative treatment effect smaller in magnitude than the previous positive treatment effect. We already see that the estimate is highly dependent on the choice of our particular model. This is why we should always put a lot of consideration into choosing the right model (which is not necessarily the one with the largest or most desired treatment effect).\n\n```{r}\n#| echo: false\n\n# Plot regression lines for linear and square (all data)\nggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +\n  geom_point(size = 5, pch = 19, alpha = .8) +\n  geom_abline(slope = filter(model_wrong, term == \"education\")$estimate, \n              intercept = filter(model_wrong, term == \"(Intercept)\")$estimate,\n              color = ggthemr::swatch()[2], linewidth = 0.75) +\n  geom_abline(slope = filter(model_wrong, term == \"education\")$estimate, \n              intercept = filter(model_wrong, term == \"(Intercept)\")$estimate +\n                filter(model_wrong, term == \"treatmentTreated\")$estimate,\n              color = ggthemr::swatch()[3], linewidth = 0.75) +\n  geom_line(data = model_wrong1_fitted, \n            aes(x = education, y = .fitted, color = treatment),\n            linewidth = 0.75, show.legend = F) +\n  labs(x = \"Education\", y = \"Outcome\") +\n  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 0, b = 0))\n```\n\nBut we still have not really dealt with the fact that there are some regions where there are only units from one group. So let's think about what we would have to do to adjust for the confounder $Z$. Ideally, we would like to compare treated and control group in an area where both groups are present. So the lower and upper region should actually be left out. Only where there is overlap of both groups, we can compare apples with apples. Focusing only on this (somewhat arbitrary) data points and drawing the lines and curves for both specifications of the regression, we get another different result. Now it seems, that there is no effect at all.\n\n```{r}\n#| echo: false\n\n# Plot regression line (matched data)\nggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +\n  geom_point(aes(alpha = type), size = 5, pch = 21, color = \"#0F2231\") +\n  geom_abline(slope = filter(model_better, term == \"education\")$estimate, \n              intercept = filter(model_better, term == \"(Intercept)\")$estimate,\n              color = ggthemr::swatch()[2], linewidth = 0.75) +\n  geom_abline(slope = filter(model_better, term == \"education\")$estimate, \n              intercept = filter(model_better, term == \"(Intercept)\")$estimate +\n                filter(model_better, term == \"treatmentTreated\")$estimate,\n              color = ggthemr::swatch()[3], linewidth = 0.75) +\n  scale_alpha_manual(values = c(1, 0.2), guide = \"none\") +\n  labs(x = \"Education\", y = \"Outcome\") +\n  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 0, b = 0))\n```\n\nSo, how do we know what units to select, especially when there is more than one dimension? We will explore some techniques in the following sections but the short answer is: we need to create a balance for vales of all confounding variables treatment and control group, i.e. treatment and control group should be as similar as possible. And of course, we also need to assume, that all confounding variables are observed.\n\n# Subclassification Estimator\n\nAlthough not often used in practice, the subclassification estimator is an intuitive way of thinking about matching comparable observations and thereby controlling for confounders.\n\nEssentially, the subclassification estimator **estimates local treatment effects withing small groups that share the same values for (confounding) covariates**. Sometimes, values does not need to be the same but only similar, e.g. for continuous variables like age it is very unlikely to find another observation unit with the exact same birthday, so you would define e.g. a birthyear. By only comparing observations in a small subgroup as defined by the covariates, conditional independence is ensured as there is no variation in the confounding variable.\n\nImagine, that a treatment was not randomly assigned for women and men and sex also plays a role for the outcome as well. One imaginary example could be the effect of a trainee program on later salary that was advertised more heavily to women and therefore more women participated. And being a women might also have an impact on salary. Then, we have the situation of a confounding variable, sex, and to control for it, we estimate a treatment effect for women and a treatment effect for men and average the effects weighted by the respective subsample size.\n\nTo check how that affects the estimated treatment effect we can simulate some data according to the following relationships.\n\n```{r}\n# Number of observations\nn <- 1e+3\n\n# Variables\nZ <- rbernoulli(n, 0.5) # 0 => Male, 1 => Female\nD <- rbernoulli(n, p = if_else(Z, 0.65, 0.35)) # 1 => Treat, 0 => Control\nY <- 0.2*D - 0.2*Z + rnorm(n, 5, 0.1) # => Salary\n\n# Create tibble\ndf <- tibble(\n  Z = Z,\n  D = D,\n  Y = Y\n)\n```\n\n$Z$ (= sex) is the confounder, $D$ (= participation in program) is the treatment and $Y$ (salary) is the outcome. As you can see both $D$ and $Y$ are related to $Z$ and $Y$ is influenced by both $D$ and $Z$. But we are only interested in the effect of $D$ on $Z$ and want to isolate it. From the commands, we know that the true causal effect of $D$ on $Y$ is 0.2.\n\nLet's see what a naive comparison would return, where we would just compare average salary of the individuals who have participated in the trainee program are compared to the individuals who have not participated.\n\n::: callout-note\n## Accessing with base `R`\n\nTo access only certain elements of a table you can use `table[row,col]`. You can also use the `$` operator to access columns.\n:::\n\n```{r}\n# Naive comparison\nE_0 <- mean(df[df$D==F, ]$Y) # control group\nE_1 <- mean(df[df$D==T, ]$Y) # treatment group\nE_1 - E_0\n```\n\nA simpler way to compute it would be:\n\n```{r}\n# Regression of Y on D\nsummary(lm(Y ~ D, df))\n```\n\nDue to confounding, the naive comparison cannot reconstruct our implemented treatment effect (defined when we simulated the outcome variable) and is off. We should get closer if we account for confounding by taking averages treatment effects for men and women separately.\n\n```{r}\n# Subclassification estimator (subclasses: Z = 0 and Z = 1)\n# E(Z, D)\nE_00 <- mean(df[(df$Z==F & df$D==F), ]$Y) # control men\nE_10 <- mean(df[(df$Z==T & df$D==F), ]$Y) # control women\nE_01 <- mean(df[(df$Z==F & df$D==T), ]$Y) # treatment men\nE_11 <- mean(df[(df$Z==T & df$D==T), ]$Y) # treatment women\n\n# Weighted by K (proportion of female/male)\nK <- mean(Z)\n\nK*(E_11-E_10) + (1-K)*(E_01 - E_00)\n```\n\nAnd in fact, now we are really close to the true treatment effect. The remaining difference is caused by randomness in our sampling process.\n\nThis way, we have closed the backdoor and are able to retrieve the causal effect. However, note that the effect can only be causally interpreted, if sex is the only confounding variable. If there are more confounders, we have to build smaller groups that are defined by value combinations of all confounders, which will at some point lead groups that are extremely small or maybe even empty. This is called the curse of dimensionality and we'll deal with it in the following sections.\n\n# Matching as a Concept\n\nIn the previous section, observations from treatment and control group were required to have equal covariates. But matching can also take other forms that are less strict. Generally, we are trying to select observations in the control group that are similar to those in the treated group. To do so, we need to define what similar means. By enforcing treatment and control group to have little variation in the matching variable, we close the backdoor. When the backdoor variable does not vary or varies only very little, it cannot induce changes in treatment and outcome. So, when we suppress this variation in the backdoor variable, we can interpret the effect from treatment to outcome as causal.\n\nThere are many ways and methods to conduct this and we will go through some of them. But let's first define matching conceptually.\n\nIn general, the **outcome of a matching** method are **weights for each observation** **based on one or several matching variables**. For some matching methods the weights are either 0 or 1 (in or out), but there are also a lot of methods that give weights between 0 and 1 (less and more important) to observations.\n\nThe **matching variables need to cover** the variables that can - when adjusted on - block **all backdoor paths** between our treatment variable and the outcome. Then, the treatment effect is computed as a weighted mean of the outcomes for the treatment and control group.\n\nMatching methods can be classified into [two main approaches]{.underline}: matching **based on (1) distance** and matching **based on (2) propensity scores**.\n\n1.  **Distance**: observations are similar if they have similar covariates (or more specific: similar values for the matching variables). This approach minimizes the distance between observations in treatment and control group based on their covariate values.\n\n2.  **Propensity**: observations are similar if they are equally likely to be treated, i.e. matching variables are used to compute how likely an observation is to be in treatment group, regardless whether it actually is in the treatment or control group.\n\nAs you can also see in the plot, another difference across methods is whether observations are compared to matches, which classify other observations as either in our out or compared to matched weighted samples, where each observation obtains a different weight depending on how close they are. For matches, you have to decide how many to include or what the worst acceptable match is while for matched weighted samples you need to decide how weights decay with distance.\n\n```{r}\n#| echo: false\n#| layout-nrow: 1\n#| fig-asp: 1.1\n#| fig-cap: \n#|   - \"Initial situation: one treated observation and five potential matches from the control group.\"\n#|   - \"The closest match is selected to compute treatment effect.\"\n#|   - \"Several units are selected and weighted by their distance.\"\n\n# Create tibble of potential matches with two variables, in/out and weight\ndf <- tibble(\nx1 = c(1, 2, 3, 3, 3.2),\nx2 = c(1.5, 2.2, 3.1, 2.7, 2.5),\nm  = as.factor(c(0, 1, 0, 0, 0))\n)\n\n# Create focal point\ntreated <- tibble(\n  x1 = 2.35,\n  x2 = 2.2\n)\n\n# Initial situation\np1 <- ggplot(df, aes(x = x1, y = x2)) +\n  geom_point(size = 4) +\n    geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill= ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Initial situation\")\n\n# In/Out matching\np2 <- ggplot(filter(df, m==1), aes(x = x1, y = x2)) +\n  geom_point(data = filter(df, m == 0), color = \"grey\", alpha = .42, size = 4) +\n  geom_point(size = 7, color = ggthemr::swatch()[4]) +\n    geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill = ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"In or Out\")\n\n# Compute distance based on Euclidean distance\neuclidean <- function(a, b) sqrt(sum((a - b)^2))\ndistance <- lapply(1:nrow(df), function(row) {\n  vec_df <- c(df$x1[row], df$x2[row])\n  vec_tr <- as_vector(treated)\n  \n  euclidean(vec_df, vec_tr)\n}) %>% unlist()\ndf$w <- 1/distance\n\n# Weighted by distance\np3 <- ggplot(df, aes(x = x1, y = x2, size = w)) +\n  geom_point(color = ggthemr::swatch()[4]) +\n    geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill= ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Weighted by distance\")\n\n# plot all\np1\np2\np3\n```\n\nWe can now outline a structure for the [**general process of matching**]{.underline} that applies to all methods.\n\n1.  First, it starts with some kind of preprocessing, which is a \"manual\" part, where you use all information about the causal mechanisms that you have. Essentially, it forces you to look at the DAG you can draw based on all your theoretical knowledge and thoughts, and particularly what you know about the treatment assignment. In observational studies, this will leave you with a sample that is different from your raw sample. In randomized studies, it might be the same as you do not have to for example close a backdoor path.\n2.  Then, using this preprocessed data you can build your model to estimate the treatment effect.\n\nNow, let's look at some methods in detail and afterwards get more application-oriented.\n\n## Single Matching Variable\n\nProbably the most simple method, though rarely applied in practice, is matching on a single variable. Its rare use in practice is due to the fact that matching on a single variable is only applicable if there is only one backdoor path that can be closed by this matching variable.\n\nBut for ease of explanation, we'll have a look at it.\n\n```{r}\n#| echo: false\n#| fig-cap: \"Generic form of confounder and example. By fixing the level of confounder, backdoor is closed and valid estimation of causal treatment effect for that specific group at level of confounder. Example scenario: Effect of being late on settling credict card debt in the past on being late again. Confounded by size of credit card debt. It could be suspected that customers with higher debts in the past are unlikely to be financially recovered by now.\"\n#| fig-subcap: \n#|   - \"Generic DAG with matching\"\n#|   - \"Applied to credit card example\"\n#| layout-ncol: 2\n\n# Generic confounder\nconfounder <- dagify(\n  D ~ Z,\n  Y ~ Z,\n  Y ~ D,\n  coords = list(x = c(Y = 3, Z = 2, D = 1),\n                y = c(Y = 0, Z = 1, D = 0))\n)\n\nggdag(confounder) +\n  annotate(\"text\", x = 1.5, y = 1, label = \"Z fixed at constant level\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n# Example confounder\nexample_confounder <- dagify(\n  D ~ Z,\n  Y ~ Z,\n  Y ~ D,\n  coords = list(x = c(Y = 3, Z = 2, D = 1),\n                y = c(Y = 0, Z = 1, D = 0)),\n  labels = list(D = \"Being late previously\", Y = \"Being late next month\", Z = \"\")\n)\n\nggdag(example_confounder, use_labels = \"label\") +\n  annotate(\"text\", x = 1.5, y = 1, label = \"Size of previous bill\\n equal to 5000$\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n```\n\n## (Coarsened) Exact Matching\n\nWhen you think of the previous credit card example, two things could come up in your mind:\n\n-   How likely is it that when fixing a continuous variable such as previous bill, we can find matching treatment and control observations?\n\n-   How do we control for more than one variable?\n\nTo address the first issue we can coarsen continuous variables. That means, we create different levels/bins for the size of the previous bill, which could be for example: 1-50€, 51-250€, 251-1000€ and so on. This increases the likelihood of finding matches substantially. However, we also lose a bit of precision. In general: the wider the bins, the more matches but the less precision.\n\nIf there is more than one backdoor, we can also match on more than one variable. Matched observations need to correspond on all variables that are selected as matching variables.\n\nEspecially if matching on a single variable, there might be more than one match for a particular observation. In this case, you would weight the matches. On the other hand, especially if trying to find (coarsened) exact matches on multiple variables, some observations might not be matched at all. That's a situation where nearest-neighbor matching comes in handy.\n\n## Nearest-Neighbor Matching\n\nIt is less strict than exact matching as it does not require observations to match exactly in all (coarsened) variables. Instead, the distance between observations is computed using distance measures such as the distance or the Euclidean distance.\n\n```{r}\n#| echo: false\n#| layout-nrow: 1\n#| fig-asp: 1.1\n#| fig-cap: \n#|   - \"Initial situation: six potential matches.\"\n#|   - \"Four best matches are selected.\"\n#|   - \"Matches are only accepted if they fall into specified vicinity.\"\n#| message: false\n#| warning: false\n\n# Create tibble of potential matches with two variables, in/out and weight\ndf <- tibble(\nx1 = c(0.5, 2.2, 2, 4.5, 4.2, 1),\nx2 = c(3.4, 3, 4.5, 2.1, 0.8, 1),\nm  = as.factor(c(1, 1, 1, 1, 0, 0))\n)\n\n# Create focal point\ntreated <- tibble(\n  x1 = 2.2,\n  x2 = 4.1\n)\n\n# Initial situation\np1 <- ggplot(df, aes(x = x1, y = x2)) +\n  geom_point(size = 4) +\n    geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill= ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 6),\n                  ylim = c(0, 6)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Initial situation\")\n\n# In/Out matching\np2 <- ggplot(filter(df, m==1), aes(x = x1, y = x2)) +\n  geom_point(data = filter(df, m == 0), color = \"grey\", alpha = .42, size = 4) +\n  geom_point(size = 7, color = ggthemr::swatch()[3]) +\n    geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill = ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 6),\n                  ylim = c(0, 6)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Nearest-Neighbor: k = 4\") \n\n# Compute distance based on Euclidean distance\neuclidean <- function(a, b) sqrt(sum((a - b)^2))\ndistance <- lapply(1:nrow(df), function(row) {\n  vec_df <- c(df$x1[row], df$x2[row])\n  vec_tr <- as_vector(treated)\n  \n  euclidean(vec_df, vec_tr)\n}) %>% unlist()\ndf$w <- 1/distance\n\n# Weighted by distance\np3 <- ggplot(df, aes(x = x1, y = x2, size = w, color = m)) +\n  geom_point(color = c(rep(ggthemr::swatch()[3], 3), rep(\"grey\", 3))) +\n  geom_point(aes(x = treated$x1, treated$x2),\n             pch = 23, size = 7, color = \"white\", fill= ggthemr::swatch()[5]) +\n  coord_cartesian(xlim = c(0, 6),\n                  ylim = c(0, 6)) +\n  ggforce::geom_circle(aes(x0 = 2.2, y0 = 4.1, r = 1.85),\n                       color = alpha(\"white\", 0.7), linetype = \"solid\", lwd = 0.4) + \n  theme(legend.position = \"none\") +\n  labs(title = \"Using kernel weight\")\n\n# plot all\np1\np2\np3\n```\n\nFor two observations, a scalar is returned calculated from the distances of all matching variables. Based on that value, $k$ matching observations are selected to build the control group.\n\nIf $k \\neq 1$, you also have to decide whether to weight the control observations on their distance. Weighting the observations makes the approach less sensitive to the choice of $k$ because less importance observations are down-weighted. Eventually the weights goes to zero.\n\n## Propensity score\n\nWith an increasing number of matching variables nearest-neighbors matching becomes unfeasible, as well. To reduce the dimensionality onto one dimension, for each observation a propensity score can be computed. The propensity score expresses the estimated probability of treatment. In absence of selection bias, propensity scores should be very similar across treatment and control group. Thinking back to our probability chapter, the propensity score is:\n\n$$\nP(D_i = 1|X_i)\n$$\n\nWe'll discuss how to use it later in detail but essentially, we exploit that there are units in the treatment group that were unlikely to be treated and vice versa. The most recommended method that uses propensity scores as matching input is called **inverse probability weighting** and weights each observation is weighted by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight.\n\n# Application\n\n## Multiple Matching Variables\n\nLet us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.\n\nNow you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age. Let's assume for demonstration purposes that these are the only confounding factors. In practice, there might be more, however.\n\nWe can use a DAG to think about the correct identification strategy. Using `dagitty` and `ggdag` we see that we need to close two backdoor paths: initial health status and age.\n\n```{r}\n#| fig-cap: \"DAG showing what needs to be accounted for.\"\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Define DAG\ndag_model <- 'dag {\n  bb=\"0,0,1,1\"\n  \"Health Program\" [exposure,pos=\"0.25,0.2\"]\n  \"Initial Health Status\" [pos=\"0.35,0.25\"]\n  \"Sick Days\" [outcome,pos=\"0.35,0.2\"]\n  Age [pos=\"0.25,0.25\"]\n  \"Initial Health Status\" -> \"Health Program\"\n  \"Initial Health Status\" -> \"Sick Days\"\n  Age -> \"Health Program\"\n  Age -> \"Initial Health Status\"\n  Age -> \"Sick Days\"\n}'\n\n# DAG with adjustment sets (and custom layout)\nggdag_adjustment_set(dag_model, shadow = T, use_labels = \"name\", text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n```\n\n```{r}\n#| include: false\n\n# [3] Generate synthetic data ----\n# Number of observations\nn <- 1e+4\n# Z_1: age\nage <- scales::rescale(\n  rbeta(n, shape1 = 3, shape2 = 2), \n  to = c(18, 65))\n# Z_2: initial health status (sick days before)\nsick_days_before <- round(rexp(n, 25/age) + rnorm(n, 2, 0.5))\n# D: participation in health program\nd_cont <- 0.04*age + sick_days_before + rnorm(n)\nhealth_program <- if_else(\n  d_cont > median(d_cont),\n  rbernoulli(n, 0.65),\n  rbernoulli(n, 0.35)\n  )\nmean(health_program)\n        \n# Y: sick days (after)\nsick_days <- round(\n  0.04*age + sick_days_before + 0.5*health_program + rnorm(n, 2, 1)\n  )\n# Add variables to tibble\ndf <- tibble(\n  age              = age,\n  sick_days_before = sick_days_before,\n  health_program   = health_program,\n  sick_days        = sick_days\n)\n\n# Save\nsaveRDS(df, \"../../datasets/health_program.rds\")\n```\n\nLet's load the data (you probably have to change the path) and have a glance at it.\n\n```{r}\n#| eval: false\n\n# Read data\ndf <- readRDS(\"../../datasets/health_program.rds\")\n\n# Show data \ndf\n```\n\n::: callout-important\n## True Treatment Effect\n\nBecause the data is simulated, we know the true treatment effect: **0.5**\n\nCompare it to the estimates in the following sections!\n:::\n\nA naive estimate would be obtained by just regressing `sick_days` on `health_program`.\n\n```{r}\n# Naive estimation (not accounting for backdoors)\nmodel_naive <- lm(sick_days ~ health_program, data = df)\nsummary(model_naive)\n```\n\nBut we suspect that treatment assignment was not random, so let's see if we can improve the validity of our estimation using matching.\n\n## (Coarsened) Exact Matching\n\nAgain, in case of exact matching, only observations that share the same values in (coarsened) matching variables are matched in. To perform Coarsened Exact Matching (CEM) you can use the `MatchIt` package in R. If you do not specify how to coarsen the data, it will be done automatically based on an algorithm. Other than that, we provide a formula containing our treatment dependent on the matching variables, the data, what method to use (`'cem'` = Coarsened Exact Matching) and what estimate we are interested in.\n\n```{r}\n# Load 'MatchIt' library\nlibrary(MatchIt)\n\n# Without specifying coarsening\n# (1) Matching\ncem <- matchit(health_program ~ age + sick_days_before,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n```\n\nUsing the `summary()` function we can check how well balanced the covariates are compared to before. In this case, they are almost perfectly balanced.\n\n```{r}\n# Covariate balance\nsummary(cem)\n```\n\nNow, we can use the matched data and see how the coefficient changes. Actually, it changes quite a lot. Even when at first glance, the covariates were not too different before matching.\n\n```{r}\n# Use matched data\ndf_cem <- match.data(cem)\n\n# (2) Estimation\nmodel_cem <- lm(sick_days ~ health_program, data = df_cem, weights = weights)\nsummary(model_cem)\n```\n\nInstead of letting the algorithm decide how to coarsen the data, we can also provide custom cut-points. Let's do that and check if we can create matched data as balanced as by the algorithm.\n\nWe'll see that we are able to decrease the imbalance, but not to the same degree as the algorithm did it.\n\n```{r}\n# Custom coarsening\n# (1) Matching\ncutpoints <- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))\ncem_coars <- matchit(health_program ~ age + sick_days_before,\n                     data = df, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n\n# Covariate balance\nsummary(cem_coars)\n```\n\nWe can also visualize the subsamples and see how data points are weighted. Weights depend on how many treated and control units there are in a specific subsample. You can see that in the top-right corner, for example. From the plot we can also see that the cut-points are too broad as matches could be way closer.\n\n```{r}\n#| fig-cap: \"Coarsening: each subsample should contain only similar units.\"\n\n# Use matched data\ndf_cem_coars <- match.data(cem_coars)\n\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sick_days_before,\n                         size = weights, color = as.factor(health_program))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sick_days_before),\n              mapping = aes(intercept = y, slope = 0), \n              linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  geom_vline(data.frame(y = cutpoints$age),\n              mapping = aes(xintercept = y),\n             linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  theme(legend.position = \"none\")\n```\n\nWith custom coarsening, we again get another coefficient. It could indicate that this way, backdoors are not properly closed.\n\n```{r}\n# (2) Estimation\nmodel_cem_coars <- lm(sick_days ~ health_program, data = df_cem_coars, \n                      weights = weights)\nsummary(model_cem_coars)\n```\n\n## Nearest-Neighbor matching\n\nFor nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular distance measures used to find so called *nearest neighbors* is the Mahalanobis distance.\n\nAgain, we use `MatchIt` to conduct the matching process. We just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching. The result differs from (coarsened) exact matching but again, we have almost perfect balance.\n\n```{r}\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn <- matchit(health_program ~ age + sick_days_before,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n```\n\nAnd also the estimated average treatment effect is very similar to the one obtained by (default) CEM.\n\n```{r}\n# Use matched data\ndf_nn <- match.data(nn)\n\n# (2) Estimation\nmodel_nn <- lm(sick_days ~ health_program, data = df_nn, weights = weights)\nsummary(model_nn)\n```\n\n::: callout-important\n## Curse of dimensionality\n\nWith exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don't find any matches at all.\n\nRegarding exact matching, consider for example the situation with two covariates with each five different values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates \\~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations.\n:::\n\n## Inverse Probability weighting\n\nOne way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let's go into more detail.\n\n### Estimating Propensity Score\n\nWe start by understanding what **probability** in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as **propensity score**.\n\nHere, we will use a logistic regression for prediction. A logistic regression, opposed to a linear regression, is designed for outcomes that are between 0 and 1, such as probabilities. The coefficients are a bit more difficult to interpret, so we'll leave that for now. But what we see is, that `age` and `sick_days_before` are relevant and positive predictors for the probability of treatment.\n\n```{r}\n# (1) Propensity scores\nmodel_prop <- glm(health_program ~ age + sick_days_before,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n```\n\nFor each observation we can compute a probability and add it to a table. It is important to specify `type = \"response\"` in the `predict()` command to obtain probabilities.\n\n```{r}\n# Add propensities to table\ndf_aug <- df %>% mutate(propensity = predict(model_prop, type = \"response\"))\n```\n\nHaving obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as **propensity score matching**. However, there are several reasons why this is not a good identification strategy[^1], mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that same covariate values indeed imply the same propensity score.\n\n[^1]: <https://gking.harvard.edu/publications/why-propensity-scores-should-not-be-used-formatching>\n\n### Weighting by Propensity Score\n\nInstead **inverse probability weighting (IPW)** has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use the propensity score of an observation unit to in- or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\n$$\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n$$\n\nwhere only one of the terms is always active as $D_i$ is either one or zero. Now we should better understand what \"inverse probability weighting\" actually means. It weights each observation by its inverse of its treatment probability. Let's compute it for our data.\n\n```{r}\n# Extend data by IPW scores\ndf_ipw <- df_aug %>% mutate(\n  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw)\n```\n\nImagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. $D_i=1$. Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let's assume it was 0.8, for demonstration. Then his/her weight would equal $w_i = \\frac{1}{0.8} = 1.25$.\n\nCompared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let's say his/her probability of participating would be 0.2. Then, his/her weight would be $w_i = \\frac{1}{0.2} = 5$. So we see, he/she would obtain a significantly higher weight.\n\nIn general, IPW upweights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group.\n\nRunning a linear regression with weights as provided by IPW yields a coefficient not as precise as the ones obtained by previous matching procedures.\n\n```{r}\n# (2) Estimation\nmodel_ipw <- lm(sick_days ~ health_program,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n```\n\nA reason for that could be that there are some extreme and very atypical observations with either very high or low probabilities. Those observations than get a very high weight.\n\n```{r}\n#| fig-cap: \"Most predicted treatment probabilities are between 0.4 and 0.6. Only a few outliers.\"\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n```\n\n```{r}\n# Looking for observations with highest weights\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw) %>% \n  arrange(desc(ipw))\n```\n\nA rule of thumb is to filter out all observations with a propensity score less than 0.15 and higher than 0.85. Doing that,the coefficient is very close to the true treatment effect.\n\n```{r}\n# Run with high weights excluded\nmodel_ipw_trim <- lm(sick_days ~ health_program,\n                data = df_ipw %>% filter(propensity %>% between(0.15, 0.85)),\n                weights = ipw)\nsummary(model_ipw_trim)\n```\n\nOpposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the probability model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders.\n\n## Comparison\n\nComparing the methods, we see that some of them were yield results very close to the true treatment effect. The naive estimation, however, was far off. Also, using custom cut-points (that were too wide) for coarsening also reduces the accuracy.\n\n```{r}\n# Summary of naive and matching methods\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                \"CEM1\"  = model_cem,\n                                \"CEM2\"  = model_cem_coars,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n```\n\n# Assumptions\n\nTo obtain valid estimates with any matching estimator, we still need to fulfill some assumptions. Most important are the following:\n\n-   **Conditional Independence**: all backdoors needs to be closed so it has to be ensured that all confounders are in the set of matching variables.\n\n-   **Common Support: for** treated units, there need to exist appropriate control units. Without a substantial overlap in the distribution of matching variables or the propensity score, estimating causal effects is impossible.\n\nParts of these assumptions can be checked by looking at the balance of covariates, e.g. with `summary()` and the `MatchIt` package or with base `R` functions. If the degree balance is not satisfactory, you can change parameters of your matching method and do a new iteration until you are confident to have a good balance between treatment and control.\n\nHaving fulfilled the necessary assumptions allows the valid estimation of a treatment. It is interesting to note that, depending on what method you chose, a different kind of treatment effect is estimated. Most of the methods start with having a treated unit and search for matches (selecting or weighting) across the control units, which will return the average treatment effect on the treated, $ATT$. By the same logic, these methods can also yield the average treatment effect on the untreated, $ATU$.\n\nThen, the average treatment effect can be estimated by a weighted estimate of the group treatment effects.\n\n$$\nATE = p*ATT + (1-p)*ATU\n$$\n\nIn case of IPW, $ATE$ is computed directly, as it directly changes the weights of units from both groups.\n\n::: callout-important\n## Matching vs. Regression\n\nMatching methods are a very active research field and even thinking about the correct way to choose parameters in those methods offers a wide range of questions that there is no definite answer to. To some degree, choosing matching parameters is arbitrary. A benefit, however, is that after matching model dependence is reduced when using simple techniques like weighted means.\n\nBut how does it compare to using a simple linear regression? In one of the previous chapters, we said that closing back doors can also be achieved by including confounders into the regression equation. One difference is that when using regression, the $ATE$ points in direction where covariates have more variance, i.e. when there is a higher treatment variance for older people for example, they obtain a higher weight as compared to the matching approach. Another difference is that a regression hinges on the assumption of linearity, which we do not need to assume in matching approaches.\n:::\n\n# Assignment\n\nImagine, the following situation. You are running an online store and you one year ago, you introduced a plus membership to bind customers to your store and increase revenue. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. But of course, there are potentially confounding variables such as `age`, `sex` or `pre_avg_purch` (previous average purchases).\n\nLoad the data `membership.rds`. Then,\n\n1.  Check the relationships between the variables and draw a DAG as you understand the relations.\n2.  Compute a naive estimate of the average treatment effect.\n3.  Use the following matching methods to obtain more precise estimates:\n    1.  (Coarsened) Exact Matching.\n\n    2.  Nearest-Neighbor Matching.\n\n    3.  Inverse Probability Weighting.\n    \n::: callout-warning\n## How to submit your solutions!\n\nPlease see [here](https://owmork.github.io/causal_ds/submission/submission.html) how you have to sucessfully submit your solutions. I would recommend you to solve the assignments first in `.R` scripts and in the end convert them to the required format as explained in the submission instructions.\n:::\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"blogdown::html_page":{"toc":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"06_match.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":["darkly","../../theme-darkly.scss"],"mainfont":"arial","linestretch":1.7,"title":"Matching and Subclassification","linktitle":"Matching and Subclassification","date":"2022-11-23","menu":{"example":{"parent":"Toolbox","weight":7}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}}}