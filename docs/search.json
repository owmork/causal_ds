[
  {
    "objectID": "submission/submission.html",
    "href": "submission/submission.html",
    "title": "Submission via GitHub Classroom",
    "section": "",
    "text": "To submit your assignments, you will create your own Quarto1 journal that you upload to GitHub2. GitHub is is a hosting platform for so called repositories, which typically consists of data and code.\nYou will write your solutions including your code in .qmd (Quarto) documents that can run and knit R code. You will publish these .qmd files using Quarto and GitHub to create your own password-protected website.\nIn the following I will guide your through the necessary steps. Some steps might not be identical and slightly changed in the meantime, but you should be able to follow along.\n\n\n\n\n\n\nDon’t worry!\n\n\n\nYour solutions don’t need to be completely free from errors but I should be able to recognize that you understand the core concepts and attempt to solve the assignments. Please make sure, that your code is running and generating output."
  },
  {
    "objectID": "submission/submission.html#footnotes",
    "href": "submission/submission.html#footnotes",
    "title": "Submission via GitHub Classroom",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://quarto.org/↩︎\nhttps://github.com/↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS\n        ",
    "section": "",
    "text": "lv3060Winter term 2023/2024Institute of EntrepreneurshipTUHH"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "\n            CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS\n        ",
    "section": "Course description",
    "text": "Course description\n\nDistinguishing causal relationships from simple correlation is what commonly used approaches in business analytics often fall short of. In this course, we will provide you with the skill set to answer questions like\n\nwhat happens to \\(Y\\) if we do \\(X\\)?\nwas it \\(X\\) that caused \\(Y\\) to change?\n\nIntroducing you to causal inference with the help of data science will allow you to carry out state-of-the-art causal analyses by yourself and extrapolate causal knowledge across different business contexts and various management areas."
  },
  {
    "objectID": "index.html#website-structure",
    "href": "index.html#website-structure",
    "title": "\n            CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS\n        ",
    "section": "Website Structure",
    "text": "Website Structure\n\nBrowse all the class content, assignments and examples by going to Content. For instruction how to pass the course go to Submission. If you have problems regarding the content, solving the assignments or any other questions, use the chat icon to directly go to our Mattermost discussion channel."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "\n            CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS\n        ",
    "section": "Instructor",
    "text": "Instructor\n   Oliver Mork\n   oliver.mork@tuhh.de\n   StartupEngineer"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "\n            CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS\n        ",
    "section": "Course Details",
    "text": "Course Details\n   06.11. / 27.11. / 18.12.\n   09:00 - 17:00\n   Q - 1.121"
  },
  {
    "objectID": "content/toolbox/07_did.html",
    "href": "content/toolbox/07_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Another quasi-experimental method from our toolbox is the difference-in-differences (DiD) approach. It is the most popular research design in quantitative and social sciences. As the name implies, the method captures differences by observing a treatment and a control group over time to estimate causal average effects.\nIn its simplest form, DiD compares two groups (control and treatment) at two points in time (before treatment and after) by observing if and how different both groups change. It is important to note, that both groups do not need to be equal before the treatment.\nBy taking two differences, two different kind of biases can be avoided. First, by comparing both groups at both points in time, any external effect that affects the outcome of both groups in the same way is equalized. Secondly, taking only the difference of a change in consideration, we can disregard selection bias. Potential outcomes can differ:\n\\[\nE[Y_0|D = 0] \\gtreqqless E[Y_0|D = 1]\n\\]\nWe don’t care whether initially treatment and control group are different. We only assume that they behave similarly in absence of treatment.\nAs can be seen in the table, the difference in outcome for the treatment group before and after treatment is \\(D + T\\), while for the control group it is only \\(T\\). The difference of these two differences then reduces to only \\(D\\), which is the treatment effect we want to estimate.\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\nWe can also break it down in our known notation:\n\\[\n\\delta = ATE = \\bigg(E[Y_{D=1}|T=1] - E[Y_{D=1}|T=0] \\bigg)- \\bigg(E[Y_{D=0}|T=1] - E[Y_{D=0}|T=0]\\bigg)\n\\]\nBecause there are a lot of subscripts, it can also help to write down the formula in pseudo-math:\n\\[\nATE = (Y_{Treatment, After} - Y_{Treatment, Before}) - (Y_{Control, After} - Y_{Control, Before})\n\\]\nAgain, opposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential outcomes \\(E[Y_0|D=1] = E[Y_0|D=1]\\) are equal. That is a big difference, because do not have to assume that observation units are similar in all their characteristics.\nInstead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but only the change of outcomes from before to after. It implies that there is no factor that has only an impact on just one of the groups. If units differ in characteristics, they are only allowed to have a constant effect. If the effect varies with time, the parallel trends assumption is violated."
  },
  {
    "objectID": "content/toolbox/07_did.html#parallel-trends",
    "href": "content/toolbox/07_did.html#parallel-trends",
    "title": "Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\nScenario A\nTo compute an estimated treatment effect, we filter the data to the two periods just around treatment and implement the formulas as in the introduction. Not surprisingly, we get an estimate that is very close to our true treatment effect.\n\n# [1.1.1] (A) Fulfillment ----\n# Scenario (A)\n# Only show last data point before and first data point after treatment.\ndf_A_zoom_in  &lt;- df_A %&gt;% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_A &lt;- df_A_zoom_in %&gt;%\n  filter(treat == 0, after == 0) %&gt;% \n  pull(sales)\nbefore_treatment_A &lt;- df_A_zoom_in %&gt;%\n  filter(treat == 1, after == 0) %&gt;% \n  pull(sales)\n\ndiff_before_A &lt;- before_treatment_A - before_control_A\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_A &lt;- df_A_zoom_in %&gt;%\n  filter(treat == 0, after == 1) %&gt;% \n  pull(sales)\nafter_treatment_A &lt;- df_A_zoom_in %&gt;%\n  filter(treat == 1, after == 1) %&gt;% \n  pull(sales)\n\ndiff_after_A &lt;- after_treatment_A - after_control_A\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate may vary from \n# true treatment effect, as we also include some noise in the data generating \n# process.\ndiff_diff_A &lt;- diff_after_A - diff_before_A\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_A, delta)\n\n[1] \"Estimate: 1.05, True Effect: 1.00\"\n\n\nLooking at the last period before and the first period after treatment, the impact of treatment can clearly be seen. The dashed line represents the counterfactual value for the treated group, i.e. the value it would have if it had not been treated. This value is not observed, but by the parallel trends assumptions, it would have developed like the value for the untreated group.\n\n\n\n\n\n\nPlots in this chapter\n\n\n\nGenerating the plots for this chapter is a bit tricky as they contain a lot of annotations and other extensions. I left the code for those who want to replicate it. But you do not worry if you cannot reproduce them.\n\n\n\nPlot parallel trends assumption# Compute counterfactual sales for treated group\ncf_treat_A &lt;- df_A[!df_A$treat == 1, \"sales\"] + diff_before_A\ndf_A[df_A$treat == 1, \"sales_cf\"] &lt;- cf_treat_A\n\n# Add to zoomed in table\ndf_A_zoom_in &lt;- df_A_zoom_in %&gt;% left_join(df_A)\n\n# Plot DiD with parallel trends assumption\nggplot(df_A_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A_zoom_in %&gt;% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_A - (diff_diff_A / 2), \n           label = \"Treatment effect\", size = 3, fill = ggthemr::swatch()[4]) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_A + 1.1*diff_before_A + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3, color = ggthemr::swatch()[3]) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A:\\nParallel Trends Assumption\")\n\n\n\nIf parallel trends assumption can be assumed, treatment effect is valid. Counterfactual line shows how outcome would have been evolved in absence of treatment.\n\n\n\nScenario B\nRepeating the steps for scenario B yields an unexpected result. The estimated treatment effect is different from what we would have expected.\n\n# [1.1.2] (B) Violation ----\n# Scenario (B)\n# Only show last data point before and first data point after treatment.\ndf_B_zoom_in  &lt;- df_B %&gt;% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_B &lt;- df_B_zoom_in %&gt;%\n  filter(treat == 0, after == 0) %&gt;% \n  pull(sales)\nbefore_treatment_B &lt;- df_B_zoom_in %&gt;%\n  filter(treat == 1, after == 0) %&gt;% \n  pull(sales)\n\ndiff_before_B &lt;- before_treatment_B - before_control_B\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_B &lt;- df_B_zoom_in %&gt;%\n  filter(treat == 0, after == 1) %&gt;% \n  pull(sales)\nafter_treatment_B &lt;- df_B_zoom_in %&gt;%\n  filter(treat == 1, after == 1) %&gt;% \n  pull(sales)\n\ndiff_after_B &lt;- after_treatment_B - after_control_B\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate varies from \n# true treatment effect due to confounding and added noise.\ndiff_diff_B &lt;- diff_after_B - diff_before_B\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_B, delta)\n\n[1] \"Estimate: 1.23, True Effect: 1.00\"\n\n\nAgain, the picture is very similar. Having only four data points, treatment before and after and control before and after, there is no way to test the parallel trends assumption which leaves room for doubt. So how can we check whether we made a mistake or the parallel trends assumption is violated?\n\nPlot parallel trends assumption# Compute counterfactual sales for treated group\ncf_treat_B &lt;- df_B[!df_B$treat == 1, \"sales\"] + diff_before_B\ndf_B[df_B$treat == 1, \"sales_cf\"] &lt;- cf_treat_B\n\n# Add to zoomed in table\ndf_B_zoom_in &lt;- df_B_zoom_in %&gt;% left_join(df_B)\n\n# Plot DiD with parallel trends assumption\nggplot(df_B_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B_zoom_in %&gt;% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_B - (diff_diff_B / 2), \n           label = \"Treatment effect\", size = 3, fill = ggthemr::swatch()[4]) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_B + 1.1*diff_before_B + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3, color = ggthemr::swatch()[3]) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B:\\nParallel Trends Assumption\")\n\n\n\nKnowing the true treatment effect, we know that the estimated treatment effect is too high. But this plot can’t tell us why."
  },
  {
    "objectID": "content/toolbox/07_did.html#event-study",
    "href": "content/toolbox/07_did.html#event-study",
    "title": "Difference-in-Differences",
    "section": "Event Study",
    "text": "Event Study\nHence, numerous researchers endeavor to enhance the reliability of their findings through the utilization of an event study. Comparing trends before treatment across treatment and control group, it should show that there was no difference prior to the treatment. Because if there was no difference before treatment, why should there be difference after the treatment (if not for the treatment itself)?\nHowever, event studies cannot provide full certainty about the parallel trends assumption. There still might be other unobserved factors that could affect the treatment. But still, it is a good way to argue that treatment and control group are comparable.\n\nCode: Event Study A# [1.2] Event study ----\n# To provide evidence of the credibility in assuming parallel trends, \n# researchers often perform an event study, if possible. Instead of only\n# looking at the last period before and the first period after treatment,\n# further periods are included to examine the validity of the parallel trends\n# assumption and the treatment effect estimate.\n\n# [1.2.1] (A) Fulfillment ----\n# Zoom out and show that parallel trend assumption is fulfilled in scenario (a)\n\n# Compute difference in control group\ndiff_control_A &lt;- after_control_A - before_control_A\n\n# Plot event study\nev_stdy_A &lt;- ggplot(df_A, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A %&gt;% filter(treat == 1, period &gt;= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(breaks = 1:P) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A\\nEvent Study\")\n\n\n\nCode: Event Study B# [1.2.2] (B) Violation----\n# Zoom out and show that parallel trend assumption is violated in scenario (b)\n\n# Compute difference in treatment group\n# Before treatment\ndiff_control_B &lt;- after_control_B - before_control_B\n\n# Increase from t0 to before treatment\ninit_treatment_B &lt;- df_B %&gt;%\n  filter(treat == 1, period == 1) %&gt;%\n  pull(sales)\ndiff_treatment_B &lt;-  (before_treatment_B - init_treatment_B) / (P/2)\n\n# Plot event study\nev_stdy_B &lt;- ggplot(df_B, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B %&gt;% filter(treat == 1, period &gt;= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[2], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_B,\n           yend = before_treatment_B + (P/2)*(diff_treatment_B),\n           linetype = \"dashed\", color = ggthemr::swatch()[3]) +\n  # Estimated treatment effect\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(name   = \"Period\", breaks = 1:P) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B\\nEvent Study\")\n\n\n\n\n\n\nFor treatment and control group, we see the same trend over time. This lends credibility to the parallel trends assumption and consequently, to the validity of the causal treatment effect.\n\n\n\nOther than in scenario A, the parallel trends assumption does not seem to hold. The estimated treatment effect is larger than the actual treatment effect. This is due to different trends in both groups. The treatment group has a more positive trend even without treatment and the groups would have further diverged after treatment (see dashed red line). The difference between the dashed red and dashed blue line is attributable to this trend and should not be part of the treatment effect."
  },
  {
    "objectID": "content/toolbox/07_did.html#modeling",
    "href": "content/toolbox/07_did.html#modeling",
    "title": "Difference-in-Differences",
    "section": "Modeling",
    "text": "Modeling\nA more typical situation is usually that there is more than one unit in the treatment and control group. You could e.g. imagine that you are managing more than two stores and are implementing an ad campaign in a specific region.\nTo simulate such a scenario, we generate data for 3’000 stores that are split evenly into two regions. In one region, the ad campaign will be run (treatment region) and in the other there will be no campaign (control region). The variable relationships as defined in the previous section still hold.\n\n# [1.4] Linear regression ----\n# Now assume that there are more than two stores and treatment is performed\n# in a specific region which is, depending on scenario (A) and (B) \n# different to the control region.\n\n# Generate a bunch of samples and combine in one table. Here, we choose a higher\n# standard deviation.\n# We assume that we only have data from one period prior and one period after \n# treatment.\nn_stores &lt;- 3e+3\n\n# Scenario A\ndf_A_lm  &lt;- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"A\")}) %&gt;%\n    bind_rows() %&gt;%\n  filter(period %in% (P/2):(P/2+1))\n\n# Scenario B\ndf_B_lm  &lt;- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"B\")}) %&gt;%\n  bind_rows() %&gt;%\n  filter(period %in% (P/2):(P/2+1))\n\nScenario A\nSo how do we compute the average treatment effect? Previously in this chapter, we just used basic math calculations (particularly subtraction). But there is an easier way: we can use regression again. This is because the average treatment effect is the coefficient of the interaction of group and time.\n\\[\ny_i = \\beta_0 + \\beta_1 * Period_i + \\beta_2 * Treatment_i + \\beta_3 * (Time_i \\times Treatment_i) + \\epsilon_i\n\\]\n\\(Time\\) indicates whether the period is before or after the treatment and \\(Treatment\\) whether an observation belongs to the treatment group. We can interpret the estimated coefficient \\(\\hat\\beta_1\\) as the time effect for both groups, it indicates how the outcome evolves without treatment. \\(\\hat\\beta_2\\) represents the time-invariant level difference between groups, it is positive when the treatment group initially exhibited a higher outcome level.\nWhat we are primarily interested in is the interaction \\(Time \\times Treatment\\) and its estimate \\(\\hat\\beta_3\\). It is only active for the treatment group after the treatment (\\(Time = 1\\), \\(Treatment=1\\)) and represents the average treatment effect.\nFor scenario A, we can see that there is no need to adjust for the covariate \\(x1\\). You could for example think of \\(x_1\\) as the purchase power of the respective region. If you check the formulas again, your will notice that \\(x1\\) has a constant and time-invariant effect on sales and therefore it does not violate the parallel trends assumption.\nIncluding or leaving out \\(x1\\) in the regression yields a very similar unbiased estimate (close to defined true size) for our variable of interest \\(store:after\\), the parameter of interest.\n\n# [1.4.1] (A) ----\n# (a): Due to the construction of the data set, we expect interaction\n# coefficient to be significant as well as the covariate and period. However, as\n# the covariate does not have a time-varying effect, it is not a confounder and\n# interaction coefficient should be unbiased even if not adjusting for the\n# covariate.\nsummary(lm(sales ~ treat*after , data = df_A_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.176 -0.952  0.002  0.956  6.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.9610     0.0257 1981.43   &lt;2e-16 ***\ntreat         1.0372     0.0364   28.52   &lt;2e-16 ***\nafter         0.2642     0.0364    7.26    4e-13 ***\ntreat:after   0.9813     0.0514   19.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 11996 degrees of freedom\nMultiple R-squared:  0.284, Adjusted R-squared:  0.284 \nF-statistic: 1.58e+03 on 3 and 11996 DF,  p-value: &lt;2e-16\n\nsummary(lm(sales ~ treat*after + x1, data = df_A_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after + x1, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.947 -0.680  0.000  0.673  4.204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.98128    0.01821 2800.25   &lt;2e-16 ***\ntreat        0.03914    0.02732    1.43     0.15    \nafter        0.22876    0.02575    8.88   &lt;2e-16 ***\nx1           0.98721    0.00903  109.31   &lt;2e-16 ***\ntreat:after  0.97808    0.03641   26.86   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11995 degrees of freedom\nMultiple R-squared:  0.641, Adjusted R-squared:  0.641 \nF-statistic: 5.36e+03 on 4 and 11995 DF,  p-value: &lt;2e-16\n\n\nScenario B\nIn scenario B, the effect of \\(x_1\\) is different because it has a time-varying effect. Therefore it violates the parallel trends assumption, leading to a biased estimate if \\(x_1\\) is not included (e.g. because it is unobserved).\nBecause we constructed the data set ourselves, we are able to see that the bias in fact is quite large and the treatment effect seems to include the actual treatment effect plus the effect of \\(x_1\\).\n\n# [1.4.2] (B) ----\n# (b): Due to the construction of the data set, we expect interaction coefficient\n# to be significant and accurate only when adjusting for the time-varying effect\n# of the covariate and main effects for period and covariate.\nsummary(lm(sales ~ treat*after, data = df_B_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after, data = df_B_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.025  -2.331   0.008   2.293  16.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  51.0835     0.0643  794.08   &lt;2e-16 ***\ntreat         3.6321     0.0910   39.92   &lt;2e-16 ***\nafter         0.0682     0.0910    0.75     0.45    \ntreat:after   1.4315     0.1287   11.13   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 11996 degrees of freedom\nMultiple R-squared:  0.287, Adjusted R-squared:  0.287 \nF-statistic: 1.61e+03 on 3 and 11996 DF,  p-value: &lt;2e-16\n\n\nOnly with including \\(x_1\\) and as a main effect and moderator, we can reconstruct the true treatment effect.\n\n# Including time-varying effect\nsummary(lm(sales ~ treat*after + after*x1 + treat*x1, data = df_B_lm)) # best\n\n\nCall:\nlm(formula = sales ~ treat * after + after * x1 + treat * x1, \n    data = df_B_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.084 -0.682  0.001  0.679  4.348 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 51.00199    0.01818 2804.68  &lt; 2e-16 ***\ntreat        0.00336    0.03012    0.11     0.91    \nafter        0.16597    0.02571    6.45  1.1e-10 ***\nx1           2.63804    0.01591  165.81  &lt; 2e-16 ***\ntreat:after  1.00018    0.04063   24.62  &lt; 2e-16 ***\nafter:x1     0.36083    0.01823   19.80  &lt; 2e-16 ***\ntreat:x1     1.03128    0.01823   56.57  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11993 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.943 \nF-statistic: 3.31e+04 on 6 and 11993 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "content/toolbox/07_did.html#footnotes",
    "href": "content/toolbox/07_did.html#footnotes",
    "title": "Difference-in-Differences",
    "section": "Footnotes",
    "text": "Footnotes\n\nData and example from: https://www.stata.com/new-in-stata/difference-in-differences-DID-DDD/↩︎"
  },
  {
    "objectID": "content/toolbox/09_rdd.html",
    "href": "content/toolbox/09_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "The next tool we introduce is called regression discontinuity design (RDD). Fist used in 1960 (Campbell), it did not play a large role until 1999 and since then has experienced growing acceptance due to the advance of more rigorous requirements regarding credibility and causality in social sciences. Another factor that made many researchers use RDDs is the increased availability of digitized administrative data that is often linked to arbitrary rules that can be exploited to capture “as-if” randomization processes for treatment assignment.\nAssuming a data-generating process, where we have a variable \\(X\\) that is a confounder as it has an impact on treatment assignment \\(D\\) and the outcome \\(Y\\). Additionally, we could have an unobserved confounders between \\(X\\) and \\(Y\\).\nThen, as can be seen in the second DAG, the regression discontinuity design exploits the fact that \\(X\\) determines \\(D\\) and data is filtered such that there are only observations that were close to a cut-off value determining their treatment status. This way, treated and untreated units are very similar and comparable and RDD is able to eliminate selection bias for that sub-population. Note that the treatment effect you calculate using this method is an average treatment effect for a subgroup rather than for the whole population. Like with IV, it is the local average treatment effect (\\(LATE\\)).\n\\(X\\) is called the running variable and is a continuous variable assigning units to treatment \\(D\\) based on a cut-off score \\(c_0\\). Because it has an impact on \\(Y\\) as well, it is a confounder and opens a backdoor path. Now, the problem is that due to the cut-off determining the treatment \\(D\\), the backdoor cannot be closed with regular ways like e.g. matching as there is no overlap, i.e. there are no treated and untreated units for all levels of \\(X\\).\n\n\n\n\n\n\nImportant\n\n\n\n\nRunning/forcing variable: variable that determines or strongly influences treatment assignment.\nCut-off/threshold: rule-based value that divides units into treatment an control group.\n\n\n\n\n\n\n\nRelationships when using full sample.\n\n\n\nRelationships when restricting sample to units close to cut-off.\n\n\n\nTherefore, as the second graph shows, the causal effect is identified by analyzing only observations that are in close neighborhood to \\(c_0\\). The identified treatment effect is the local average treatment effect (LATE).\n\\[\nLATE_{RDD} = E[Y_1 - Y_0| X \\rightarrow c_0]\n\\]"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#identification-strategy",
    "href": "content/toolbox/09_rdd.html#identification-strategy",
    "title": "Regression Discontinuity",
    "section": "Identification Strategy",
    "text": "Identification Strategy\nRDDs are quite intuitive and very graphical. For this reason, we will go through estimation and inference using an application and explain at each step what has to be considered.\nIn our application we want to study how sending coupons to customers influences their purchasing behavior. We could run a randomized controlled trial and only send a coupon to a random selection. However, let’s imagine we have already conducted another campaign without randomization and want to exploit the results. Last year 5€ coupons were sent to customers who had not bought within the previous 60 days.\nThat is an arbitrary cut-off rule that we have discussed in the introduction. We can exploit it because we have customers around the cut-off that should be very similar. However, we could not convincingly argue that customers that had bought within the last few days are similar to those who have bought for example more than three months before.\nThus, our cut-off value is \\(c_0 = 60\\).\nLet’s have a look at what the data looks like. We know when the customers bought the last time and based on that received a coupon or did not receive anything and we know the purchases they made after the coupon distribution.\n\n# Read data. You probably have to add the correct path\ndf &lt;- readRDS(\"coupon.rds\")\n\n# Define cut-off\nc0 &lt;- 60\n\n\n# Show data\ndf\n\n\n\n  \n\n\n\nIllustrating the relationships in a DAG, we can see that for our running variable days_since_last being close to the cut-off value, we can estimate a local average treatment effect.\n\nCode# Running variable does not affect purchases itself as we only\n# estimate a local average treatment effect. Also, there is no non-random\n# heaping at cut-off.\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Directed Acyclic Graph\n# Define\nrdd &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ X,\n  exposure = \"D\",\n  outcome = \"Y\",\n  latent = \"U\",\n  coords = list(x = c(Y = 1, D = 0, X = 0, U = 1),\n                y = c(Y = 0, D = 0, X = 1, U = 1)),\n  labels = list(X = \"Days since last\\naround cut-off value\",\n                Y = \"Purchases after\",\n                U = \"Unobserved\",\n                D = \"Coupon\")\n)\n\n# Plot\nggdag_status(rdd, text = T) +\n  theme_dag_cds() +\n  guides(color = \"none\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\nBecause the coupons were assigned by a computer, we deal with a sharp cut-off. This means every customer with no purchase history within the last 60 days did receive the coupon and every customer with purchases made within this period did not receive it.\n\nCode# [2] Visualization ----\n# [2.1] Compliance ----\n# As expected, perfect \"compliance\" and sharp cutoff. All \n# customers below the cutoff get no coupon, while all customers above\n# the cutoff get a coupon.\ncompl &lt;- \n  ggplot(df, aes(x = days_since_last, y = coupon, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.2, position = position_jitter()) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"No coupon\", \"Coupon\"))+\n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\")\ncompl\n\n\n\nSharp cut-off. All eligible customer were sent an coupon and not eligibles did not receive it."
  },
  {
    "objectID": "content/toolbox/09_rdd.html#random-assignment",
    "href": "content/toolbox/09_rdd.html#random-assignment",
    "title": "Regression Discontinuity",
    "section": "Random assignment",
    "text": "Random assignment\nAs already mentioned, for RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is no decline or incline at the cut-off and therefore can assume that the continuity assumption holds.\n\nCode# [2.2] Random assignment test ----\n# identifying assumption: random assignment to either side of cut-off\n# Manual plot\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", boundary = c0, alpha = .6) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\nNumber of customers for given days since last purchase. As there are many customers who buy regularly and also a number of one-time purchasers, the distribution has a decreasing curve.\n\n\n\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package. Using a so-called McCrary density test, we obtain a p-value indicating how confident we are that our assumption is satisfied. As the p-value (\\(=.24\\)) is larger than any common \\(\\alpha\\) value, we reject the null hypothesis that the number of units at either side are different. Our assumption is satisfied.\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd &lt;- rddensity(df$days_since_last, c = c0)\nsummary(rddd)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       5000\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 60                Left of c           Right of c          \nNumber of obs         3854                1146                \nEff. Number of obs    1486                734                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           31.203              31.915              \n\nMethod                T                   P &gt; |T|             \nRobust                1.1559              0.2477              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          &lt;c     &gt;=c    P&gt;|T|\n0.500                      20      20    1.0000\n1.000                      31      35    0.7122\n1.500                      44      47    0.8341\n2.000                      70      68    0.9322\n2.500                      92      89    0.8819\n3.000                     110     105    0.7851\n3.500                     123     118    0.7967\n4.000                     135     129    0.7584\n4.500                     148     142    0.7691\n5.000                     164     159    0.8239\n\n\nAdditionally, the plot confirms our assumption. You can see that the confidence intervals overlap. If they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to obtain valid results.\n\n# Visually check continuity at running variable\nrdd_plot &lt;- rdplotdensity(rddd, df$days_since_last, plotN = 100)"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#visualization",
    "href": "content/toolbox/09_rdd.html#visualization",
    "title": "Regression Discontinuity",
    "section": "Visualization",
    "text": "Visualization\nHaving checked potential violations of the continuity assumptions, we can move on and estimate the treatment effect. We start with selecting a bandwidth, i.e. we select what and how many observations should be compared. The larger the bandwidth, the more observations are taken into consideration but it also reduces the comparability because observations at the respective limits might not be similar enough. On the other hand, choosing a small bandwidth results in a lower number of observations but ensures similarity and comparability.\nThere is no safe rule how to best select the bandwidth, although there are algorithms attempting to look for the optimal bandwidth. For now, we just use common sense and select a bandwidth of 5 days, resulting in an analysis window \\([55, 65]\\), which still leaves us with about 300 observations.\n\n# [3] Dependent variable ----\n# [3.1] Average Treatment Effect ----\n# Plot regression lines for full and specified bandwidth.\n# Specify bandwidth\nbw &lt;- c0 + c(-5, 5)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\n# Alternative way to define tables\n# df_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1], days_since_last  &lt; c0)\n# df_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0, days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n[1] 323   4\n\n\nTo illustrate the difference between using only a small window and all data, we plot the resulting regression lines. You can see that both approaches would lead to different results.\n\nCode# Plot dependent variable vs running variable\ndep_var &lt;-\n  ggplot(df, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.1, size = 0.2) +\n  # add lines for the full range\n  geom_smooth(data = filter(df, days_since_last &lt;= c0), \n              method = \"lm\", se = F, linewidth = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(df, days_since_last &gt; c0), \n              method = \"lm\", se = F, linewidth = 1, linetype = \"dashed\") +\n  # add lines for specified bandwidth\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, \n              color = ggthemr::swatch()[7], linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, \n              color = ggthemr::swatch()[7], linewidth = 2) +\n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var\n\n\n\nVisual comparison of purchases after coupon allocation.\n\n\n\nFrom the plot above, it is hard to see what the difference between observations close to the cut-off is. So what we can do is to compute two regressions, one for the observations in \\([55, 60)\\) and another one for the observations in \\([60, 65]\\).\nThen, using the resulting coefficients, we compute what both models predict for the cut-off value \\(c0\\) and take the difference. The difference is the local average treatment effect (LATE).\n\n# [3.2] Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n\n[1] \"LATE: 7.99\"\n\n\nIt’s a bit messy when we plot all observations, so let’s zoom in to see if we can detect the local average treatment effect graphically. Not surprisingly, it is equal to what we have just computed.\n\nCode# Minimum and maximum for y-axis limits\nmin_y &lt;- min(df_bw$purchase_after)\nmax_y &lt;- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw &lt;- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5], linewidth = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dotted\", color = ggthemr::swatch()[4]) +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dotted\", color = ggthemr::swatch()[4]) +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           color = ggthemr::swatch()[4], fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n\n\nTreatment effect for customers within +- 5 days."
  },
  {
    "objectID": "content/toolbox/09_rdd.html#estimation",
    "href": "content/toolbox/09_rdd.html#estimation",
    "title": "Regression Discontinuity",
    "section": "Estimation",
    "text": "Estimation\nWhat you will see most in studies is a regression to compute the LATE. That means, we can use the lm() command again. Instead of raw days_since_last variable, we prefer to use days_since_last_centered, which is the raw days_since_last variable centered, i.e. subtracted by the cut-off value \\(c_0\\). That simplifies the interpretation, however, it does not change the coefficient of interest, the LATE and also not the coefficient of days_since_last or days_since_last_centered. It merely shifts the intercept.\nThe coefficient we are most interested in is the one for couponTRUE. It is very to the effect in the plot above, but the regression summary also yields additional statistical information. We see that the LATE is statistically significant.\n\n# [4] Estimation ----\n# [4.1] Parametric ----\n# Compute coefficients for specified bandwidth.\nlm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n\n\nCall:\nlm(formula = purchase_after ~ days_since_last_centered + coupon, \n    data = df_bw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.497  -2.131  -0.095   2.019  10.416 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                11.424      0.396   28.81   &lt;2e-16 ***\ndays_since_last_centered    0.383      0.126    3.05   0.0025 ** \ncouponTRUE                  7.933      0.709   11.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.2 on 320 degrees of freedom\nMultiple R-squared:  0.707, Adjusted R-squared:  0.706 \nF-statistic:  387 on 2 and 320 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Content",
    "section": "",
    "text": "Welcome to “Causal Data Science for Business Analytics”!\nIn this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nA large global survey1 conducted among data science practitioners in the industry in 2020 states the importance of causal data science. 83% of the respondents consider causal inference in data-driven decisions making increasingly important and 44% state that, in their data science project, causal inference already plays an important role. Additionally,\n\n45% recognize the necessity to invest into causal inference in the future\n42% plan to intensify the training of their workforce in causal inference\n36% intend to hire talent in the field of causal inference\n\nWhile the primary goal of machine learning is typically the development of algorithms for a high prediction and classification accuracy, causal inference aims to understand and establish cause-and-effect relationships between variables.\nTypical applications in business therefore aim to answer questions like:\n\nDoes a customer loyalty program actually bind customers? Or do only customers sign up that are loyal anyways?\nDoes discount a product lead to more revenue? Or do only customers who would have bought anyways buy?\n\nMany successful companies have already recognized the advantages of causal data science. Click on the link to get more details how these companies are using tools from causal inference to generate value within their organizations.\n\n\nSample of companies involved in causal data science\n\n\nUber provides a Python package accompanied by a white paper suited for use cases such as campaign targeting optimization (identify customers most likely to respond to ad) and personalized engagement (find optimal personalized recommendation systems).\nLacking the possibility to measure the benefit of a new tool, AirBnB developed a method to mimic a randomized experiment called Artificial Counterfactual Estimation (ACE) leveraging causal inference and machine learning.\nData scientists at booking.com make use of encouragement design and instrumental variables to examine whether property partners can be reactivated by contacting.\nAt LinkedIn, methods from causal inference are used to extract effect estimates from observational data as randomized experiments are often not feasible. Four case studies dealing with job postings, free trials, user contributions and marketing campaigns are presented here.\nNetflix often uses A/B testing but sometimes refrains from its use in order to provide the same value to all customers. Then data scientists need to draw on observational data and e.g. use double machine learning to study the impact of localized content (through subtitles and dubs) or introduce causal inference into recommendation models to overcome the purely associative nature.\nGoogle provides an R package to help inferring causal impact from marketing campaigns on e.g. web searches, product installs or sales.\nMeta runs experiments to improve the user experience and reduce resource usage. Applying methods from causal inference and machine learning, Instagram users are provided with better notifications.\nMicrosoft brings together state-of-the-art machine learning techniques and statistics to tackle causal inference problems in its open source ecosystem for causal machine learning.\nZalando puts a focus on experimentation and A/B-testing in their business decision-making.\nData science teams at Lyft rely on large causal models to model customer decisions and driver incentives in order to manage their marketplace."
  },
  {
    "objectID": "content/index.html#mattermost",
    "href": "content/index.html#mattermost",
    "title": "Content",
    "section": "Mattermost",
    "text": "Mattermost\nIn the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you have any questions about the class content, coding problems and other challenges, please use our Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\nFollow these steps to join the channel:\n\nGo on https://communicating.tuhh.de/\n\nClick Click here to sign in\n\nClick the Button GitLab\n\nYou may need to login to GitLab with your Kerberos/LDAP data (e.g. cba1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\nAfter accessing Mattermost, join the team W-11 students\n\nJoin Causal Data Science Channel (you might need to wait a bit, as I first have to add you)\n\nThere, and in the sessions, I will try to help you as much as possible.\nIn order to keep the discussion efficient and manageable it is necessary that we all follow some basic rules:\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\nSee in this minimalistic example how little formatting makes your code and error easy to read.\n\n \n```r\nx %&gt;% sum()\n```\n\n\n\n**Error:**\nError in x %&gt;% sum() : could not find function \"%&gt;%\"\n\n\n\nHow to format your code and error when you ask for help."
  },
  {
    "objectID": "content/index.html#installing-r-rstudio-ide",
    "href": "content/index.html#installing-r-rstudio-ide",
    "title": "Content",
    "section": "Installing R & RStudio IDE",
    "text": "Installing R & RStudio IDE\nBefore we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -&gt; Global options -&gt; Appearance and switch theme in -&gt; Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab."
  },
  {
    "objectID": "content/index.html#introduction-to-r",
    "href": "content/index.html#introduction-to-r",
    "title": "Content",
    "section": "Introduction to R",
    "text": "Introduction to R\nBefore you begin coding, it’s important to remember this final point: do not let the errors, warnings, and other messages that you, like everyone else, are bound to encounter intimidate you. There is no reason to panic just because you see red text in your console and in fact, what is returned will often times already help you to solve the problem and lead you onto the right track.\nThere are three different types of texts that communicate issues or information about the code execution:\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\nMessages: these are just friendly texts that provide you with useful information. They do not need immediate attention but can provide useful supplementary information.\n\nInteractive Tutorials:\nBut let’s no more talk about it but instead start coding because the best way to get familiar with R and to code is to just start.\nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console.\nFeel free to complete as many tutorials as you want, but for this class, the following tutorial is of particular use: The R Programming Environment (Chapter 2-12)\nswirl()does not come with R by default but is an optional package. R packages are extensions of the base functionality implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nOnce: install the package. As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nAlways: load the package. By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later. It’s best to write it down so that you do not forget it.\n\nswirl()\n\nSelect the course you just installed: The R Programming Environment and start with Chapter 2. You should skip Chapter 1 because its irrelevant for you. If you accidentally selected Chapter 1, just quickly go through it and choose No at the last question.\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")\n\nIf, at some point, you want to take a break, you can leave the swirl course by typing bye() or the Esc key. You can return to the course by typing swirl() and hitting Enter. And remember, to use the same name you used the first time.\nYou don’t need to submit anything from this step. Just focus on getting familiar with R by completing the tutorial (I recommend to solve chapter 2-12)!\n\n\n\n\n\n\nNote\n\n\n\nWhenever you want to find out more about a command or you have difficulties understanding what it does, you can click on it and a help page will show up."
  },
  {
    "objectID": "content/index.html#footnotes",
    "href": "content/index.html#footnotes",
    "title": "Content",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.causalscience.org/blog/causal-data-science-in-practice/↩︎"
  },
  {
    "objectID": "content/fundamentals/02_reg.html",
    "href": "content/fundamentals/02_reg.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Statistical inference aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores or other business-related units.\nIn practice, it is often impossible to collect data about the whole population, which is why we draw (ideally random) samples from the whole population and use statistical inference to draw conclusions about the whole population using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#r",
    "href": "content/fundamentals/02_reg.html#r",
    "title": "Regression and Statistical Inference",
    "section": "R",
    "text": "R\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in R. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\nBefore we start, we load the R package tidyverse, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\n# Load tidyerse package\nlibrary(tidyverse)\n\nWe create a tibble, which is a table containing data, with two columns, \\(x\\) and \\(y\\). For \\(x\\), we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable \\(y\\), we make dependent on \\(x\\), i.e. for each unit \\(i\\), we create a \\(y_i\\) as\n\\[\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n\\]\nwhere \\(\\epsilon_i\\) is random noise.\nLet’s have a look at how the data is presented. We have a table containing 10 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n# Simluate data\n\n# number of observations\nn &lt;- 10\n\n# Create tibble\nlm_dat &lt;- tibble(\n  # draw from normal distribution\n  x = rnorm(n, mean = 3, sd = 1),\n  # y depends on x and noise from normal distribution\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\n# Show data\nlm_dat\n\n\n\n  \n\n\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n\n\n\n\n\nNote\n\n\n\nggplot2 is a data visualization package that comes with tidyverse. There are also base R graphics, but ggplot2 allows you to modify components with way more options. In its most basic form, the grammar works as follows:\n\nProvide the data: ggplot(data, …)\nDefine aesthetics: ggplot(data, aes(x = x, y = y)\nAdd geometric objects: e.g. + geom_bar()\n\n\n\n\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8) # change size and opacity of points\n\n\n\n\n\n\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between \\(X\\) and \\(Y\\). However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\nLet’s have a look at some random lines. Please note that there is an infinite amount of lines that we could draw but we stick to lines that seem somehow realistic.\n\n\n\n\n\n\n\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the residuals are zero.\n\n\n\n\n\n\n\n\nThe true best line is highlighted in red.\nIf we want to mathematically compute the line in R, we can use the lm() function and provide data and the assumed functional relationship as arguments. lm() is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: lm_mod), which we can interpret best when using summary() to show the resulting coefficients and other statistical information.\n\n\n\n\n\n\nNote\n\n\n\nlm() is a function that fits a linear model. You have to provide data and a regression equation in the form of for example outcome ~ regressor_1 + regressor_2 or outcome ~ ., if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call summary().\n\n\nLooking at the regression summary, we see that the line is modeled by \\(y = -0.1918 + 0.3354*x\\). It means that for the fitted model, an increase of one unit in \\(x\\) is related to an 0.3354 increase in \\(y\\). That is relatively close to what we simulated (0.3). But because we added random noise in our simulation it slightly differs.\n\n# Fit model and print summary\nlm_mod &lt;- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = y ~ x, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18597 -0.05210 -0.00783  0.06584  0.24115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.1918     0.1300   -1.48     0.18    \nx             0.3354     0.0443    7.57  6.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 8 degrees of freedom\nMultiple R-squared:  0.878, Adjusted R-squared:  0.862 \nF-statistic: 57.3 on 1 and 8 DF,  p-value: 6.48e-05\n\n\nNow, let’s check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations \\(y_i\\) and predictions \\(\\hat{y_i}\\) for each \\(i\\) or plotting the residuals \\(r_i = y_i - \\hat{y_i}\\) and comparing it to the \\(x\\)-axis.\n\n# Add fitted values and residuals to data\nlm_dat_fit &lt;- lm_dat %&gt;% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that although we are not able to perfectly capture each data point, the line on average is as close as possible. We cannot get any closer as the data points deviate in both directions."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#math",
    "href": "content/fundamentals/02_reg.html#math",
    "title": "Regression and Statistical Inference",
    "section": "Math",
    "text": "Math\nMathematically, the best line is found by the ordinary least squares (OLS) method.\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\nGiven \\(n\\) samples of observed pairs of dependent and independent variables \\(\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}\\), we plug all of them into the equation\n\\[\ny_i=\\beta_0+\\beta_1x_i+u_i\n\\]\nand together with our assumptions \\(E(u) = 0\\) and \\(E(u|x)=0\\) we obtain the equations to be solved to retrieve estimates for \\(\\beta_0\\) and \\(\\beta_1\\).\nFrom the independence of \\(x\\) and \\(u\\) and our understanding of probabilities and expectations, we also know that the expected value of the product of \\(x\\) and \\(u\\) has to be zero: \\(E(xu)=0\\). Substituting \\(u\\) with \\(y-\\beta_0-\\beta_1\\), we obtain the two conditions that when being solved give us the optimal estimates for our \\(\\beta\\) parameters.\n\\[\n\\begin{align}\nE(y-\\beta_0-\\beta_1x) = E\\Big(x[y-\\beta_0-\\beta_1x]\\Big) = 0\n\\end{align}\n\\]\nTranslated into its sample counterpart:\n\\[\n\\begin{align}\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\end{align}\n\\]\nLooking at the sample equations, we know our sample size \\(n\\), our sampled values \\(y_i\\) and \\(x_i\\). The coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\nSkipping a few transformation steps, we obtain\n\\[\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n\\]\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our \\(\\beta_1\\) is defined as the covariance of \\(X\\) and \\(Y\\) divided by the variance of \\(X\\). It also shows that the variance of \\(X\\) has to be greater than zero, which means that not all values of \\(x_i\\) can be the same. You need to observe different values to be able to estimate how \\(y_i\\) reacts to \\(x_i\\).\n\\(\\beta_0\\) follows directly by plugging \\(\\beta_1\\) into \\(\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta_1}\\overline{x}\\). A bar above a variable always represents the sample value of that particular variable. Thus, \\(\\beta_0\\) is (as expected) constant and linear in \\(\\beta_1\\).\nKnowing the equation for the regression line, we can compute fitted values \\(y_i\\) for all \\(i\\)\n\\[\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n\\]\nIn almost all cases however, \\(\\widehat{y}_i\\) won’t be equal to \\(y_i\\) but there will be a prediction error, commonly referred to as residual \\(\\widehat{u}_i\\). Make sure that you don’t mix it up with \\(u\\), the error term, which is always unobserved.\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\n\n\n\n\n\n\nInfo\n\n\n\nSquares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n\n\n\\[\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n\\]\nAgain, most of the residuals won’t be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#interpretation",
    "href": "content/fundamentals/02_reg.html#interpretation",
    "title": "Regression and Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\nSo let’s run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. Later, we will see why that might be dangerous and why we should be careful in selecting our variables.\n\n# Include all potential regressors\nlm_all &lt;- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n\n\nCall:\nlm(formula = expected_cost ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-406.5 -123.9  -26.4  102.7  942.0 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 450.51998   28.77380   15.66   &lt;2e-16 ***\nregionE       8.57388   16.25854    0.53   0.5981    \nregionS      -1.99015   16.66160   -0.12   0.9049    \nregionW       1.65634   16.93503    0.10   0.9221    \nsex          -9.38273   11.77360   -0.80   0.4257    \nsmoking     205.70391   13.21526   15.57   &lt;2e-16 ***\nage           9.29488    0.51525   18.04   &lt;2e-16 ***\nincome       -0.05113    0.00384  -13.30   &lt;2e-16 ***\nbmi          -2.07654    0.76406   -2.72   0.0067 ** \nchildren     -9.25879   14.55410   -0.64   0.5248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185 on 990 degrees of freedom\nMultiple R-squared:  0.404, Adjusted R-squared:  0.398 \nF-statistic: 74.4 on 9 and 990 DF,  p-value: &lt;2e-16\n\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\nThe estimate is what we know as regression coefficient from before, typically denoted by \\(\\widehat{\\beta}\\) or other Greek letters. Let’s take for example \\(\\widehat{\\beta_{age}}\\). As it is the estimated version, it has a hat. The estimate for \\(\\beta_{age}\\) tells you by how much the dependent variable \\(\\widehat{y}\\) varies when \\(age\\) is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect (in our example of \\(x_{age}\\)) ceteris paribus, Latin for “the others equal”. It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as controlling for the other variables."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#statistical-significance",
    "href": "content/fundamentals/02_reg.html#statistical-significance",
    "title": "Regression and Statistical Inference",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate. (Remember the graph from the previous chapter that showed the histogram of a random variable with different values for the standard deviation.)\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error. Most widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data.\nLet’s take the example of the correlation between smoking and expected_cost from the regression above. The null hypothesis states that there is no correlation between the dependent variable expected_cost and the independent variable smoking. The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis.The alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable. Because the p-value is low, we reject the null hypothesis and the alternative hypothesis (significant correlation) is true. If the previous paragraph wasn’t clear to you right away, please take a moment to reread it, as it contains crucial information.\nStatistical significance can directly be derived from the p-value and an arbitrary significance level \\(\\alpha\\). However, the most widely used level of \\(\\alpha\\) is \\(0.05\\). Less often used are levels of \\(.1\\), \\(.01\\) or \\(.001\\).\nAn estimate with a p-value less than \\(\\alpha\\) is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level \\(\\alpha\\). Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that \\(age\\), \\(income\\), \\(bmi\\), and \\(smoking\\) are statistically significant (at different levels though, indicated by the number of stars).\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A \\((1-\\alpha)\\) confidence interval has a probability of \\((1-\\alpha)*100 \\%\\) to contain the true value of our estimated coefficient. That means, if we would sample \\(100\\) times, \\(\\widehat{\\beta}\\) would be contained in the sample \\((1-\\alpha)*100\\) times.\n\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n\n              2.5 %  97.5 %\n(Intercept) 394.055 506.985\nregionE     -23.331  40.479\nregionS     -34.686  30.706\nregionW     -31.576  34.889\nsex         -32.487  13.721\nsmoking     179.771 231.637\nage           8.284  10.306\nincome       -0.059  -0.044\nbmi          -3.576  -0.577\nchildren    -37.819  19.302\n\n\n\n# alpha = 0.1\nconfint(lm_all, level = 0.90)\n\n                5 %    95 %\n(Intercept) 403.147 497.893\nregionE     -18.194  35.342\nregionS     -29.422  25.441\nregionW     -26.225  29.538\nsex         -28.767  10.001\nsmoking     183.946 227.461\nage           8.447  10.143\nincome       -0.057  -0.045\nbmi          -3.334  -0.819\nchildren    -33.221  14.703\n\n\nAn estimate whose interval is either completely positive or completely negative is different from zero and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the significant coefficient."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#model-selection",
    "href": "content/fundamentals/02_reg.html#model-selection",
    "title": "Regression and Statistical Inference",
    "section": "Model selection",
    "text": "Model selection\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\nLooking at the spread of residuals (\\(\\widehat{u}_i = y_i - \\widehat{y}_i\\)) we want them to be spread evenly around zero.\n\n# Plot histogram of residuals\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n\n\n\n\n\n\n\nWe can see that the residuals are in fact almost normally distributed.\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit, namely \\(R^2\\). It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\\[\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n\\]\nConveniently, \\(R^2\\) is always between \\(0\\) and \\(1\\) and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high \\(R^2\\) can even point to a biased model while a model with a low \\(R^2\\) can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of \\(R^2\\) does also not change the interpretation of the regression coefficients.\nA problem with \\(R^2\\) is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the \\(\\text{Adjusted} \\, R^2\\). It includes a term for the number of independent variables used.\n\\[\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n\\]\nwhere \\(n\\) is the sample size and \\(p\\) the number of independent variables. This way, you can compare models and account for their scarcity.\nLet’s build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n# Include only significant regressors\nlm_imp &lt;- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + smoking, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-481.5 -138.9  -27.2  112.5  931.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  392.959     27.560   14.26   &lt;2e-16 ***\nage            7.198      0.478   15.05   &lt;2e-16 ***\nbmi           -2.071      0.824   -2.51    0.012 *  \nsmoking      198.401     14.283   13.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201 on 996 degrees of freedom\nMultiple R-squared:  0.296, Adjusted R-squared:  0.294 \nF-statistic:  140 on 3 and 996 DF,  p-value: &lt;2e-16\n\n\nExcept for \\(bmi\\), coefficients are very similar. We’ll look into that in just a second. But first let us compare both models with regard to \\(\\text{(Adjusted)} \\, R^2\\).\n\n\n\n\n\n\nNote\n\n\n\nTo elegantly print variables in a specified format, you can use sprintf().\nIf you do not want to load a library with library() because you only need a few functions for a couple of times you can also use the following pattern: package::function().\n\n\n\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.40\"\n\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.29\"\n\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike’s Information Criteria \\(AIC\\) and Bayesian Information Criteria \\(BIC\\). Both \\(AIC\\) and \\(BIC\\) penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\n\n[1] \"AIC: 13294.02\"\n\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n\n[1] \"AIC: 13447.79\"\n\n\nFor both \\(AIC\\) and \\(BIC\\), the model with the lowest value is preferred.\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\nFor example, is it correct to assume a linear relationship between \\(bmi\\) and the outcome \\(expected\\_cost\\)? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term “linear”, so what can we do about it?\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. \\(bmi^2\\).\n\n# Include quadratic term for BMI\nlm_sq &lt;- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + I(bmi^2) + smoking, \n    data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-389.6  -80.3   -2.6   90.2  350.2 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1964.4792    41.7901    47.0   &lt;2e-16 ***\nage            7.8004     0.2919    26.7   &lt;2e-16 ***\nbmi         -122.6122     2.9778   -41.2   &lt;2e-16 ***\nI(bmi^2)       2.0803     0.0507    41.1   &lt;2e-16 ***\nsmoking      184.4309     8.7113    21.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122 on 995 degrees of freedom\nMultiple R-squared:  0.739, Adjusted R-squared:  0.738 \nF-statistic:  704 on 4 and 995 DF,  p-value: &lt;2e-16\n\n\nFrom the summary, we can see that including the square term significantly improves the model fit. Check for yourself the metrics \\(R^2\\) and \\(AIC\\) and plot the histogram of residuals.\nYou should now have seen, that it is extremely important to rely on theoretical considerations when building models.\n\n\n\n\n\n\nAssumptions\n\n\n\nThe following assumptions must be fulfilled for a linear regression to deliver valid results.\n\nLinearity: Relationship between \\(X\\) and \\(Y\\) is linear.\nHomoscedasticity: Variance of residual is the same for any value of \\(X\\).\nIndependence: Observations are independent of each other. Residuals are independent of each other.\nNormality: For any fixed value of \\(X\\), \\(Y\\) is normally distributed. Residuals of the model are normally distributed."
  },
  {
    "objectID": "content/fundamentals/03_caus.html",
    "href": "content/fundamentals/03_caus.html",
    "title": "Causality",
    "section": "",
    "text": "Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.\nHowever, there are many other contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure.\n\n\nDifference between Prediction and Explanation\n\nInstead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to understand characteristics of the data-generating process, i.e. the system of cause and effect and extract useful information from the data. That is what science is about, explaining why things are happening.\n\nExample 1 - confounding factors:\nA simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.\nBut what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. Ideally, we would want to look at a hotel at one specific point in time where all factors are fixed and then observe the number of bookings for different prices. In practice, this is impossible but causal methods try to get as close as possible to that scenario. Only then, we can extract valid estimates and are able to understand the underlying mechanism which help us and businesses to take the right actions and decisions.\nExample 2 - direction of causation:\nAnother example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong. Many machine learning models cannot take such prior knowledge into consideration when building models and will therefore yield wrong estimates. Causal methods, however, built on a research and identification strategy to include prior knowledge.\nBased on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains, that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#why-causality",
    "href": "content/fundamentals/03_caus.html#why-causality",
    "title": "Causality",
    "section": "",
    "text": "Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.\nHowever, there are many other contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure.\n\n\nDifference between Prediction and Explanation\n\nInstead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to understand characteristics of the data-generating process, i.e. the system of cause and effect and extract useful information from the data. That is what science is about, explaining why things are happening."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#illustrative-examples",
    "href": "content/fundamentals/03_caus.html#illustrative-examples",
    "title": "Causality",
    "section": "",
    "text": "Example 1 - confounding factors:\nA simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.\nBut what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. Ideally, we would want to look at a hotel at one specific point in time where all factors are fixed and then observe the number of bookings for different prices. In practice, this is impossible but causal methods try to get as close as possible to that scenario. Only then, we can extract valid estimates and are able to understand the underlying mechanism which help us and businesses to take the right actions and decisions.\nExample 2 - direction of causation:\nAnother example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong. Many machine learning models cannot take such prior knowledge into consideration when building models and will therefore yield wrong estimates. Causal methods, however, built on a research and identification strategy to include prior knowledge.\nBased on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains, that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "href": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "title": "Causality",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nNow, let’s think again about the research question. How can we find out what the benefit of having parking spots is? Ideally, we would be able to compute the individual treatment effect (\\(ITE\\)) of each store \\(i\\). That means, for each store, we would know what the sales would be with and without parking spots. Then we could take the difference of those two outcomes and we would know what part of the sales would be only attributable to having parking spots. This is called the individual treatment effect (\\(ITE\\)):\n\\[\n\\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\n\\(Y_{i1}\\) are sales when there are parking spots at store \\(i\\) and \\(Y_{i0}\\) are sales when there are no parking spots at store \\(i\\). However, observing both outcomes is impossible.\nTo compute the individual treatment effect we would have to know the amount of sales that would have happened if a store without parking spots had parking spotrs. Not being able to observe an observation unit in both states (= with and without treatment/parking spots) is called the fundamental problem of causal inference, essentially a missing data problem.\nThis is why technically the outcomes \\(Y_{i1}\\) and \\(Y_{i0}\\) are potential outcomes. To come from potential outcomes to the observed outcome, we can use the switching equation. For example for store \\(A\\):\n\\[\n\\begin{align}\nY_A &= D_AY_{A1} + (1-D_A)Y_{A0} \\\\\n&= 0*Y_{A1} + 1*Y_{A0} \\\\\n&= Y_{A0}\n\\end{align}\n\\]\nWe are able to observe \\(Y_{A0}\\), the sales for store \\(A\\) having no parking spots, but we are not able to observe \\(Y_{A1}\\), the state in which store \\(A\\) would have parking spots. But to estimate a individual causal effect, we would have to know what happens when we intervene and when we don’t intervene.\n\\(Y_{A1}\\) and \\(Y_{A0}\\) are potential outcomes, of which the one actually happened is called factual and the one that did not happen is called counterfactual. Note, that they describe outcomes for the same unit and although we cannot observe one of them, we can still define it mathematically."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#average-treatment-effect",
    "href": "content/fundamentals/03_caus.html#average-treatment-effect",
    "title": "Causality",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nFor now, we will leave the ITE behind and focus on a metric that is more accessible in analyses, the average treatment effect (ATE). The average treatment is defined as\n\\[\n\\text{ATE} = E[Y_1 - Y_0] \\,\\,,\n\\]\nthe expected difference in outcomes under both states. So the causal effect is defined as a comparison between two states of the world, the actual or factual state compared to the never observed counterfactual world.\nOther forms of average treatment effects are the average treatment effect on the treated (ATT) and the average effect on the untreated (ATU).\n\\[\n\\begin{align}\nATT = E[Y_1 - Y_0|D = 1] \\\\\nATU = E[Y_1 - Y_0|D = 0]\n\\end{align}\n\\]\nNow let’s ignore the fundamental problem of causal inference for a minute and imagine the impossible scenario that we would be able to observe all outcomes for all stores for all different states. That means, we would be able to magically know the sales of each stores with and without parking spots. Just for illustration, the unobserved outcomes are crossed out, but we’ll still use them for computation.\n\n\n\\(i\\)\n\\(Y_{i0}\\)\n\\(Y_{i1}\\)\n\\(D_i\\)\n\\(Y_i\\)\n\\(\\text{ITE}\\)\n\n\n\n\\(A\\)\n135\n145\n0\n135\n+10\n\n\n\\(B\\)\n121\n125\n0\n121\n+4\n\n\n\\(C\\)\n74\n102\n1\n102\n+28\n\n\n\\(D\\)\n68\n94\n1\n94\n+26\n\n\n\nKnowing all states, we would be able to easily compute the average treatment effect by averaging the last column \\(ITE\\). The \\(ATE\\) is the average of all \\(ITE\\) and in this impossible scenario, we actually know the true estimate:\n\\[\n\\text{ATE} = \\frac{1}{4}(28 + 26 + 10 + 4)= 17\n\\]\nWe can already see that for the treated stores, the ones with parking spots, the treatment effect is way higher. We can show that by calculating the average treatment effect for the treated (\\(D_i = 1\\)) and for the untreated (\\(D_i=0\\)).\n\\[\n\\begin{align}\nATT &= \\frac{1}{2}(28+26) = 27 \\\\\nATU &= \\frac{1}{2}(10+4) = 7\n\\end{align}\n\\]\nBut again, we cannot see the table as it is shown above but instead, what we would see is the following table.\n\n\nStore\n\\(y_0\\)\n\\(y_1\\)\n\\(d\\)\n\\(y\\)\n\\(\\text{ITE}\\)\n\n\n\n\\(A\\)\n135\n-\n0\n135\n-\n\n\n\\(B\\)\n121\n-\n0\n121\n-\n\n\n\\(C\\)\n-\n102\n1\n102\n-\n\n\n\\(D\\)\n-\n94\n1\n94\n-\n\n\n\nOne idea you could come up with is to compare the mean of treated units to the mean of untreated units and take the difference as the ATE. Treated units are called the treatment group while untreated units are called control group. Knowing the true average treatment effect from our hypothetical table above, let’s see how it works.\n\\[\n\\text{ATE} = E[Y|D=1] -E[Y|D=0] = \\frac{102+94}{2} - \\frac{135+121}{2} = -30\n\\]\nThis would leave us with an average treatment effect of \\(-30\\), which is is very far away from our true estimate of \\(+27\\). In fact, it even goes in the other direction. This is why we need to be extremely careful when attempting to prove causal effects. Naive estimations and simple methods might not only under- or overestimate the effect or not identify a true effect, but they could get it even completely wrong."
  },
  {
    "objectID": "content/fundamentals/04_dag.html",
    "href": "content/fundamentals/04_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "We have already learned that correlation and causation can easily be confused. Now, we will define concepts and acquire tools that help us to develop credible identification strategies to separate correlation from causation. One essential part is graphically modeling your theoretical knowledge about the data-generating process.\n\n\n\n\n\n\nImportant\n\n\n\nData-generating process (DGP) refers to the underlying mechanisms that generate the data we observe. It represents real-world processes that produce the data points we analyze.\n\n\nIn causal inference, directed acylic graphs (DAGs) do the graphic modeling part. They are the foundation of any analysis strategy and communicate your research plan.\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come from things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nA DAG should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?\n\nBesides being helpful in guiding your analysis and identification strategy, DAGs also show your research design to your audience.\nA simple example of a DAG could be the effect of having a university degree on future salary: at first, it might be intuitive to say that future salary increases when you get an university degree.\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# without confounder\nschooling_1 &lt;- dagify(\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3),\n                y = c(uni_degree = 1, salary = 1))\n)\n\n# Plot DAG\nggdag(schooling_1, use_labels = \"name\", text = F) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n# with confounder\nschooling_2 &lt;- dagify(\n  uni_degree ~ ability,\n  salary ~ ability,\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3, ability = 2),\n                y = c(uni_degree = 1, salary = 1, ability = 2))\n)\n\n# Plot DAG\nggdag(schooling_2, text = F) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = name))\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut zooming out and thinking about why a university degree is correlated with higher salaries could lead you to the conclusion that both university degree and salary are influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree.\nIt is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#chain",
    "href": "content/fundamentals/04_dag.html#chain",
    "title": "Directed Acyclic Graphs",
    "section": "Chain",
    "text": "Chain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\nCode# Chain\nchain &lt;- dagify(\n  Y ~ Z,\n  Z ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 0, X = 0))\n)\n\n# Plot DAG\nggdag(chain) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nAn example of such a causal mechanism (page 37, Pearl) could be the effect of work hours on training and training on race time. In the DAG, the variables would be:\n\n\n\\(X\\): work hours\n\n\\(Z\\): training\n\n\\(Y\\): race time\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nIn terms of dependencies,\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow. If you work longer hours, you can train less.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow. If you train more, your race time improves.\n\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)). More work leads to less training and worse race time.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on the training amount, that means we hold training amount fixed at a particular level, then there is no effect from work hours to race time as there is no direct effect, but only an effect through the amount of training. In other words, for individuals that differ in the hours they work but still have the same amount of training, there is no association between working hours and race time.\n\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#fork",
    "href": "content/fundamentals/04_dag.html#fork",
    "title": "Directed Acyclic Graphs",
    "section": "Fork",
    "text": "Fork\nAnother mechanism is the fork, also called common cause.\n\nCode# Fork\nfork &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0))\n)\n\n# Plot DAG\nggdag(fork) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\n\n\n\\(X\\): ice cream sales\n\n\\(Z\\): temperature\n\n\\(Y\\): number of crimes\n\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nAgain, let’s check in term of dependencies:\n\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow. Higher temperature leads to more ice cream sales.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow. Higher temperature leads to more crimes.\n\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\). Variation in temperature affects ice cream sales and number of crimes simultaneously.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\n\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collision",
    "href": "content/fundamentals/04_dag.html#collision",
    "title": "Directed Acyclic Graphs",
    "section": "Collision",
    "text": "Collision\nThe last mechanism is the collision, which is also called common effect.\n\nCode# Collider\ncollider &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1))\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\n\\(X\\): attractiveness\n\n\\(Y\\): talent\n\n\\(Z\\): celebrity\n\nFirst of all, in the general population, there is no correlation between attractiveness and talent (3rd dependency). Second, being either attractive or having a talent will help you to become a celebrity (1st and 2nd dependency).\nBut what about the last dependency? Why are attractiveness and talent suddenly correlated when conditioned on e.g. being a celebrity? That is because when you know someone is a celebrity and has no talent, the likelihood that he/she is attractive increases because otherwise he/she would likely not be a celebrity. Vice versa, if you know someone is a celebrity and is not attractive, he/she is probably talented in some form.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\))."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#confounding",
    "href": "content/fundamentals/04_dag.html#confounding",
    "title": "Directed Acyclic Graphs",
    "section": "Confounding",
    "text": "Confounding\nA very common problem when trying to prove causal effects is confounding.\n\nCode# Confounder\nconfounding &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0)),\n  labels = list(X = \"university degree\",\n                Y = \"salary\",\n                Z = \"ability\")\n)\n\n# Plot DAG\nggdag(confounding) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\nIt is the same DAG as in the introduction. So just recall and imagine \\(X\\) was university degree, \\(Y\\) future salary and \\(Z\\) ability.\nThere are two paths from \\(X\\) to \\(Y\\):\n\ndirect path \\(X \\rightarrow Y\\)\nbackdoor path \\(X \\leftarrow Z \\rightarrow Y\\)\n\nFirst of all, it is important to clarify what effect we are actually interested in and that is the direct causal effect of university degree on future salary, \\(X\\) on \\(Y\\).\nThe indirect effect is not causal but is only spurious correlation induced by \\(Z\\) which we are generally not interested in. It is called backdoor path.\nSo how do we proceed to extract that effect? We need to somehow remove the association between \\(X\\) and \\(Y\\) that is only due to variation in \\(Z\\). And from the previous section we know how to do that. \\(X\\) and \\(Y\\) need to be independent conditional on \\(Z\\) and block the path from \\(X\\) to \\(Y\\) over \\(Z\\).\nBut what does blocking the path mean? It means that we have to condition on \\(Z\\), to keep it at a fixed level. Then, the variations in \\(X\\) that cause \\(Y\\) to vary are not due to \\(Z\\) because it does not vary at all and cannot have an impact on either \\(X\\) or \\(Y\\). Doing that we closed the backdoor and are able to retrieve the causal effect.\nNot blocking the path would falsify our results and is what is called the omitted variable bias. That is why \\(Z\\) is called confounder, because it confounds the ability to measure the causal effect.\nHowever, the main problem is that in many cases you might not be able to block the path for two different reasons:\n\nYou are aware of the confounder, but you did not collect data for it\nYou are not aware of the confounder (and probably did not collect data for it either)\n\nThis stresses the importance of theoretical knowledge about the phenomenon you are researching. Without it, it is very unlikely that you can prove truly causal effects. The risk of not paying attention to confounders gets clear when we look at graphs visualizing an example of Simpson’s paradox: accounting for a third variable reverses the sign of correlation.\nWe can illustrate it with an imaginary example. Let’s assume you want to measure how a specific characteristic affects salary. So you start to collect data about both variables, throw them into a regression and your result tells you that there is a positive correlation. But what happens if you take the education level into account? You can see how the lines show a positive correlation on the left and a negative correlations on the right. Including a third variable, namely the confounder, the relationship reverses.\nIt is neither wrong or right to always include or exclude variables, but it depends on the application and question you want to answer. This is why causal reasoning is so important.\n\nCode# Simpson's paradox ----\n\n# Simulate data\n# number of observations\nn &lt;- 1e+03\n\n# draw and create variables with specific dependencies\neducation &lt;- rbinom(n, 2, 0.5)\ncharacter &lt;- rnorm(n) + education\nsalary &lt;- education * 2 + rnorm(n) - character * 0.3\n\n# rescale to realistic values\nsalary &lt;- sample(10000:11000,1) + scales::rescale(salary, to = c(0, 100000))\ncharacter &lt;- scales::rescale(character, to = c(0, 7))\neducation &lt;- factor(education, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# create tibble\ndf &lt;- tibble(\n  salary,\n  character,\n  education\n)\n\n# Not conditioning on education\nsimps_not_cond &lt;- ggplot(df, aes(x = character, y = salary)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on education  \nsimps_cond &lt;- ggplot(df, aes(x = character, y = salary, color = education)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsimps_not_cond\nsimps_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collider",
    "href": "content/fundamentals/04_dag.html#collider",
    "title": "Directed Acyclic Graphs",
    "section": "Collider",
    "text": "Collider\nYou probably noticed that the confounding example was related to the common cause in the previous section. The next example is related to the common effect mechanism.\n\nCode# Collider   \ncollider &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1)),\n  labels = list(X = \"GPA\",\n                Y = \"musical talent\", \n                Z = \"accepted\")\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\nNow consider the following example: a research group wants to examine if there is a causal relationship between grade point averages (\\(X\\)) and musical talent (\\(Y\\)). They can use data from their own university where both is measured for every student as scoring high in one of these characteristics substantially increases your chance of being accepted.\nNow the researchers, who hypothesized a positive or no correlation between \\(X\\) and \\(Y\\), perform a simple analysis and, to their surprise, find out that there is a strong negative correlation. This strange result can be explained by collider bias. Implicitly, as they only used data from their own university students, they also conditioned on a collider \\(Z\\), which is “accepted to university” and is fixed on a constant level of actually being accepted to the university.\nAgain, we have two paths from \\(X\\) to \\(Y\\), but one thing is different: the backdoor path is already closed because \\(Z\\) is a collider, both arrows point toward it.\nTo correct their results they would need to have data from the whole population and not only students from a university that puts focus on either grade point averages or musical talent. Having a full population as a sample would probably lead to the result that there is no correlation between \\(X\\) and \\(Y\\).\nThis case (collider) is different to the case before (confounding), where conditioning was the correct solution. But if you have a collider in you DAG, make sure not to condition on it as it creates a dependence between \\(X\\) and \\(Y\\). You “open” a backdoor path that was closed before, just due to the presence of the collider.\nCollider bias often arises when your sample is not very representative of the population you are making claims about. How that could change your result can be seen in an example Berkson’s paradox. While there is no correlation for the whole population, for smaller subgroups there are. And therefore, it is crucial that you clearly state what effect you are interested in.\n\nCode# Berkson's paradox ----\n\n# Simulate data\n# number of observations\nn &lt;- 1e+03\n# draw and create data with specific dependencies\nability     &lt;- rnorm(n)\nmotivation  &lt;- rnorm(n)\naptitude    &lt;- 1/2 * ability + 1/2 * motivation + rnorm(n, 0, .1)\n\n# create tibble\ndf &lt;- tibble(\n  ability    = ability,\n  motivation = motivation,\n  aptitude   = aptitude,\n  student    = ifelse(aptitude &gt; 0, \"student\", \"no_student\")\n)\n\n# Not conditioning on student\nberk_not_cond &lt;- ggplot(df, aes(x = motivation, y = ability)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on student  \nberk_cond &lt;- ggplot(df, aes(x = motivation, y = ability,\n                            color = student, \n                            alpha = student)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_color_manual(values = c(\"student\" = ggthemr::swatch()[4],\n                                \"no_student\" = ggthemr::swatch()[5])) +\n  scale_alpha_manual(values = c(\"student\" = 1, \"no_student\" = 0.2)) +\n  theme(legend.position = \"right\")\n\n# Plot both\nberk_not_cond\nberk_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#d-separation",
    "href": "content/fundamentals/04_dag.html#d-separation",
    "title": "Directed Acyclic Graphs",
    "section": "d-separation",
    "text": "d-separation\nOne concept, that we have not named yet but implicitly used, is d-separation. If an effect of \\(X\\) on \\(Y\\) is d-separated, there is no statistical association that can flow between \\(X\\) and \\(Y\\) except for the direct effect. In fact, d-separation determines conditional independence.\nD-separation formalizes what we have already learned when going through the tree types of association.\nPractically1, it is easier to define the opposite, d-connection:\n\nRule 1: unconditional separation: \\(X\\) and \\(Y\\) are d-connected if there is an unblocked path between them. (As an example, imagine a confounder, that is not conditioned on.)\nRule 2: blocking by conditioning: \\(X\\) and \\(Y\\) are d-connected, conditioned on a set of \\(Z\\) nodes, if there is a collider-free path between \\(X\\) and \\(Y\\) that traverses no member. (Think of a mediated effect that takes away parts from the direct effect.)\nRule 3: conditioning on colliders: If a collider is a member of conditioning set \\(Z\\), or has a descendant in \\(Z\\), then it no longer blocks any path that traces this collider. (Image the collider example in the previous section.)\n\nKnowing these rules and having mapped our assumptions into the DAG allows us to treat observational data like experimental data and simulate interventions as we would have conducted an experiment. However, it is not a silver bullet as in many cases data availability will stop you from isolating the causal effect. For example, if you do not observer a confounder, you cannot control for it. All empirical work requires theory and with observational data we need to be extra careful to make sure to actually extract the effects we are interested in."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "href": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "title": "Directed Acyclic Graphs",
    "section": "Backdoor/Frontdoor Criterion",
    "text": "Backdoor/Frontdoor Criterion\nSpecial cases that are derived from d-separation rules are the backdoor and frontdoor criterion/adjustment.\nTo satisfy the backdoor criterion, we have to make sure all backdoors are closed, which, as already mentioned, differs for confounders and colliders.\n\n\n\n\nAgain, we want to block all other paths between \\(X\\) (treatment) and \\(Y\\) (outcome). So depending on the structure of the DAG, the following can block a path:\n\na chain or a fork whose middle node is in \\(Z\\)\na collider that is not conditioned on, which means it is not in \\(Z\\)\n\nThe frontdoor criterion, which is actually a consecutive application of the backdoor criterion, is a bit more complicated and we will leave it out for now, but in the chapter about instrumental variables we will deal with it extensively."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "href": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "title": "Directed Acyclic Graphs",
    "section": "Algorithms to identify causally valid estimates",
    "text": "Algorithms to identify causally valid estimates\nIf DAGs become more complex because there are a lot of variables that are somehow related, we can make use of algorithms to check all the rules for us.\nOne application to help in such cases is http://dagitty.net/dags.html, where you can draw your DAG, define what is the treatment and outcome, which variables are observed and unobserved and many other things. Then it will show you what kind of adjustment is necessary to estimate the causal effect of interest.\n\n\n\n\nTry to build this graph on dagitty.net and look what useful information you can get from the site.\ndagitty is also implemented in R and combined with ggdag you are also able to plot your DAGs in a easy manner and obtain information needed for designing your research.\nFirst, let’s just plot the DAG. When we define exposure, which is a different term for intervention or treatment, and the outcome, they are highlighted in another color. When drawing the DAG yourself, you can leave out theme_dag_cds() as it is only a custom theme that I have defined for this website. Instead you could use other themes such as theme_dag(), theme_dag_grey(), theme_dag_blank() or proceed without any theme.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model &lt;- 'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nD -&gt; Y\nD -&gt; Z3\nZ1 -&gt; D\nZ1 -&gt; Y\nZ2 -&gt; Y\nZ2 -&gt; Z3\nZ3 -&gt; Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nLet’s check what paths there are from treatment \\(D\\) to \\(Y\\). Of course, there is a direct path (the causal path) and there are two other paths, one of which is closed.\n\n# find all paths\npaths(dag_model)\n\n$paths\n[1] \"D -&gt; Y\"             \"D -&gt; Z3 &lt;- Z2 -&gt; Y\" \"D &lt;- Z1 -&gt; Y\"      \n\n$open\n[1]  TRUE FALSE  TRUE\n\n\nWe can also plot the open paths. One path is already blocked by a collider (remember: we do not want to condition on a collider).\n\n# plot paths\nggdag_paths(dag_model) +\n  theme_dag_cds()\n\n\n\n\n\n\n\nTo see what we have to adjust for to isolate the causal effect, we use adjustmentsSets(). As you might have figured out already, it is \\(Z1\\) that needs to be conditioned on.\n\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n\n{ Z1 }\n\n\nA very concise summary plot is returned by the function ggdag_adjustment_set(), which shows what needs to be adjusted, the open paths and the whole DAG.\n\n# plot adjustment sets\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nBut what does it actually mean in practice? How do we block a path or condition on a variable?\nIf you use a linear regression, including a variable as an independent variable is the sames as conditioning on it. If you use other models, you might have to use subsets and average them or do some kind of matching where you only compare units that have the same value for the variables that has to be conditioned on.\nIn the following chapters, we will deal with a large variety of techniques and figure out clever ways to isolate causal effects."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#footnotes",
    "href": "content/fundamentals/04_dag.html#footnotes",
    "title": "Directed Acyclic Graphs",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttp://bayes.cs.ucla.edu/BOOK-2K/d-sep.html↩︎"
  },
  {
    "objectID": "content/fundamentals/01_b_stats.html",
    "href": "content/fundamentals/01_b_stats.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Now we will talk about some statistical concepts, that are the foundation for modeling processes in both statistical and causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to read here."
  },
  {
    "objectID": "content/fundamentals/01_b_stats.html#footnotes",
    "href": "content/fundamentals/01_b_stats.html#footnotes",
    "title": "Statistical Concepts",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://ourworldindata.org/human-height↩︎"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html",
    "href": "content/fundamentals/01_a_prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Before we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part.\n\n\n\n\n\n\nAssignments\n\n\n\nPlease note: in this chapter, there are two assignments in between. In the other chapters, you’ll usually find the assignments at the end."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "href": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "title": "Probability Theory",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nConsider the most simple example: flipping coins. We define the outcome of the flip of a coin as a random variable as we are uncertain about what side the coin lands on. To express this uncertainty, we make us of probability theory.\nAfter flipping the coin, we will see what side the coin has landed on and our random variable has taken on of the two possible events \\(\\{H, T\\} \\subseteq \\Omega\\). It will be either Head or Tail.\nSo we have already defined two terms: random variable and events. Now what is a probability? A probability is always linked to an event typically denoted by a capital letter, here either \\(H\\) and \\(T\\), and expresses how likely this event is to happen. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\\[\nP(H) = P(T) = 0.5\n\\]\nExtreme cases: If an event \\(A\\) is impossible, its probability is \\(P(A) = 0\\) and if it is certain to occur, it is \\(P(A)=1\\).\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 1: Probability is a real number greater or equal to 0.\n\n\nWe can also introduce the compliment \\(\\overline{A}\\), which is what happens when \\(A\\) does not happen and consequently, \\(P(A) + P(\\overline{A}) = 1\\). \\(A\\) and \\(\\overline{A}\\) are mutually exclusive, by definition. But there could also be two events \\(A\\) and \\(B\\) that are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\\[\nP(H \\cup T) = P(H) + P(T) = 1\n\\]\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 2: Total probability is equal to 1.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 3: Probability of mutually exclusive events is the sum of the probabilities. (Mutually exclusive: events can’t happen at the same time)\n\n\nTo understand what not mutually exclusive events are, consider events \\(studying\\) and \\(working\\). For a random person, we don’t know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\nThen, the probability of at least one of the events happening is calculated by\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nwith \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the addition rule.\n\n\n\n\n\n\nTip\n\n\n\n\\(\\cup\\) : Union, can be translated as “or”.\n\\(\\cap\\) : Intersection, can be translated as “and”.\n\n\n\n\n\n\nFor mutually exclusive events:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B)\n\\]\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule,\n\\[\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n\\]\nwhere \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened. It is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\nUsing the example with workers and students: without knowing exact numbers, we can assume that students are less likely to work than individuals who are not studying.\n\\[\nP(working|studying) &lt; P(working|\\overline{studying})\n\\]\nEssentially, we are looking at probabilities restricted to a subset of the sample, which in this comparison are the subsamples of studying persons and non-studying persons.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways. Let’s think of rolling a die twice (first roll \\(R_1\\) and second roll \\(R_2\\)).\n\\[\nP(R_2 \\mid R_1) = P(R_2)\n\\]\nThe second roll does not depend on the first one. With each roll the outcomes \\({1, 2, 3, 4, 5, 6}\\) have the same probability likely independent of the previous roll. If we want to compute the probability of both rolls being a \\(6\\), we would just have to multiply the probabilities for each roll.\n\\[\nP(R_1 = 6 \\cap R_2 = 6) = P(R_1 = 6) \\ P(R_2 = 6)\n\\]"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#probability-tree",
    "href": "content/fundamentals/01_a_prob.html#probability-tree",
    "title": "Probability Theory",
    "section": "Probability Tree",
    "text": "Probability Tree\nLet’s go back to the case where events are dependent on each other. An intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events happens. The probability of two consecutive events is obtained by multiplying the probabilities.\nConsider the following example: you are project manager and based on your are interested in the probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Using historical data about past projects, you come up with the following tree."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#assignment-i",
    "href": "content/fundamentals/01_a_prob.html#assignment-i",
    "title": "Probability Theory",
    "section": "Assignment I",
    "text": "Assignment I\n\nDefine being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\). (Hint: Check here, if you are not sure what is shown in the probability tree.)\nThen, compute the following probabilities and the sum of all four probabilities.\n\n\\(P(T \\cap S)\\)\n\\(P(T \\cap \\overline{S})\\)\n\\(P(\\overline{T} \\cap S)\\)\n\\(P(\\overline{T} \\cap \\overline{S})\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWith some browsers and specific operating systems, the compliment probability is not shown correctly (missing the horizontal bar above the letter). In that case it often helps to zoom in or out.\n\n\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to successfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#set-theory",
    "href": "content/fundamentals/01_a_prob.html#set-theory",
    "title": "Probability Theory",
    "section": "Set Theory",
    "text": "Set Theory\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams that are based on set theory. We already used a simple one above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\nLet’s use an example to understand some other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices: smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current usage distribution.\nInstead of using actual data, we simulate the data collection process here. If you are interested how to do it in R, you can expand and check out the code by clicking on Code. But you don’t have to. And don’t worry, if it looks too complicated at this point, just move on.\n\n\n\n\n\n\nNote\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R.\ntibble() is the most convenient way to create tablets. You specify column name and content and assign your tibble to an object to store it.\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\nmutate() is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %&gt;% mutate(new_variable = existing_var / 100).\n\n\n\nCode# Load tidyverse package\nlibrary(tidyverse)\n\n# Number of obervations\nn &lt;- 1000\n\n# Create tibble\napp_usage &lt;- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer. Please note that \\(1\\) indicates usage and \\(0\\) indicates no usage.\n\n\n\n\n\n\nNote\n\n\n\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\n\n\n# Show first ten lines\nhead(app_usage, 10)\n\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n\n\n\n\n\nNote\n\n\n\nSumming all values by column is done by colSums(table). To sum rows, you can use rowSums(table).\n\n\n\n# Show column sums\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n\n\nThe sum of \\(user\\_id\\) does not really tell us anything. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n\n\n\n\n\nNote\n\n\n\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\n\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %&gt;% select(smartphone, tablet, computer) %&gt;% colSums()\n\nsmartphone     tablet   computer \n       589        389        226 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n\nGeneric Venn diagram\n\n\n\n\n\n\n\nNote\n\n\n\nwhich() checks a condition and returns the indices.\n\n\n\n# Set of phone, tablet and computer users\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\nAssignment II\n\nUsing the Venn diagram above, answer the following questions.\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\n\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to successfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions.\n\n\n\nYou can also use the example to go through the basic probability rules defined above (that does not belong to the assignment anymore).\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - P(T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "href": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "title": "Probability Theory",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMath\nA very important theorem in probability theory is Bayes theorem. In fact, it has been called the most powerful rule of probability and statistics. Let’s quickly go through the math. By reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nand using the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem expresses a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before. With the Bayes theorem you can answer questions like:\n\nHow likely is it that it will rain, when there are clouds in the morning?\nHow likely is it that an email is spam if certain keywords appear?\n\n\n\n\n\n\n\nTip\n\n\n\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]\n\n\nApplication\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) \n\\(P(B|\\overline{A}) = 0.01\\) \n\\(P(A) = 0.04\\) \nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B)\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nAssignment III\n\nCompute\n\n\n\\(P(\\overline{A}|B)\\) (1)\n\n\\(P(A|B)\\) (2)\n\nand fill the gaps in the following sentence:\nThese results show that in case the alarm is triggered, there is a possibility of about __% that the product is flawless and a probability of __% that the product is faulty.\n\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to successfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions."
  },
  {
    "objectID": "content/toolbox/05_rct.html",
    "href": "content/toolbox/05_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Let’s recall the fundamental problem of causal inference: we are not able to observe individual treatment effects. Only one potential outcome can be observed because there is only one state of the world.\nBut that does not mean we have to give up. There are ways to estimate the treatment effect and arguably, the most promising way to deal with this problem is randomization of the observation units and in particular randomized experiments, also known as randomized controlled trials (RCTs). Due to their statistical rigor and simplicity, RCTs are called the gold standard of causal inference.\nRCTs do not solve the fundamental problem of only observing one potential outcome but instead treatment and control group are randomized such that both groups are expected to be almost equal. Having similar groups that either received or not received treatment, we can calculate a valid causal estimate, the Average Treatment Effect (ATE). But it is only due to the randomization of observation units (e.g. individuals) that we are able to interpret it causally. If you remember the example with parking spots from a previous chapter, the computed \\(ATE\\) can also return invalid estimates if units are not randomized."
  },
  {
    "objectID": "content/toolbox/05_rct.html#identification",
    "href": "content/toolbox/05_rct.html#identification",
    "title": "Randomized Controlled Trials",
    "section": "Identification",
    "text": "Identification\nTwo assumptions are crucial for the ATE to be interpreted causally.\nIndependence assumption\nWe have to assume independence between the potential outcomes and the treatment assignment, i.e. treatment assignment to a unit hast nothing to do with the size of treatment effect for a unit. In other words, it is not only those in the treatment group who benefit the most or the least from the treatment.\n\\[\nD_i \\perp (Y_{i0}, Y_{i1})\n\\]\nThis is where we exploit randomization. We can actually ensure that there is no association between potential outcomes and treatment by randomly assigning observation units to control and treatment group.\nThis way, both groups will be very similar on average, both in observed and in unobserved characteristics. They will only differ in their treatment status and possibly in the observed outcome, which makes the estimation of a causal effect possible.\nPlease make sure, that you understand the formula correctly. It does not mean that there is no treatment effect. It means the (potential) outcome under \\(D=0\\) or \\(D=1\\) is not affected by whether a particular observation unit does or does not receive the treatment. However, the observed outcome \\(Y_i\\) might depend on \\(D_i\\), and in fact, that is the effect we are interested in.\nA related way to express it is\n\\[\nE[Y_0|D=0] = E[Y_0|D=1]\n\\]\nRegardless of the treatment value a unit receives, the expected (but not always observed) potential outcome is the same in both treatment (\\(D=1\\)) and control group (\\(D=0\\)). The mean potential outcome is equal for both groups.\nThis does also imply equality of ATE and ATT, as there is no bias and the association we see is equal to the causation. Remember, we saw that \\(ATE \\neq ATT\\) when there is selection bias, i.e. observation units chose to be treated or not to be treated. But in case of randomization, by definition, selection bias cannot occur.\nWhen is the independence assumption violated?\nAn example, where the independence between treatment and potential outcomes is not given is if the treatment assignment is not randomized but people are able to self-select into on of the groups.\nThen, it could happen that for e.g. more motivated people would choose the treatment and when motivation had an impact on the potential outcome, e.g. more motivated people have a higher outcome for both potential outcomes compared to less motivated people, that are more likely to be in the control group. Under these circumstances, the independence assumption would be violated.\nSUTVA\nThe second assumption that needs to be fulfilled is the stable unit treatment value assumption (SUTVA).\nIt ensures that there is no interference between units. In other words, one unit’s treatment does not affect outcomes of other units. If unit \\(i\\) received a treatment, than this treatment of unit \\(i\\) should have no effect on another unit.\nImplicitly, the assumption states that there are only two potential outcomes for each unit and they only depend on a unit’s own treatment status.\nWhen is the SUTVA violated?\nIn situations where observation units are somehow clustered like e.g. in classrooms, departments or other kind of groups, violations of SUTVA can occur.\nAs an example, imagine you are running a company and select a few of your employees to participate in a program that teaches them about safety measures. After the program, it is very likely that they share some of the program content with their colleagues in their department, who might not have been selected for participation. Then, there are spillover effects.\nTo deal with violations of SUTVA you could change your selection process or change the level of analysis (analyzing clusters instead of individuals)."
  },
  {
    "objectID": "content/toolbox/05_rct.html#randomization",
    "href": "content/toolbox/05_rct.html#randomization",
    "title": "Randomized Controlled Trials",
    "section": "Randomization",
    "text": "Randomization\nIn practice, randomization is done automatically by software programs but to get an intuition, you could also think of it as e.g. flipping a coin for each observation unit or individual and assigning units that get head to the treatment group, while units that get tail are assigned to the control group (or the other way around).\nIn fact, that is already a special case because the probability of being treated and being not treated is 50% for both cases. But treatment probabilities could also take different values for a variety of reasons, for example because treatment is costly. However, you need to ensure that both groups are large enough to be comparable in order to fulfill the independence assumption.\nLet’s see what that means. We assume that we have a population of 1’000 individuals which we want to learn something about. Using runif() and rbinom(), we synthetically generate this population with random values for the characteristics \\(age\\) and \\(sex\\) and we also assign each individual to either treatment or control group.\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Define population size\nn &lt;- 1e+4\n\n# Create population with two characteristics\nX &lt;- tibble(\n  age = runif(n, 18, 65), # draw random values from uniform distribution\n  sex = rbinom(n, 1, 0.5), # draw random values from binomial distribution\n  id = 1:n\n)\n\n# Add randomly generated treatment variable\ndf &lt;- tibble(X, treat = rbinom(n, 1, .5))\ndf\n\n\n\n  \n\n\n\nRemember, randomization of treatment should achieve that we are able to interpret the average treatment effect causally and for that, both groups need to be as similar as possible. The image illustrates the randomization process. Try to think what could happen if you have just very few units in both groups. How likely is is that they are very similar regarding their characteristics? You can probably already sense that this might not be sufficient to make groups comparable.\n\n\nIllustration of randomization process\n\nBut let’s try it out and see how average group characteristics develop when we change the sample size.\n\nviewof N = Inputs.range(\n  [10, 10000], \n  {value: 10, step: 10, label: \"Number of individuals:\"}\n)\n\n\n\n\n\n\n\n\n\ntheme = ({\n  style: \"background-color: #0F2231\",\n})\n\nmutable yDomain = [10, 100]\n\nPlot.plot({\n  ...theme,\n  facet: {data: filtered, x: \"variable\"},\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.dot(filtered, {x: \"treat\", y: \"mean\", stroke: \"#00C1D4\", tickSize: 1, fill: \"#00C1D4\"}),\n    Plot.tickY(filtered, {x: \"treat\", y: \"ci_l\", stroke: \"#00C1D4\"}),\n    Plot.tickY(filtered, {x: \"treat\", y: \"ci_h\", stroke: \"#00C1D4\"}),\n    Plot.ruleX(filtered, {x: \"treat\", y1: \"ci_l\", y2: \"ci_h\", stroke: \"#00C1D4\"})\n  ],\n  y: {domain: yDomain}\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data).filter(function(df) {\n  return N === df.N;\n})\n\n\n\n\n\n\nAs you can see in the plots, group average characteristics converge with increasing sample size. The more units are assigned to either group, the less differences are between the groups and thus, the independence assumption, stating that groups only differ by their treatment status, is fulfilled. But although you need a minimum amount of units, there is not much improvement after increasing the sample size way beyond that (also, the difference is already really small)."
  },
  {
    "objectID": "content/toolbox/05_rct.html#average-treatment-effect",
    "href": "content/toolbox/05_rct.html#average-treatment-effect",
    "title": "Randomized Controlled Trials",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nJust because we can, we use the whole sample. That means there should be about ~500 units per group. There are many suggested rules and guidelines to choose the right sample size, but for now, we will disregard it as our data is simulated and therefore, we do not have any data problems.\nSo far, we have just looked at the covariate balance but have not included the outcome variable. Let’s do that now. In the background I simulated the outcome after treatment and added the column outcome to our table.\n\n# Show data with outcome variable\ndf_out\n\n\n\n  \n\n\n\nAs already mentioned, having balanced baseline characteristics between treatment and control group allows us to estimate the average treatment effect.\nBut how do we calculate the average treatment effect? We can just take a simple difference in means to estimate it. By the way, groups can be of different group size. It is only important, that they are comparable in their characteristics.\nLet’s compute the average outcome per group. We see that there seems to be a difference, the average outcome in the treatment group is higher.\n\n# Group by treatment group and compute average outcome\ndf_out %&gt;% \n  group_by(treat) %&gt;% \n  summarise(mean_outcome = mean(outcome))\n\n\n\n  \n\n\n\nGenerally, it is recommendable to use a linear regression to get an estimate of the treatment effect. You don’t have to manually compute the difference and additionally, the output of lm() and summary() yields information to be used for statistical inference. Then, we see that this effect is in fact highly statistically significant. The effect is equal to the difference of the two values just seen above. Check it out (small differences possible due to rounding)!\n\n# Compute ATE with linear regression\nlm_ate &lt;- lm(outcome ~ treat, data = df_out)\nsummary(lm_ate)\n\n\nCall:\nlm(formula = outcome ~ treat, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.207  -4.261  -0.038   4.197  24.837 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.0337     0.0885   -0.38      0.7    \ntreat        11.1626     0.1246   89.57   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.2 on 9998 degrees of freedom\nMultiple R-squared:  0.445, Adjusted R-squared:  0.445 \nF-statistic: 8.02e+03 on 1 and 9998 DF,  p-value: &lt;2e-16\n\n\nOne way to present your results to your audience could be a boxplot that on the one hand shows the difference of regressors by group and on the other hand the difference of outcomes. Here we will show the 95% confidence intervals for our estimates and it can be seen that there is a substantial difference between both groups. However, for our independent variables, age and sex , both groups are very similar.\n\n# Plot independent and and dependent difference\n# age (independent)\ncompare_age &lt;- \n  ggplot(df_out, \n         aes(x = treat, \n             y = age, \n             color = as.factor(treat))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Age\", title = \"Difference in age\")\n\n# sex (independent)\ncompare_sex &lt;- \n  ggplot(df_out, \n         aes(x = treat, \n             y = sex, \n             color = as.factor(treat))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Sex\", title = \"Difference in sex\")\n\n# outcome (dependent)\ncompare_outcome &lt;- \n  ggplot(df_out, \n         aes(x = treat, \n             y = outcome, \n             color = as.factor(treat))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n\n# Plot age, sex and outcome differences for both groups\ncompare_age\ncompare_sex\ncompare_outcome\n\n\n\n\nTreatment and control group overlap regarding average age.\n\n\n\nTreatment and control have similar proportion in terms of gender.\n\n\n\nDifferences in outcome can therefore be ascribed to treatment variable.\n\n\n\n\nBut why did we not include \\(age\\) and \\(sex\\) into our regression? Because they are similarly distributed across both groups it should not change the treatment effect. But still, they might have an impact on the outcome, as well. Although being similarly distributed in both groups, they can still vary within each group. So let’s see what happens if we include them.\nBoth regressors turn out to be significant. However, as expected, the treatment effect is almost unchanged. If there had not been a covariate balance between the two groups, our treatment effect would have been biased.\n\n# Include other regressors\nlm_all &lt;- lm(outcome ~ treat + age + sex, data = df_out)\nsummary(lm_all)\n\n\nCall:\nlm(formula = outcome ~ treat + age + sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.987  -3.908  -0.086   3.900  21.746 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.70708    0.19990   -23.6   &lt;2e-16 ***\ntreat       11.14857    0.11324    98.5   &lt;2e-16 ***\nage          0.05264    0.00419    12.6   &lt;2e-16 ***\nsex          4.98913    0.11324    44.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.7 on 9996 degrees of freedom\nMultiple R-squared:  0.542, Adjusted R-squared:  0.542 \nF-statistic: 3.94e+03 on 3 and 9996 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "content/toolbox/05_rct.html#subgroup-analysis",
    "href": "content/toolbox/05_rct.html#subgroup-analysis",
    "title": "Randomized Controlled Trials",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nThe significance of \\(age\\) and \\(sex\\) could also indicate that there are different treatment effects across different levels of both covariates. Then, a so called interaction/moderation effect would be covered behind the statistical coefficients.\nA moderation effect expresses different strengths of the treatment for different subgroups. For example older people might benefit relatively more from the treatment than younger people. And women might benefit more from treatment compared to men.\nIn R, we include interaction effects by using either using a product x1*x2 or a colon x1:x2. In fact, when we do that, the interactions are significant and the treatment effect changes substantially.\nNote, that now the treatment effect differs for everyone dependent on their age and sex. Now it requires a bit of addition to obtain the treatment effect. Moreover, it is not the \\(ATE\\) anymore, but instead the conditional average treatment effect \\(CATE\\) as it depends on other covariates.\n\n# Include interaction\nlm_mod &lt;- lm(outcome ~ treat * age + treat * sex, data = df_out)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = outcome ~ treat * age + treat * sex, data = df_out)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-19.19  -3.45   0.02   3.39  18.78 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.21009    0.24269   -0.87     0.39    \ntreat        2.42849    0.34049    7.13  1.1e-12 ***\nage          0.00561    0.00530    1.06     0.29    \nsex         -0.11579    0.14276   -0.81     0.42    \ntreat:age    0.08841    0.00745   11.86  &lt; 2e-16 ***\ntreat:sex   10.10470    0.20116   50.23  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5 on 9994 degrees of freedom\nMultiple R-squared:  0.639, Adjusted R-squared:  0.639 \nF-statistic: 3.54e+03 on 5 and 9994 DF,  p-value: &lt;2e-16\n\n\nNote also that \\(R^2\\) has increased with each addition to the regression. But why do you think the main treatment effect is not significant anymore?\nAs the data is simulated, we can check what the true data-generating process is and based on that evaluate what regression equation provides the best solution. Check for yourself what model should be used.\n\\[\noutcome = 2*treat + 0.1*treat*age + 10*treat*sex + \\epsilon\n\\]\nAgain, it shows how crucial theoretical knowledge of the phenomenon you are studying is. Imagine a situation with a high number of regressors. Testing out all potential variables as moderators requires some effort and might even lead to results just due to chance. You should therefore plan your research design and hypotheses beforehand. As a matter of fact, many scientific publications therefore have to define a pre-analysis plan1."
  },
  {
    "objectID": "content/toolbox/05_rct.html#footnotes",
    "href": "content/toolbox/05_rct.html#footnotes",
    "title": "Randomized Controlled Trials",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://blogs.worldbank.org/impactevaluations/a-pre-analysis-plan-checklist↩︎"
  },
  {
    "objectID": "content/toolbox/06_match.html",
    "href": "content/toolbox/06_match.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Almost always, the problem we are trying to solve in causal inference relates to the fundamental problem of causal inference, the fact that we cannot observe two states for one particular observation unit, e.g. we cannot see how a person’s health status after taking a specific drug and after not taking the same drug. Consequently, we cannot know the individual treatment effect.\nThus, we said we can make use of averages and try to estimate the average treatment effect by taking the difference between a group of treated observation units and a group of untreated observation units. Written in a formula, we can present it as\n\\[\nATE = E[Y_1| D = 1] - E[Y_0 | D = 0]\n\\]\nAssuming that individuals (or other kinds of observation units) were randomly assigned for example to take the drug, then this result gives us exactly what we want. It compares the outcomes of groups under treatment with groups not under treatment (control).\nBut the estimate hinges on the assumption that both groups are comparable, formally\n\\[\nE[Y_0|D=0] = E[Y_0|D=1]\n\\]\nwhich we cannot test but in randomized settings we have good reason to believe it to be true. Under these circumstances, average treatment effect (\\(ATE\\)) and the average treatment effect on the treated (\\(ATT\\)) are equal.\nHowever, if there are underlying group differences because the treatment assignment was not randomized and e.g. individuals were able to choose their treatment, we are not measuring the estimate that we are interested in. Then, the difference between \\(ATE\\) and \\(ATT\\) is what we call selection bias, an unmeasured factor representing systematic bias:\n\\[\nATE = ATT + \\text{selection bias}\n\\]\nGraphically, we can show an example of when the naive estimate would fail.\n\n\n\n\nEffect of D on Y is confounded by variation in Z. In other words, there are two paths from D to Y, one direct path and a backdoor path via Z.\n\n\n\nBy now, you might recognize what kind of problem the DAG depicts: confounding. A variable \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) and to estimate the causal effect of \\(X\\) on \\(Y\\), we need to close the backdoor path of \\(X\\) to \\(Y\\) via \\(Z\\)."
  },
  {
    "objectID": "content/toolbox/06_match.html#single-matching-variable",
    "href": "content/toolbox/06_match.html#single-matching-variable",
    "title": "Matching and Subclassification",
    "section": "Single Matching Variable",
    "text": "Single Matching Variable\nThe most simple scenario would be to match on a single variable. In practice, it is rare that only one variable is a potential confounder but for the purpose of practice, we’ll start with this scenario. Consider a scenario where you are employed by a bank and need to make a decision regarding whether to issue a credit card to a customer who has previously experienced credit difficulties.\nLet’s delve into specific methods in depth, and then transition towards a more practical application-focused approach.\n\n\n\n\nGeneric DAG with matching. By fixing the level of confounder, backdoor is closed and valid estimation of causal treatment effect for that specific group at level of confounder.\n\n\n\nApplied to credit card example. Effect of being late on settling credict card debt in the past on being late again. Confounded by size of credit card debt. It could be suspected that customers with higher debts in the past are unlikely to be financially recovered by now. Therefore, we have to condition on the level of the previous bill."
  },
  {
    "objectID": "content/toolbox/06_match.html#coarsened-exact-matching",
    "href": "content/toolbox/06_match.html#coarsened-exact-matching",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching",
    "text": "(Coarsened) Exact Matching\nWhen you think of the previous credit card example, you might have asked yourself:\nHow likely is it that when fixing a continuous variable such as previous bill, we can find matching treatment and control observations? I.e. we have a treated and an untreated observation with exactly the same previous bill.\nIt is in fact very unlikely, and to address this issue we rely on coarsening continuous variables. That means, we create different levels/bins for the size of the previous bill, which could be for example: 1-50€, 51-250€, 251-1000€ and so on. This increases the likelihood of finding matches substantially. However, we also lose a bit of precision. In general: the wider the bins, the more matches but the less precision.\nNow, however, there might be even more than one match for a particular observation. In this case, you could compute an average or weight the matches based on their distance to the focal observation."
  },
  {
    "objectID": "content/toolbox/06_match.html#nearest-neighbor-matching",
    "href": "content/toolbox/06_match.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching",
    "text": "Nearest-Neighbor Matching\nDistance also plays a crucial role when taking more than one matching variable into account. Let’s add the variable \\(age\\) and plot the two dimensions.\nLess less strict than exact or coarsened matching, matching on distance finds observations that are close. The distance between observations is computed using distance measures such as the distance or the Euclidean distance and close matches are identified e.g. by nearest-neighbor matching or by using kernel weights.\n\n\n\n\nInitial situation: six potential matches.\n\n\n\nFour best matches are selected.\n\n\n\nMatches are only accepted if they fall into specified vicinity.\n\n\n\nUsing nearest-neighbor matching, for each two observations, a scalar is returned calculated from the distances of all matching variables. Based on that value, \\(k\\) matching observations are selected to build the control group. If \\(k \\neq 1\\), you also have to decide whether to weight the control observations on their distance. Weighting the observations makes the approach less sensitive to the choice of \\(k\\) because less importance observations are down-weighted. Eventually the weights goes to zero.\nKernel weights are similar but define a vicinity observations have to fall into."
  },
  {
    "objectID": "content/toolbox/06_match.html#propensity-score",
    "href": "content/toolbox/06_match.html#propensity-score",
    "title": "Matching and Subclassification",
    "section": "Propensity score",
    "text": "Propensity score\nWith an increasing number of matching variables nearest-neighbors matching becomes unfeasible, as well. To reduce the dimensionality onto one dimension, for each observation a propensity score can be computed. The propensity score expresses the estimated probability of treatment. In absence of selection bias, propensity scores should be very similar across treatment and control group. Thinking back to our probability chapter, the propensity score is:\n\\[\nP(D_i = 1|\\mathbf{X}_i)\n\\]\nWe’ll discuss how to use it later in detail but essentially, we exploit that there are units in the treatment group that were unlikely to be treated and vice versa. The most recommended method that uses propensity scores as matching input is called inverse probability weighting and weights each observation by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight."
  },
  {
    "objectID": "content/toolbox/06_match.html#multiple-matching-variables",
    "href": "content/toolbox/06_match.html#multiple-matching-variables",
    "title": "Matching and Subclassification",
    "section": "Multiple Matching Variables",
    "text": "Multiple Matching Variables\nLet us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.\nNow you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age. Let’s assume for demonstration purposes that these are the only confounding factors. In practice, there might be more, however.\nWe can use a DAG to think about the correct identification strategy. Using dagitty and ggdag we see that we need to close two backdoor paths: initial health status and age.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Define DAG\ndag_model &lt;- 'dag {\n  bb=\"0,0,1,1\"\n  \"Health Program\" [exposure,pos=\"0.25,0.2\"]\n  \"Initial Health Status\" [pos=\"0.35,0.25\"]\n  \"Sick Days\" [outcome,pos=\"0.35,0.2\"]\n  Age [pos=\"0.25,0.25\"]\n  \"Initial Health Status\" -&gt; \"Health Program\"\n  \"Initial Health Status\" -&gt; \"Sick Days\"\n  Age -&gt; \"Health Program\"\n  Age -&gt; \"Initial Health Status\"\n  Age -&gt; \"Sick Days\"\n}'\n\n# DAG with adjustment sets (and custom layout)\nggdag_adjustment_set(dag_model, shadow = T, text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag_cds() + # custom layout (can be left out)\n  geom_dag_point(color = ggthemr::swatch()[2]) + # custom color, can take any color\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = name))\n\n\n\nDAG showing what needs to be accounted for.\n\n\n\nLet’s load the data (you probably have to change the path) and have a glance at it.\n\n# Read data\ndf &lt;- readRDS(\"../../datasets/health_program.rds\")\n\n\n# Show data \ndf\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTrue Treatment Effect\n\n\n\nBecause the data is simulated, we know the true treatment effect: 0.5\nCompare it to the estimates in the following sections!\n\n\nA naive estimate would be obtained by just regressing sick_days on health_program.\n\n# Naive estimation (not accounting for backdoors)\nmodel_naive &lt;- lm(sick_days ~ health_program, data = df)\nsummary(model_naive)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.546 -1.546 -0.248  0.752 20.454 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.2483     0.0328   220.9   &lt;2e-16 ***\nhealth_programTRUE   1.2980     0.0464    27.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 9998 degrees of freedom\nMultiple R-squared:  0.0725,    Adjusted R-squared:  0.0724 \nF-statistic:  781 on 1 and 9998 DF,  p-value: &lt;2e-16\n\n\nThe naive estimate is far off. But we already suspected that treatment assignment was not random, so let’s see if we can improve the validity of our estimation using matching procedures."
  },
  {
    "objectID": "content/toolbox/06_match.html#coarsened-exact-matching-1",
    "href": "content/toolbox/06_match.html#coarsened-exact-matching-1",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching",
    "text": "(Coarsened) Exact Matching\nAgain, in case of exact matching, only observations that share the same values in (coarsened) matching variables are matched in. To perform Coarsened Exact Matching (CEM) you can use the MatchIt package in R. We provide a formula containing our treatment dependent on the matching variables, the data, what method to use ('cem' = Coarsened Exact Matching) and what estimate we are interested in. Please note we regress the treatment on the matching variables. The outcome variable is not included at this stage.\n\n# Load 'MatchIt' library\nlibrary(MatchIt)\n\n# Without specifying coarsening\n# (1) Matching\ncem &lt;- matchit(health_program ~ age + sick_days_before,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n\nUsing the summary() function we can check how well balanced the covariates are compared to before. The balance before is titled Summary of Balance for All Data and after Summary of Balance for Matched Data. In this case, they are almost perfectly balanced after matching.\n\n# Covariate balance\nsummary(cem)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\")\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.3          44.7            0.17       0.94     0.045    0.067\nsick_days_before           4.2           3.5            0.37       1.54     0.034    0.177\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.5          45.5          -0.001          1     0.002    0.009            0.11\nsick_days_before           3.8           3.8           0.008          1     0.001    0.017            0.10\n\nSample Sizes:\n              Control Treated\nAll              5006    4994\nMatched (ESS)    4707    4700\nMatched          5002    4941\nUnmatched           4      53\nDiscarded           0       0\n\n\nNow, we can use the matched data and see how the coefficient changes. Actually, it changes quite a lot. Even when at first glance, the covariates were not too different before matching.\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(sick_days ~ health_program, data = df_cem, weights = weights)\nsummary(model_cem)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_cem, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-8.319 -1.447 -0.483  0.960 14.772 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.5864     0.0318   238.9   &lt;2e-16 ***\nhealth_programTRUE   0.5296     0.0450    11.8   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 9941 degrees of freedom\nMultiple R-squared:  0.0137,    Adjusted R-squared:  0.0136 \nF-statistic:  138 on 1 and 9941 DF,  p-value: &lt;2e-16\n\n\nLooking at our matching command matchit() you might have noticed that we did not specify any cut points to perform the coarsened matching. Instead the MatchIt module took care of that in the background. But we can also choose to specify cut points.\nWe’ll see that we are able to decrease the imbalance, but not to the same degree as the algorithm did it. Feel free to check out if you can get any closer.\n\n# Custom coarsening\n# (1) Matching\ncutpoints &lt;- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))\ncem_coars &lt;- matchit(health_program ~ age + sick_days_before,\n                     data = df, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n\n# Covariate balance\nsummary(cem_coars)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.3          44.7            0.17       0.94     0.045    0.067\nsick_days_before           4.2           3.5            0.37       1.54     0.034    0.177\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.3           0.025       0.99     0.006    0.016            0.47\nsick_days_before           3.9           3.7           0.118       1.09     0.012    0.090            0.59\n\nSample Sizes:\n              Control Treated\nAll              5006    4994\nMatched (ESS)    4857    4840\nMatched          5006    4989\nUnmatched           0       5\nDiscarded           0       0\n\n\nWe can also visualize the subsamples and see how data points are weighted. Weights depend on how many treated and control units there are in a specific subsample. You can see that in the top-right corner, for example. From the plot we can also see that the cut-points specified by us are too broad as matches could be way closer.\n\n# Use matched data\ndf_cem_coars &lt;- match.data(cem_coars)\n\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sick_days_before,\n                         size = weights, color = as.factor(health_program))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sick_days_before),\n              mapping = aes(intercept = y, slope = 0), \n              linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  geom_vline(data.frame(y = cutpoints$age),\n              mapping = aes(xintercept = y),\n             linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  theme(legend.position = \"none\")\n\n\n\nCoarsening: each subsample only contains similar units. If there are just a few treated observations, the toal weight of control units is lower for a specific subsample.\n\n\n\nWith custom coarsening, we again get another coefficient. It could indicate that this way, backdoors are not properly closed because we our cut points were defined too broadly.\n\n# (2) Estimation\nmodel_cem_coars &lt;- lm(sick_days ~ health_program, data = df_cem_coars, \n                      weights = weights)\nsummary(model_cem_coars)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_cem_coars, \n    weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-8.401 -1.445 -0.307  0.839 26.167 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.5126     0.0330   227.6   &lt;2e-16 ***\nhealth_programTRUE   0.7530     0.0467    16.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 9993 degrees of freedom\nMultiple R-squared:  0.0253,    Adjusted R-squared:  0.0252 \nF-statistic:  260 on 1 and 9993 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "content/toolbox/06_match.html#nearest-neighbor-matching-1",
    "href": "content/toolbox/06_match.html#nearest-neighbor-matching-1",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor matching",
    "text": "Nearest-Neighbor matching\nFor nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular distance measures used to find so called nearest neighbors is the Mahalanobis distance.\nAgain, we use MatchIt to conduct the matching process. We just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching. The result differs from (coarsened) exact matching but again, we have almost perfect balance.\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn &lt;- matchit(health_program ~ age + sick_days_before,\n              data = df,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"nearest\", distance = \"mahalanobis\", replace = T)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.3          44.7            0.17       0.94     0.045    0.067\nsick_days_before           4.2           3.5            0.33       1.54     0.034    0.177\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       46.3          46.3          -0.000          1     0.001    0.005           0.011\nsick_days_before           4.2           4.2           0.001          1     0.000    0.002           0.002\n\nSample Sizes:\n              Control Treated\nAll              5006    4994\nMatched (ESS)    1769    4994\nMatched          2643    4994\nUnmatched        2363       0\nDiscarded           0       0\n\n\nAnd also the estimated average treatment effect is very similar to the one obtained by (default) CEM.\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(sick_days ~ health_program, data = df_nn, weights = weights)\nsummary(model_nn)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_nn, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-8.942 -1.546 -0.546  1.237 29.865 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          8.0182     0.0486  164.84   &lt;2e-16 ***\nhealth_programTRUE   0.5280     0.0602    8.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.5 on 7635 degrees of freedom\nMultiple R-squared:  0.00999,   Adjusted R-squared:  0.00986 \nF-statistic: 77.1 on 1 and 7635 DF,  p-value: &lt;2e-16\n\n\n\n\n\n\n\n\nCurse of dimensionality\n\n\n\nWith exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don’t find any matches at all.\nRegarding exact matching, consider for example the situation with two covariates with each five distinct values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates ~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations."
  },
  {
    "objectID": "content/toolbox/06_match.html#inverse-probability-weighting",
    "href": "content/toolbox/06_match.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability weighting",
    "text": "Inverse Probability weighting\nOne way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let’s go into more detail.\nEstimating Propensity Score\nWe start by understanding what probability in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as propensity score.\nHere, we will use a logistic regression for prediction. A logistic regression, opposed to a linear regression, is designed for outcomes that are between 0 and 1, such as probabilities. The coefficients are a bit more difficult to interpret, so we’ll leave that for now. But what we see is, that age and sick_days_before are relevant and positive predictors for the probability of treatment.\n\n# (1) Propensity scores\nmodel_prop &lt;- glm(health_program ~ age + sick_days_before,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n\nCall:\nglm(formula = health_program ~ age + sick_days_before, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.1851     0.1000  -11.85  &lt; 2e-16 ***\nage                0.0102     0.0021    4.87  1.1e-06 ***\nsick_days_before   0.1898     0.0116   16.37  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13863  on 9999  degrees of freedom\nResidual deviance: 13497  on 9997  degrees of freedom\nAIC: 13503\n\nNumber of Fisher Scoring iterations: 4\n\n\nFor each observation we can compute a probability and add it to a table. It is important to specify type = \"response\" in the predict() command to obtain probabilities.\n\n# Add propensities to table\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\nHaving obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as propensity score matching. However, there are several reasons why this is not a good identification strategy1, mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that vice versa, covariate values indeed imply the same propensity score.\nWeighting by Propensity Score\nInstead inverse probability weighting (IPW) has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use the propensity score of an observation unit to increase or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\\[\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n\\]\nwhere only one of the terms is always active as \\(D_i\\) is either one or zero. Now we should better understand what “inverse probability weighting” actually means. It weights each observation by its inverse of its treatment probability. Let’s compute it for our data.\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(health_program, age, sick_days_before, propensity, ipw)\n\n\n\n  \n\n\n\nImagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. \\(D_i=1\\). Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let’s assume it was 0.8, for demonstration. Then his/her weight would equal \\(w_i = \\frac{1}{0.8} = 1.25\\).\nCompared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let’s say his/her probability of participating would be 0.2. Then, his/her weight would be \\(w_i = \\frac{1}{0.2} = 5\\). So we see, he/she would obtain a significantly higher weight.\nIn general, IPW weights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group.\nRunning a linear regression with weights as provided by IPW yields a coefficient not as precise as the ones obtained by previous matching procedures.\n\n# (2) Estimation\nmodel_ipw &lt;- lm(sick_days ~ health_program,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_ipw, weights = ipw)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-12.11  -2.31  -0.28   1.26  93.19 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.7567     0.0352  220.12   &lt;2e-16 ***\nhealth_programTRUE   0.4213     0.0500    8.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 9998 degrees of freedom\nMultiple R-squared:  0.00706,   Adjusted R-squared:  0.00697 \nF-statistic: 71.1 on 1 and 9998 DF,  p-value: &lt;2e-16\n\n\nA reason for that could be that there are some extreme and very atypical observations with either very high or low probabilities. Those observations than get a very high weight.\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n\n\nMost predicted treatment probabilities are between 0.4 and 0.6. Only a few outliers.\n\n\n\n\n# Looking for observations with highest weights\ndf_ipw %&gt;% \n  select(health_program, age, sick_days_before, propensity, ipw) %&gt;% \n  arrange(desc(ipw))\n\n\n\n  \n\n\n\nTo avoid assigning extreme weights, a proposed best practice is to trim the sample2 and filter out all observations with a propensity score less than 0.15 and higher than 0.85. Doing that,the coefficient is very close to the true treatment effect.\n\n# Run with high weights excluded\nmodel_ipw_trim &lt;- lm(sick_days ~ health_program,\n                data = df_ipw %&gt;% filter(propensity %&gt;% between(0.15, 0.85)),\n                weights = ipw)\nsummary(model_ipw_trim)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_ipw %&gt;% filter(propensity %&gt;% \n    between(0.15, 0.85)), weights = ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-12.034  -2.180  -0.517   1.287  23.904 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.6553     0.0324   236.1   &lt;2e-16 ***\nhealth_programTRUE   0.4776     0.0459    10.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.2 on 9956 degrees of freedom\nMultiple R-squared:  0.0107,    Adjusted R-squared:  0.0106 \nF-statistic:  108 on 1 and 9956 DF,  p-value: &lt;2e-16\n\n\nOpposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the probability model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders."
  },
  {
    "objectID": "content/toolbox/06_match.html#comparison",
    "href": "content/toolbox/06_match.html#comparison",
    "title": "Matching and Subclassification",
    "section": "Comparison",
    "text": "Comparison\nComparing the methods, we see that some of them were yield results very close to the true treatment effect. The naive estimation, however, was far off. Also, using custom cut-points (that were too wide) for coarsening also reduces the accuracy.\n\n# Summary of naive and matching methods\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                \"CEM1\"  = model_cem,\n                                \"CEM2\"  = model_cem_coars,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n\n\n\n\nNaive\nCEM1\n CEM2\nNN\n IPW1\n IPW2\n\n\n\n(Intercept)\n7.248\n7.586\n7.513\n8.018\n7.757\n7.655\n\n\n\n(0.033)\n(0.032)\n(0.033)\n(0.049)\n(0.035)\n(0.032)\n\n\nhealth_programTRUE\n1.298\n0.530\n0.753\n0.528\n0.421\n0.478\n\n\n\n(0.046)\n(0.045)\n(0.047)\n(0.060)\n(0.050)\n(0.046)\n\n\nNum.Obs.\n10000\n9943\n9995\n7637\n10000\n9958\n\n\nR2\n0.072\n0.014\n0.025\n0.010\n0.007\n0.011\n\n\nR2 Adj.\n0.072\n0.014\n0.025\n0.010\n0.007\n0.011\n\n\nAIC\n45227.3\n44559.1\n45463.2\n36140.1\n46896.8\n44959.1\n\n\nBIC\n45248.9\n44580.7\n45484.8\n36160.9\n46918.5\n44980.7\n\n\nLog.Lik.\n-22610.644\n-22276.549\n-22728.594\n-18067.050\n-23445.417\n-22476.548\n\n\nRMSE\n2.32\n2.24\n2.32\n2.43\n2.36\n2.23"
  },
  {
    "objectID": "content/toolbox/06_match.html#footnotes",
    "href": "content/toolbox/06_match.html#footnotes",
    "title": "Matching and Subclassification",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://gking.harvard.edu/publications/why-propensity-scores-should-not-be-used-formatching↩︎\nhttps://onlinelibrary.wiley.com/doi/full/10.1002/sim.6607↩︎"
  },
  {
    "objectID": "content/toolbox/08_iv.html",
    "href": "content/toolbox/08_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "The method we introduce in this chapter is called instrumental variables estimation (IV), a quasi-experimental method heavily used in economics to identify causal effects in observational studies with unobserved confounders. Thus, it can control for omitted variable bias which is present when we were either not able to collect all relevant data or we are not aware of some confounders.\nDue to not observing particular confounders, we cannot close the backdoor path by for example matching or regression. IV provides an alternative by introducing an additional variable (the instrumental variable/instrument), which affects the outcome only through the treatment variable.\nThe instrumental variable is exogenous, i.e. there is no other variable in the model influencing the value of the instrument (no arrow pointing to the instrument). Thus, it mimics an experiment by exploiting the exogenous variation in treatment due to the instrument. Endogenous variation from unobserved confounders can be disregarded as we are only considering the variation in treatment caused by the instrument.\nIV can be illustrated using DAGs. On the left, there is a potential initial situation you could find yourself in: you want to examine the effect of \\(D\\) on \\(Y\\), but unfortunately, there is an unobserved confounder (=omitted variable) that you would have to adjust for to identify the direct effect. As \\(U\\) is unobserved, there is no way to close the backdoor path by methods like matching, regression etc. This is where IV comes to rescue.\nAs you can see on the right, now \\(D\\) mediates between an instrument \\(Z\\) and outcome \\(Y\\). There is no direct path between \\(Z\\) and \\(Y\\) and therefore \\(Z\\) affects \\(Y\\) only through \\(D\\). For the instrument validity, there are a few assumptions that need to be fulfilled, which we discuss later in detail but summarizing, we need\n\nRelevance: \\(Z \\rightarrow D, \\,\\,\\, Cor(Z,D) \\neq 0\\) [testable]. The instrument \\(Z\\) needs to have an impact on the treatment \\(D\\).\nExcludability: \\(Z \\rightarrow D \\rightarrow Y,\\,\\, Z \\not\\to Y, \\,\\,\\, Cor(Z, Y|D) =0\\) [partly testable]. The instrument \\(Z\\) influences the outcome \\(Y\\) only through the treatment variable \\(D\\).\nExogeneity: \\(U \\not\\to Z, \\,\\,\\, Cor(Z, U)=0\\) [not testable]. The unobserved confounder \\(U\\) is uncorrelated with the instrument \\(Z\\).\n\n\n\n\n\nOmitted variable bias\n\n\n\nInstrumental variable estimation"
  },
  {
    "objectID": "content/toolbox/08_iv.html#exploration",
    "href": "content/toolbox/08_iv.html#exploration",
    "title": "Instrumental Variables",
    "section": "Exploration",
    "text": "Exploration\nFor the sake of explanation, we generate a synthetic data set with the variables \\(Z\\), \\(D\\) and \\(Y\\) as defined above. We also include the unobserved variable \\(U\\). This way, we can better explain where the bias comes from and how it affects the estimated treatment effect. The true treatment effect, which is the direct effect of \\(D\\) on \\(Y\\), is set to \\(1\\). We also add some random noise, so relationships are not perfect and what you will see is closer to reality.\n\n# Show data\ndf\n\n\n\n  \n\n\n\nFrom the table, you already have an idea what the data structure and types look like. But when developing our analysis strategy, we are mainly interested in relationships between the variables, so let’s have a look at the correlation matrix. It is a first step in assesing certain prerequisites for a valid IV strategy. Please also note, that we include \\(U\\) here, but in general, you do not observe \\(U\\), which was the reason why we need to use IV in the first place. It is just for the purpose of explanation that we include it here.\n\n# Correlation matrix\ncor(df) %&gt;% round(2)\n\n           distance program motivation   kpi\ndistance       1.00   -0.56       0.01 -0.47\nprogram       -0.56    1.00       0.39  0.92\nmotivation     0.01    0.39       1.00  0.55\nkpi           -0.47    0.92       0.55  1.00\n\n\nFrom the correlation matrix, we can take a few important insights. There is a significant negative correlation between distance and program participation, which is also called first-stage and confirms the relevance of our instrument. The distance to the next training location does affect decision to participate in a training. This is assumption (4) from above.\nHaving a synthetic data set, we can also see that our instrument is uncorrelated with the unobserved variable motivation, which means there is no confounding as stated in assumption (2). Usually, however, we would not be able to test this assumption due to an unobservable variable not being observed. Moreover, there could be additional confounders. With merely statistical concepts, we cannot prove that the effect of \\(Z\\) on \\(Y\\) goes only through \\(D\\). We have to argue why that is the case.\nCorresponding to our DAG, we also see that both program participation and KPI are correlated with the (unobserved) motivation, which thus opens another path between treatment and outcome. Being unable to close this path, we actually need an instrument in this situation.\nAdditionally to relying on a correlation matrix, it could be very useful to also plot the data to check relationships between variables."
  },
  {
    "objectID": "content/toolbox/08_iv.html#modeling",
    "href": "content/toolbox/08_iv.html#modeling",
    "title": "Instrumental Variables",
    "section": "Modeling",
    "text": "Modeling\nConfounding\nWhen you plausibly argue that your instrument is valid, you would usually perform 2SLS, short for Two Stage Least Squares. We’ll come to that shortly, but because we have all the data, we can also check what the bias would be in case we would not use an instrument and ignore the confounder.\nRemember, the true treatment effect is 1 in this case. Therefore, when we regress \\(Y\\) on \\(D\\) and \\(U\\), we should be able to recover this effect. And when you look at the regression output, in fact, we do. It is not exactly equal 1, but that is due to sampling noise.\n\n# First of all, let's look at the coefficients of the \"full\" (but unobservable)\n# model. It is unobservable, as it includes motivation, which in reality is \n# a variable that is very hard to collect or measure.\n# Coefficients are expected to be close to what he have defined in the data\n# generation section.\nmodel_full &lt;- lm(kpi ~ program + motivation, data = df)\nsummary(model_full)\n\n\nCall:\nlm(formula = kpi ~ program + motivation, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7095 -0.1358 -0.0016  0.1359  0.7078 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01391    0.01124   -1.24     0.22    \nprogram      1.00969    0.00563  179.32   &lt;2e-16 ***\nmotivation   1.02004    0.02156   47.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2 on 5997 degrees of freedom\nMultiple R-squared:  0.89,  Adjusted R-squared:  0.89 \nF-statistic: 2.43e+04 on 2 and 5997 DF,  p-value: &lt;2e-16\n\n\nBut what would happen if we ignored the confounder \\(U\\) (motivation) and only regress \\(Y\\) (KPI/efficiency) on \\(D\\) (program participation)? We do not close the backdoor path and consequently, the effect is overestimated.\nWe can see that the coefficient for program participation is higher than expected, i.e. it has an upward bias. This is because, it takes some of the variation that is actually attributable to motivation into the coefficient of program participation. More motivatd employees are more likely to participate in the program and have higher outcomes even without participation. This confirms the need to include an instrument to model causal effects when there is no way to include the confounder.\n\n# Modeling the data without the unobservable variable, i.e. only including \n# program participation in this case, returns a biased coefficient as the \n# relationship between program and the outcome is biased by a collider.\nmodel_biased &lt;- lm(kpi ~ program, data = df)\nsummary(model_biased)\n\n\nCall:\nlm(formula = kpi ~ program, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8203 -0.1604 -0.0018  0.1599  0.8175 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.48878    0.00429     114   &lt;2e-16 ***\nprogram      1.11462    0.00606     184   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.23 on 5998 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.38e+04 on 1 and 5998 DF,  p-value: &lt;2e-16\n\n\n2SLS\nTwo Stage Least Square is the estimation technique for IV and consists, as it names suggest, of two stages. In the first stage, the treatment variable is regressed on the instrument and in the second stage, the estimated values of the first stage are used as a regressors for the outcome. It sounds a little bit confusing, so let’s write it down. Remember, \\(D\\) is our treatment, \\(Z\\) the instrument and \\(Y\\) the outcome.\nFirst stage:\n\\[\nD_i = \\gamma_0 + \\gamma_1Z_i + \\nu_i\n\\]\nSecond stage:\n\\[\nY_i = \\beta_0 +\\beta_1\\widehat{D_i} +\\epsilon_i\n\\]\nLower cases indicate single observations, so \\(i\\) indicates for example the row in our data set. What is important to note, is that in the second stage, we do not use \\(D_i\\), but instead \\(\\widehat{D_i}\\). The hat indicates that these are fitted values from the first stage.\nWe can do 2SLS manually in R, but for reasons I will get to later, it is recommended to use libraries built to run 2SLS. However, for purpose of explanation, we’ll do it also manually here.\nFirst stage: As already discussed, regress treatment variable on instrument and obtain the fitted model. The model coefficient returned by the model summary should be significant, otherwise there is reason to doubt the relevance and validity of the instrument. You can also look at the F-statistic, which should be above 10.\n\n\n\n\n\n\nUse linear regression\n\n\n\nAlthough program is a binary variable, we use linear regression instead of logistic regression. Otherwise, 2SLS does not provide correct results. A linear regression predicting binary outcomes is called linear probability model (LPM).\n\n\nHere, the instrument is highly significant. The higher the distance, the lower the likelihood of participation.\n\n# First stage\nfirst_stage &lt;- lm(program ~ distance, data = df)\nsummary(first_stage)\n\n\nCall:\nlm(formula = program ~ distance, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1268 -0.3320 -0.0371  0.3479  1.0206 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4680     0.0191    76.8   &lt;2e-16 ***\ndistance     -1.6207     0.0307   -52.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.41 on 5998 degrees of freedom\nMultiple R-squared:  0.317, Adjusted R-squared:  0.316 \nF-statistic: 2.78e+03 on 1 and 5998 DF,  p-value: &lt;2e-16\n\n\nLet’s look at the fitted values from the first stage. The fitted values is what you get when you use the calculated coefficients and for each observation compute what the model predicts as an expected value. So, they are most likely a different from the actual values. How different they are depends on the goodness of fit of your model. This is why it is important that your instrument has a good explanatory value for the treatment variable.\nLet’s see how well we can explain the decision to participate using the distance to the training location. For employees that actually participated, predicted probabilities are on average higher than for those who have not participated. However, there is a large amount of overlap between both groups. By the way, because we used a linear probability model, not all probabilities are between 0 and 1.\n\n# Predicted 'probabilities' from first stage\npred_fs &lt;- predict(first_stage)\n\n# Create table with predictions and actual decisions\npred_vs_actl &lt;- tibble(\n  pred = pred_fs,\n  actl = df$program\n)\n\n# Plot predictions vs original\nggplot(pred_vs_actl, aes(x = pred, y = actl, color = as.factor(actl))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n    theme(legend.title = element_blank())\n\n\n\nPredicted ‘probabilities’ for treated and untreated units.\n\n\n\nNow we continue to use the fitted values from the first stage and plug it in the second stage to get the local average treatment effect. We see that the coefficient for the effect is close to one as constructed and we were able to eliminate the omitted variable bias.\n\n# Second stage\nsecond_stage &lt;- lm(df$kpi ~ first_stage$fitted.values)\nsummary(second_stage)\n\n\nCall:\nlm(formula = df$kpi ~ first_stage$fitted.values)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5789 -0.4053 -0.0111  0.4063  1.7100 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.5423     0.0141    38.5   &lt;2e-16 ***\nfirst_stage$fitted.values   1.0076     0.0245    41.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.53 on 5998 degrees of freedom\nMultiple R-squared:  0.22,  Adjusted R-squared:  0.22 \nF-statistic: 1.69e+03 on 1 and 5998 DF,  p-value: &lt;2e-16\n\n\nHowever, it is recommended to use functions, like e.g. iv_robust() from the estimatr package, as it yields correct standard errors. You see that the coefficient is the same but the standard errors slightly differ.\n\n# Using our instrument (distance to training location), we try to eliminate the\n# bias induced by the omitted variable. If all assumptions regarding the validity\n# of our instrument are met, the resulting coefficient should be\n# close to what we have defined above.\nlibrary(estimatr)\nmodel_iv &lt;- iv_robust(kpi ~ program | distance, data = df)\nsummary(model_iv)\n\n\nCall:\niv_robust(formula = kpi ~ program | distance, data = df)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper   DF\n(Intercept)    0.542    0.00639    84.9        0    0.530    0.555 5998\nprogram        1.008    0.01126    89.5        0    0.986    1.030 5998\n\nMultiple R-squared:  0.841 ,    Adjusted R-squared:  0.841 \nF-statistic: 8.01e+03 on 1 and 5998 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data",
    "section": "",
    "text": "Here you can download the data for your assignments:\n\ndownload all files\ndownload individual files"
  }
]