[
  {
    "objectID": "content/fundamentals/01_b_stats.html",
    "href": "content/fundamentals/01_b_stats.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Now we will talk about some statistical concepts, that are the foundation for modeling processes in both statistical and causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to show here.\n\nRandom Variable\nFor starters, let’s again define what a random variable is. Often represented by letters such as \\(X\\), a random variable has a set of values, also called sample space, of which any could be the outcome if we draw a random sample from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die) or continuous values (e.g. average height of individuals).\n\n\nExpected value\nA random variable is a real-valued function and can with more than one possible outcomes. This is why we cannot represent it as a scalar. The expected value of random variable, however, is a scalar and represents something like a “summary” of the random variable. Knowing the possible values (from the sample space) and the probability distribution, we can compute the expected value.\n\n\n\n\n\n\nTip\n\n\n\nThe summation operator \\(\\sum\\), denoted by the Greek capital Sigma is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, \\(x_1, x_2, …, x_n\\) to a shorter and more readable form\n\\[\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n\\]\nwith the arbitrary index of summation \\(i\\) being the lower limit and \\(n\\) the upper limit.\nBy basic math rules, the following simplifications are possible, where \\(c\\) is a constant:\n\\[\n\\sum_{i=1}^nc=nc\n\\]\nand\n\\[\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n\\]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe expected value, also called population mean, of a variable \\(X\\) is defined as the weighted average of possible values the random variable can take, where the weight is equal to the probability of the random variable taking on a specific value.\n\n\nIf we want to compute it for a discrete random variable, we make use of the summation operator. Considering a finite list of potential values \\(x_1, x_2, …, x_k\\) with probabilities \\(p_1, p_2, …, p_k\\), the expectation of \\(X\\) can be computed by\n\\[\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n\\]\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\\[\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n\\]\nProbabilities could also be different, as long as they sum to 1. For continuous random variables, we need a function returning the probabilities for each value. We leave that out here as we are only interested in understanding the intuition behind the expected value. However, it is very similar.\nLet’s type in the probabilities and outcomes of rolling a die and see what the expected value is.\n\n\n\n\n\n\nNote\n\n\n\nTo replicate values and create a vector rep() can be used. It takes the value and number of times it should be replicated.\n\n\n\n# Vector of probabilities (all equal)\np <- rep(1/6, 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n\n[1] 3.5\n\n\nAs you might have expected, it’s 3.5.\n\n\n\n\n\n\nTip\n\n\n\nAdditional rules regarding the calculation of expected values that can be useful are:\n\\[\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n\\]\n\n\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.\n\n\nConditional Expected Value\nThe conditional expected value is the expected value conditioned on some other value. Given the value \\(x\\) of \\(X\\), the expected value for \\(Y\\) obtains as\n\\[\nE[Y|X = x]\n\\]\nand is a function of \\(x\\). In other words, the conditional expected value is the best guess for \\(Y\\) knowing only that \\(X=x\\).\nAs a simple example, consider we take a representative random sample from the world population and want to compute the expected value for \\(height\\). Denoting height with \\(Y\\), the expected value for the whole population is the expected value \\(E[Y]\\) over all individuals in your sample.\nThe conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.\n\\[\nE[Y] \\neq E[Y|age < 10] \\neq E[Y|age >= 10]\n\\]\n\n\nVariance\nBefore we define variance, let’s see why it is important to know. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.\n\n\n\n\n\n\nNote\n\n\n\nseq(from, to, by) or seq(from, to, length.out) returns a vectors with a sequence as specified by the arguments.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrnorm(n, mean, sd) samples values from the normal distribution. n specifies the number of values, mean and sd define the parameters of the normal distribution.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmap() is a very useful function when you want to apply a function to each element of a list or a vector.\n\n\n\n\n\n\n\n\nNote\n\n\n\npivot_longer() changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need pivot_wider().\n\n\n\n\nCode\n# Load tidyverse package\nlibrary(tidyverse)\n\n# 100 step-wise values from 1 to 5\nX <- seq(1, 5, length.out = 100)\n\n# For each value x_i of X sample from normal distribution with mean equal to x_i\n# Note the different values for standard deviation\nY_lv <- map(X, function(i) rnorm(1, i, 0.25)) %>% unlist()\nY_hv <- map(X, function(i) rnorm(1, i, 1.5)) %>% unlist()\n\n# Create tibble with X and YY values for low and high variance\ndf <- tibble(X = X, \n             Y_low_variance = Y_lv,\n             Y_high_variance = Y_hv) %>%\n  pivot_longer(cols = c(Y_low_variance, Y_high_variance))\n\n# Plot both X~Y relations as scatter plot\nggplot(df, aes(X, value)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  facet_wrap(~name)\n\n\n\n\n\n\n\n\n\nMathematically, the variance is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The sample variance indicates how far a set of observed values spread out from their average value and is an estimate of the full population variance, that in most cases cannot be directly observed due to lack of data of the whole population.\nMathematically, the population variance is defined as\n\\[\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n\\]\nand the sample variance results as\n\\[\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n\\]\nYou might have noticed the term \\((n-1)^{-1}\\) is different from what you probably expected (\\(n^{-1}\\)). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.\n\n# High variance\nvar(Y_hv)\n\n[1] 3.8\n\n\n\n# Low variance\nvar(Y_lv)\n\n[1] 1.5\n\n\nThe standard deviation obtains as the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\n\n# High standard deviation\nsd(Y_hv)\n\n[1] 2\n\n\n\n# Low standard deviation\n# High standard deviation\nsd(Y_lv)\n\n[1] 1.2\n\n\n\n\n\n\n\n\nTip\n\n\n\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of \\(a\\), then the variance will increase by \\(a^2\\).\n\\[\nVar(aX+b)=a^2V(X)\n\\]\nYou can also conveniently compute a variance for the sum of two random variables\n\\[\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n\\]\nwhich in case of independence reduces to the sum of the individual variances due to the fact that \\(E(XY) = E(X)E(Y)\\).\n\n\n\n\nCovariance\nCovariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.\n\n\n\n\n\n\nNote\n\n\n\nas_tibble() or as.data.frame() can be used to change e.g. matrices or lists into tables.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrbind() or bind_rows() are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are cbind() and bind_cols().\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo rename columns, you can use rename(new_name = old_name).\n\n\nIf you’re interested in how to create correlated variable, you can open the following code chunk.\n\n\nCode\n# Create the variance covariance matrix\nsigma <- rbind(c(1, 0.8, 0), c(0.8, 1, -0.5), c(0, -0.5, 1))\n# Create the mean vector\nmu <- c(0, 0, 0) \n\n# Generate the multivariate normal distribution\ndf <- as_tibble(MASS::mvrnorm(n = 1e+3, mu = mu, Sigma = sigma)) %>% \n  rename(X1 = V1, X2 = V2, X3 = V3)\n\n\nWe defined the following covariance matrix:\n\n# Show variance-covariance matrix\nsigma\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.8  0.0\n[2,]  0.8  1.0 -0.5\n[3,]  0.0 -0.5  1.0\n\n\nFrom left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.\n\n\nCode\nggplot(df, aes(x = X1, y = X2)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"Positive correlation\")\nggplot(df, aes(x = X2, y = X3)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"Negative correlation\")\nggplot(df, aes(x = X1, y = X3)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"No correlation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the equality \\(E(XY) = E(X)E(Y)\\) holds, then it implies a covariance of 0 between the variables \\(X\\) and \\(Y\\). Covariance is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.\nAs a matter of form, the formula for the covariance of the random variables \\(X\\) and \\(Y\\) is\n\\[\nCov(X,Y) = E(XY) - E(X)E(Y)\n\\]\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the correlation which can be derived from the covariance if the individual variances are known.\n\\[\n\\text{Corr}(X,Y) = \\dfrac{C(X,Y)}{\\sqrt{V(X)V(Y)}}\n\\]\nThe correlation is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.\n\n\n\n\n\n\nNote\n\n\n\nTo compute correlation, variance and covariance, respectively, you can use cor(), var() and cov().\n\n\n\n\nConclusion\nMany of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.\n\n\nAssignments\nFrom the data folder, load the table random_vars.rds. It contains draws from two random variables, \\(age\\) and \\(income\\).\n\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nexpected value\nvariance\nstandard deviation\n\nExplain, if it makes sense to compare the standard deviations.\nThen, examine the relationship between both variables and compute:\n\ncovariance\ncorrelation\n\nWhat measure is easier to interpret? Please discuss your interpretation.\nCompute the conditional expected value for:\n\n\\(E[income|age <= 18]\\)\n\\(E[income|age \\in [18, 65)]\\)\n\\(E[income|age >= 65]\\)"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html",
    "href": "content/fundamentals/01_a_prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Before we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part.\n\n\n\n\n\n\nAssignments\n\n\n\nPlease note: in this chapter, there are two assignments in between. In the other chapters, you’ll usually find the assignments at the end."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "href": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "title": "Probability Theory",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nConsider the most simple example: flipping coins. We define the flip of a coin as a random variable as we don’t know the outcome. To express our uncertainty, we make us of probability theory.\nFlipping the coin, we will see how the coin has landed and our random variable can take on of the two possible events \\(\\{H, T\\} \\subseteq \\Omega\\). It will be either Head or Tail.\nSo we have already defined two terms: random variable and events. Now what is a probability? A probability is always linked to an event typically denoted by a capital letter, here either \\(H\\) and \\(T\\), and expresses how likely this event is to happen. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\\[\nP(H) = P(T) = 0.5\n\\]\nExtreme cases: If an event \\(A\\) is impossible, its probability is \\(P(A) = 0\\) and if it is certain to occur, it is \\(P(A)=1)\\).\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 1: Probability is a real number greater or equal to 0.\n\n\nWe can also introduce the compliment \\(\\overline{A}\\), which is what happens when \\(A\\) does not happen and consequently, \\(P(A) + P(\\overline{A}) = 1\\). \\(A\\) and \\(\\overline{A}\\) are mutually exclusive, by definition. But there could also be two events \\(A\\) and \\(B\\) that are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\\[\nP(H \\cup T) = P(H) + P(T) = 1\n\\]\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 2: Total probability is equal to 1.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 3: Probability of mutually exclusive events is the sum of the probabilities.\n\n\nTo understand what not mutually exclusive events are, consider events \\(studying\\) and \\(working\\). For a random person, we don’t know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\nThen, the probability of both events happening is calculated by\n\\[\nP(A \\cup B) = P(A) + P(B) + P(A \\cap B)\n\\]\nwith \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the addition rule.\n\n\n\n\n\nFor mutually exclusive events:\n\\[\nP(A \\cup B) = P(A) + P(B) + P(A \\cap B) = P(A) + P(B)\n\\]\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule,\n\\[\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n\\]\nwhere \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened. It is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\nUsing the example with workers and students: without knowing exact numbers, we can almost safely assume that students are less likely to work than individuals who are not studying.\n\\[\nP(working|studying) > P(working|\\overline{studying})\n\\]\nEssentially, we are looking at probabilities restricted to a subset of the sample.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways:\n\\[\nP(A \\mid B) = P(A)\n\\]\nA typical example of independence is to roll a die twice. The second roll does not depend on the first one and each outcome is as likely independent of the first roll. This also results in\n\\[\nP(A \\cap B) = P(A) \\ P(B)\n\\]"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#probability-tree",
    "href": "content/fundamentals/01_a_prob.html#probability-tree",
    "title": "Probability Theory",
    "section": "Probability Tree",
    "text": "Probability Tree\nAn intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events has to happen. The probability of two consecutive events is obtained by multiplying the probabilities.\nConsider the following example: you are project manager and based on your are interested in th probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Based on your historical data about past projects, you come up with the following tree.\n\n\n\nProbability tree"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#assignment-1",
    "href": "content/fundamentals/01_a_prob.html#assignment-1",
    "title": "Probability Theory",
    "section": "Assignment 1",
    "text": "Assignment 1\nDefine being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\). Then, compute the following probabilities:\n\n\\(P(T \\cap S)\\)\n\\(P(T \\cap \\overline{S})\\)\n\\(P(\\overline{T} \\cap S)\\)\n\\(P(\\overline{T} \\cap \\overline{S})\\)\n\nWhat is the sum of all four probabilities?\nHint: Check here, if you are not sure what is shown in the probability tree."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#set-theory",
    "href": "content/fundamentals/01_a_prob.html#set-theory",
    "title": "Probability Theory",
    "section": "Set Theory",
    "text": "Set Theory\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams building on set theory. A very simple one we actually already used above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\nLet’s use an example to understand the other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices, smartphones, tables and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current distribution.\nWe simulate the collection process here. If you are interested how to do it in R, check out the code. But you don’t have to. And don’t worry, if it looks too complicated at this point, just move on.\n\n\n\n\n\n\nNote\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R.\n\n\n\n\n\n\n\n\nNote\n\n\n\ntibble() is the most convenient way to create tables. You specify column name and content and assign your tibble to an object to store it.\n\n\n\n\n\n\n\n\nNote\n\n\n\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmutate() is one of the most important functions for data manipulation in tables. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %>% mutate(new_variable = existing_var / 100).\n\n\n\n\nCode\n# Load tidyverse package\nlibrary(tidyverse)\n\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\n# Nmber of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  rowwise() %>% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer.\n\n\n\n\n\n\nNote\n\n\n\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\n\n\n# Show first ten lines\nhead(app_usage, 10)\n\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n\n\n\n\n\nNote\n\n\n\nSumming all values by column is done by colSums(table). For rows, you would use rowSums(table).\n\n\n\n# Show column sums\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n\n\nSumming the \\(user\\_id\\) does not make any sense. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n\n\n\n\n\nNote\n\n\n\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\n\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n\nsmartphone     tablet   computer \n       589        389        226 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n\n\nGeneric Venn diagram with three sets\n\n\n\n\n\n\n\n\nNote\n\n\n\nwhich() checks a condition and returns the indices.\n\n\n\n# Set of phone, tablet and computer users\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all <- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n\nUsing the Venn diagram, we are able to answer questions like the following:\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\nTry to answer these questions yourself.\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\n0.5%\n7.3% + 3.3% + 8.8% + 0.5%\n10% + 42.3% + 27.8%\n\n\n\n\nWe can also use the example to go through the basic probability rules defined above.\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - P(T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "href": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "title": "Probability Theory",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\n\nMath\nA very important theorem in probability theory is Bayes theorem that we get by reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nUsing the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem expresses a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before.\n\n\n\n\n\n\nTip\n\n\n\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]\n\n\n\n\nApplication\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) and consequently \\(P(\\overline{B}|A) = 0.03\\)\n\\(P(B|\\overline{A}) = 0.01\\) and consequently \\(P(\\overline{B}|\\overline{A}) = 0.99\\)\n\\(P(A) = 0.04\\) and consequently \\(P(\\overline{A}) = 0.96\\)\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B)\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\n\n\nAssignment 2\nCompute\n\n\\(P(\\overline{A}|B)\\) (1)\n\n\n\n\\(P(A|B)\\) (2)\n\nand fill the gaps in the following sentence:\nThese results show that in case the alarm is triggered, there is still a possibility of about __% that the product is flawless and in only __% of cases the product is faulty."
  },
  {
    "objectID": "content/fundamentals/02_reg.html",
    "href": "content/fundamentals/02_reg.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Statistical inference aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores and a lot of other business-related units.\nIn practice, it is often impossible to collect data about the whole population, which is why we draw (ideally random) samples from the whole population and use statistical inference to draw conclusions about the whole population using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#r",
    "href": "content/fundamentals/02_reg.html#r",
    "title": "Regression and Statistical Inference",
    "section": "R",
    "text": "R\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in R. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\nBefore we start, we load the R package tidyverse, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\n# Load tidyerse package\nlibrary(tidyverse)\n\nWe create a tibble, which is a table containing data, with two columns, \\(x\\) and \\(y\\). For \\(x\\), we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable \\(y\\), we make dependent from \\(x\\), i.e. for each unit \\(i\\), we create a \\(y_i\\) as\n\\[\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n\\]\nwhere \\(\\epsilon_i\\) is random noise.\nLet’s have a look at how the data is presented. We have a table containing 100 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n# Simluate data\n\n# number of observations\nn <- 10\n\n# Create tibble\nlm_dat <- tibble(\n  # draw from normal distribution\n  x = rnorm(n, mean = 3, sd = 1),\n  # y depends on x and noise from normal distribution\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\n# Show data\nlm_dat\n\n\n\n  \n\n\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between \\(X\\) and \\(Y\\). However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\nLet’s have a look at some random lines.\n\n\n\n\n\n\n\n\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the residuals are zero.\n\n\n\n\n\n\n\n\n\nThe resulting line is highlighted in blue.\nIf we want to mathematically compute the line in R, we have to use the lm() function and provide data and the assumed functional relationship as arguments. lm() is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: lm_mod), which we can interpret best when using summary() to show the resulting coefficients and other statistical information.\n\n\n\n\n\n\nNote\n\n\n\nlm() is a function that fits a linear model. You have to provide data and a regression equation in the form of for example outcome ~ regressor_1 + regressor_2 or outcome ~ ., if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call summary().\n\n\nLooking at the regression summary, we see that the line is modeled by \\(y = -0.1918 + 0.3354*x\\). It means that for the fitted model, an increase of one unit in \\(x\\) is related to an 0.3354 increase in \\(y\\). That is relatively close to what we simulated (0.3) and deviates due to the added random noise.\n\n# Fit model and print summary\nlm_mod <- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = y ~ x, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18597 -0.05210 -0.00783  0.06584  0.24115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1918     0.1300   -1.48     0.18    \nx             0.3354     0.0443    7.57  6.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 8 degrees of freedom\nMultiple R-squared:  0.878, Adjusted R-squared:  0.862 \nF-statistic: 57.3 on 1 and 8 DF,  p-value: 6.48e-05\n\n\nNow, let’s check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations \\(y_i\\) and predictions \\(\\hat{y_i}\\) for each \\(i\\) or plotting the residuals \\(r_i = y_i - \\hat{y_i}\\) and comparing it to the \\(x\\)-axis.\n\n# Add fitted values and residuals to data\nlm_dat_fit <- lm_dat %>% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")"
  },
  {
    "objectID": "content/fundamentals/02_reg.html#math",
    "href": "content/fundamentals/02_reg.html#math",
    "title": "Regression and Statistical Inference",
    "section": "Math",
    "text": "Math\nMathematically, the best line is found by the ordinary least squares (OLS) method.\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\nGiven \\(n\\) samples of observed pairs of dependent and independent variables \\(\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}\\), we plug any of them into the equation\n\\[\ny_i=\\beta_0+\\beta_1x_i+u_i\n\\]\nand together with our assumptions \\(E(u) = 0\\) and \\(E(u|x)=0\\) we obtain the equations to be solved to retrieve estimates for \\(\\beta_0\\) and \\(\\beta_1\\).\nFrom the independence of \\(x\\) and \\(u\\) and our understanding of probabilities and expectations, we also know that the expected value of the product of \\(x\\) and \\(u\\) has to be zero: \\(E(xu)=0\\). Substituting \\(u\\) with \\(y-\\beta_0-\\beta_1\\), we obtain the two conditions that when being solved give us the optimal estimates for our \\(\\beta\\) parameters.\n\\[\n\\begin{align}\nE(y-\\beta_0-\\beta_1x) = E\\Big(x[y-\\beta_0-\\beta_1x]\\Big) = 0\n\\end{align}\n\\]\nTranslated into its sample counterpart:\n\\[\n\\begin{align}\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\end{align}\n\\]\nLooking at the sample equations, we know our sample size \\(n\\), our sampled values \\(y_i\\) and \\(x_i\\). The coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\nSkipping a few transformation steps, we obtain\n\\[\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n\\]\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our \\(\\beta_1\\) is defined as the covariance of \\(X\\) and \\(Y\\) divided by the variance of \\(X\\). It also shows that the variance of \\(X\\) has to be greater than zero, which means that not all values of \\(x_i\\) can be the same. You need to observe different values to be able to estimate how \\(y_i\\) reacts to \\(x_i\\).\n\\(\\beta_0\\) follows directly by plugging \\(\\beta_1\\) into \\(\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta}1\\overline{x}\\). A bar above a variable always represents the sample value of that particular variable. Thus,\\(\\beta_0\\) is (as expected) constant and linear in \\(\\beta_1\\).\nKnowing the equation for the regression line, we can compute fitted values \\(y_i\\) for all \\(i\\)\n\\[\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n\\]\nIn almost all cases however, \\(\\widehat{y}_i\\) won’t be equal to \\(y_i\\) but there will be a prediction error, commonly referred to as residual \\(\\widehat{u}_i\\). Make sure that you don’t mix it up with \\(u\\), the error term, which is always unobserved.\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\n\n\n\n\n\n\nInfo\n\n\n\nSquares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n\n\n\\[\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n\\]\nAgain, most of the residuals won’t be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#interpretation",
    "href": "content/fundamentals/02_reg.html#interpretation",
    "title": "Regression and Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\nSo let’s run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. We will see in later chapters why that might be dangerous.\n\n# Include all potential regressors\nlm_all <- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n\n\nCall:\nlm(formula = expected_cost ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-406.5 -123.9  -26.4  102.7  942.0 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 450.51998   28.77380   15.66   <2e-16 ***\nregionE       8.57388   16.25854    0.53   0.5981    \nregionS      -1.99015   16.66160   -0.12   0.9049    \nregionW       1.65634   16.93503    0.10   0.9221    \nsex          -9.38273   11.77360   -0.80   0.4257    \nsmoking     205.70391   13.21526   15.57   <2e-16 ***\nage           9.29488    0.51525   18.04   <2e-16 ***\nincome       -0.05113    0.00384  -13.30   <2e-16 ***\nbmi          -2.07654    0.76406   -2.72   0.0067 ** \nchildren     -9.25879   14.55410   -0.64   0.5248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185 on 990 degrees of freedom\nMultiple R-squared:  0.404, Adjusted R-squared:  0.398 \nF-statistic: 74.4 on 9 and 990 DF,  p-value: <2e-16\n\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\nThe estimate is what we know as regression coefficient from before, typically denoted by \\(\\widehat{\\beta}_i\\) or other Greek letters. As it is the estimated version, it has a hat. The estimated for \\(\\beta_i\\) It tells you by how much the dependent variable \\(\\widehat{y}\\) varies when a particular variable is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect of \\(x_i\\) ceteris paribus, Latin for “the others equal”. It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as controlling for the other variables."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#statistical-significance",
    "href": "content/fundamentals/02_reg.html#statistical-significance",
    "title": "Regression and Statistical Inference",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate.\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error.\nMost widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data. The null hypothesis states that there is no correlation between the dependent variable \\(y\\) and the independent variable \\(x_1\\). The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis. Here, the alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable.\nStatistical significance can directly be derived from the p-value and an arbitrary significance level \\(\\alpha\\). However, the most widely used level of \\(\\alpha\\) is \\(0.05\\). Less often used are levels of \\(.1\\), \\(.01\\) or \\(.001\\).\nAn estimate with a p-value less than \\(\\alpha\\) is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level \\(\\alpha\\). Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that \\(age\\), \\(income\\), \\(bmi\\), and \\(smoking\\) are statistically significant (at different levels though, indicated by the number of stars).\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A \\((1-\\alpha)\\) confidence interval has a probability of \\((1-\\alpha)*100 \\%\\) to contain the true value of our estimated coefficient. That means, if we would sample \\(100\\) times, \\(\\beta_i\\) would be contained in the sample \\((1-\\alpha)*100\\) times.\n\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n\n              2.5 %  97.5 %\n(Intercept) 394.055 506.985\nregionE     -23.331  40.479\nregionS     -34.686  30.706\nregionW     -31.576  34.889\nsex         -32.487  13.721\nsmoking     179.771 231.637\nage           8.284  10.306\nincome       -0.059  -0.044\nbmi          -3.576  -0.577\nchildren    -37.819  19.302\n\n\n\n# alpha = 0.9\nconfint(lm_all, level = 0.90)\n\n                5 %    95 %\n(Intercept) 403.147 497.893\nregionE     -18.194  35.342\nregionS     -29.422  25.441\nregionW     -26.225  29.538\nsex         -28.767  10.001\nsmoking     183.946 227.461\nage           8.447  10.143\nincome       -0.057  -0.045\nbmi          -3.334  -0.819\nchildren    -33.221  14.703\n\n\nAn estimate whose interval is either completely positive or completely negative is different from zero and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the positive coefficient."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#model-selection",
    "href": "content/fundamentals/02_reg.html#model-selection",
    "title": "Regression and Statistical Inference",
    "section": "Model selection",
    "text": "Model selection\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\nLooking at the spread of residuals (\\(\\widehat{u}_i = y_i - \\widehat{y}_i\\)) we want them to be spread evenly around zero.\n\n# Plot histogram of residuals\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n\n\n\n\n\n\n\n\nWe can see that the residuals are in fact almost normally distributed.\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit is \\(R^2\\). It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\\[\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n\\]\nConveniently, \\(R^2\\) is always between 0 and 1 and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high \\(R^2\\) can even point to a biased model while a model with a low \\(R^2\\) can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of \\(R^2\\) does also not change the interpretation of the regression coefficients.\nA problem with \\(R^2\\) is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the \\(\\text{Adjusted} \\, R^2\\). It includes a term for the number of independent variables used.\n\\[\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n\\]\nwhere \\(n\\) is the sample size and \\(p\\) the number of independent variables. This way, you can compare models and account for their scarcity.\nLet’s build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n# Include only significant regressors\nlm_imp <- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + smoking, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-481.5 -138.9  -27.2  112.5  931.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  392.959     27.560   14.26   <2e-16 ***\nage            7.198      0.478   15.05   <2e-16 ***\nbmi           -2.071      0.824   -2.51    0.012 *  \nsmoking      198.401     14.283   13.89   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201 on 996 degrees of freedom\nMultiple R-squared:  0.296, Adjusted R-squared:  0.294 \nF-statistic:  140 on 3 and 996 DF,  p-value: <2e-16\n\n\nExcept for \\(bmi\\), coefficients are very similar. We’ll look into that in just a second. But first let us compare both models with regard to \\(\\text{(Adjusted)} \\, R^2\\).\nTo elegantly print variables in a specified format, you can use sprintf().\n\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.40\"\n\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.29\"\n\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike’s Information Criteria (AIC) and Bayesian Information Criteria (BIC). Both AIC and BIC penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\n\n[1] \"AIC: 13294.02\"\n\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n\n[1] \"AIC: 13447.79\"\n\n\nFor both criteria, the model with the lowest value is preferred.\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\nFor example, is it correct to assume a linear relationship between \\(bmi\\) and the outcome \\(expected\\_cost\\)? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term “linear”, so what can we do about it?\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. \\(bmi^2\\).\n\n# Include quadratic term for BMI\nlm_sq <- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + I(bmi^2) + smoking, \n    data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-389.6  -80.3   -2.6   90.2  350.2 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1964.4792    41.7901    47.0   <2e-16 ***\nage            7.8004     0.2919    26.7   <2e-16 ***\nbmi         -122.6122     2.9778   -41.2   <2e-16 ***\nI(bmi^2)       2.0803     0.0507    41.1   <2e-16 ***\nsmoking      184.4309     8.7113    21.2   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122 on 995 degrees of freedom\nMultiple R-squared:  0.739, Adjusted R-squared:  0.738 \nF-statistic:  704 on 4 and 995 DF,  p-value: <2e-16\n\n\nFrom the summary, we can see that including the square term significantly improves the model fit.Check for yourself the metrics \\(R^2\\) and \\(AIC\\) and plot the histogram of residuals.\nWe can take from it, that it is extremely important to rely on theoretical considerations when building models."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#assumptions",
    "href": "content/fundamentals/02_reg.html#assumptions",
    "title": "Regression and Statistical Inference",
    "section": "Assumptions",
    "text": "Assumptions\n\nLinearity: Relationship between \\(X\\) and \\(Y\\) is linear.\nHomoscedasticity: Variance of residual is the same for any value of \\(X\\).\nIndependence: Observations are independent of each other. Residuals are independent of each other.\nNormality: For any fixed value of \\(X\\), \\(Y\\) is normally distributed. Residuals of the model are normally distributed."
  },
  {
    "objectID": "content/fundamentals/04_dag.html",
    "href": "content/fundamentals/04_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "We have already learned that correlation and causation can easily be confused. Now, we will define concepts and acquire tools that help us in developing credible identification strategies to separate correlation from causation. One essential part is graphically modeling your theoretical knowledge about the data-generating process.\nIn causal inference, directed acylic graphs (DAGs) do the graphic modeling part. They are the foundation of any analysis strategy and moreover communicate your research plan.\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nA DAG should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?\n\nBesides being helpful in guiding your analysis and identification strategy, DAGs also show your research design to your audience.\nA simple example of a DAG could be the effect of having an university degree on future salary: at first, it might be intuitive to say that future salary increases when you get an university degree.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# without confounder\nschooling_1 <- dagify(\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3),\n                y = c(uni_degree = 1, salary = 1))\n)\n\n# Plot DAG\nggdag(schooling_1, use_labels = \"name\", text = F) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n# with confounder\nschooling_2 <- dagify(\n  uni_degree ~ ability,\n  salary ~ ability,\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3, ability = 2),\n                y = c(uni_degree = 1, salary = 1, ability = 2))\n)\n\n# Plot DAG\nggdag(schooling_2, use_labels = \"name\", text = F) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut zooming out and thinking about why a university degree is correlated with higher salaries could lead you to the idea that both university degree and salary are influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree.\nIt is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#chain",
    "href": "content/fundamentals/04_dag.html#chain",
    "title": "Directed Acyclic Graphs",
    "section": "Chain",
    "text": "Chain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\n\nCode\n# Chain\nchain <- dagify(\n  Y ~ Z,\n  Z ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 0, X = 0))\n)\n\n# Plot DAG\nggdag(chain) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\nAn example of such a causal mechanism (page 37, Pearl) could be the effect of work hours on training and training on race time. In the DAG, the variables would be:\n\n\\(X\\): work hours\n\\(Z\\): training\n\\(Y\\): race time\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nIn terms of dependencies,\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow.\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on the training amount, that means we hold training amount fixed at a particular level, then there is no effect from work hours to race time as there is no direct effect, but only an effect through the amount of training. In other words, for individuals that differ in the hours they work but still have the same amount of training, there is no association between working hours and race time.\n\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#fork",
    "href": "content/fundamentals/04_dag.html#fork",
    "title": "Directed Acyclic Graphs",
    "section": "Fork",
    "text": "Fork\nAnother mechanism is the fork, also called common cause.\n\n\nCode\n# Fork\nfork <- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0))\n)\n\n# Plot DAG\nggdag(fork) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nAgain, let’s check in term of dependencies:\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\n\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collision",
    "href": "content/fundamentals/04_dag.html#collision",
    "title": "Directed Acyclic Graphs",
    "section": "Collision",
    "text": "Collision\nThe last mechanism is the collision, which is also called common effect.\n\n\nCode\n# Collider\ncollider <- dagify(\n  Z ~ X,\n  Z ~ Y,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1))\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\\(X\\): attractiveness\n\\(Y\\): talent\n\\(Z\\): celebrity\n\nFirst of all, in the general population, there is no correlation between attractiveness and talent (3rd dependency). Second, being either attractive or having a talent will help you to become a celebrity (1st and 2nd dependency).\nBut what about the last dependency? Why are attractiveness and talent suddenly correlated when conditioned on e.g. being a celebrity? That is because when you know someone is a celebrity and has no talent, the likelihood that he/she is attractive increases because otherwise he/she would likely not be a celebrity. Vice versa, if you know someone is a celebrity and is not attractive, he/she is probably talented in some form.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\))."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#confounding",
    "href": "content/fundamentals/04_dag.html#confounding",
    "title": "Directed Acyclic Graphs",
    "section": "Confounding",
    "text": "Confounding\nA very common problem when trying to prove causal effects is confounding.\n\n\nCode\n# Confounder\nconfounding <- dagify(\n  X ~ Z,\n  Y ~ Z,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0)),\n  labels = list(X = \"university degree\",\n                Y = \"salary\",\n                Z = \"ability\")\n)\n\n# Plot DAG\nggdag(confounding, use_labels = \"label\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\nIt is the same DAG as in the introduction. So just recall and imagine \\(X\\) was university degree, \\(Y\\) future salary and \\(Z\\) ability.\nThere are two paths from \\(X\\) to \\(Y\\):\n\ndirect path \\(X \\rightarrow Y\\)\nbackdoor path \\(X \\leftarrow Z \\rightarrow Y\\)\n\nFirst of all, it is important to clarify what effect we are actually interested in and that is the direct causal effect of university degree on future salary, \\(X\\) on \\(Y\\).\nThe indirect effect is not causal but is only spurious correlation induced by \\(Z\\) which we are generally not interested in. It is called backdoor path.\nSo how do we proceed to extract that effect? We need to somehow remove the association between \\(X\\) and \\(Y\\) that is only due to variation in \\(Z\\). And from the previous section we know how to do that. \\(X\\) and \\(Y\\) need to be independent conditional on \\(Z\\) and block the path from \\(X\\) to \\(Y\\) over \\(Z\\).\nBut what does blocking the path mean? It means that we have to condition on \\(Z\\), to keep it at a fixed level. Then, the variations in \\(X\\) that cause \\(Y\\) to vary are not due to \\(Z\\) because it does not vary at all and cannot have an impact on either \\(X\\) or \\(Y\\). Doing that we closed the backdoor and are able to retrieve the causal effect.\nNot blocking the path would falsify our results and is what is called the omitted variable bias. That is why \\(Z\\) is called confounder, because it confounds the ability to measure the causal effect.\nHowever, the main problem is that in many cases you might not be able to block the path for two different reasons:\n\nYou are aware of the confounder, but you did not collect data for it\nYou are not aware of the confounder (and probably did not collect data for it)\n\nThis stresses the importance of theoretical knowledge about the phenomenon you are researching. Without it, it is very unlikely that you can prove truly causal effects. The risk of not paying attention to confounders gets clear when we look at graphs visualizing an example of Simpson’s paradox: accounting for a third variable reverses the sign of correlation.\nWe can illustrate it with an imaginary example. Let’s assume you want to measure how a specific characteristic affects salary. So you start to collect data about both variables, throw them into a regression and your result tells you that there is a positive correlation. But what happens if you take the education level into account? You can see how the lines show a positive correlation on the left and a negative correlation on the right. When you include a third variable, the relationship reverses.\nIt is neither wrong or right to always include or exclude variables, but it depends on the application and question you want to answer. This is why causal reasoning is so important.\n\n\nCode\n# Simpson's paradox ----\n\n# Simulate data\n# number of observations\nn <- 1e+03\n\n# draw and create variables with specific dependencies\neducation <- rbinom(n, 2, 0.5)\ncharacter <- rnorm(n) + education\nsalary <- education * 2 + rnorm(n) - character * 0.3\n\n# rescale to realistic values\nsalary <- sample(10000:11000,1) + scales::rescale(salary, to = c(0, 100000))\ncharacter <- scales::rescale(character, to = c(0, 7))\neducation <- factor(education, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# create tibble\ndf <- tibble(\n  salary,\n  character,\n  education\n)\n\n# Not conditioning on education\nsimps_not_cond <- ggplot(df, aes(x = character, y = salary)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on education  \nsimps_cond <- ggplot(df, aes(x = character, y = salary, color = education)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsimps_not_cond\nsimps_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collider",
    "href": "content/fundamentals/04_dag.html#collider",
    "title": "Directed Acyclic Graphs",
    "section": "Collider",
    "text": "Collider\nYou probably noticed that the confounding example was related to the common cause in the previous section. The next example is related to the common effect mechanism.\n\n\nCode\n# Collider   \ncollider <- dagify(\n  Z ~ X,\n  Z ~ Y,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1)),\n  labels = list(X = \"GPA\",\n                Y = \"musical talent\", \n                Z = \"accepted\")\n)\n\n# Plot DAG\nggdag(collider, use_labels = \"label\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\nNow consider the following example: a research group wants to examine if there is a causal relationship between grade point averages (\\(X\\)) and musical talent (\\(Y\\)). They can use data from their own university where both is measured for every student as scoring high in one of these characteristics substantially increases your chance of being accepted.\nNow the researchers, who hypothesized a positive or no correlation between \\(X\\) and \\(Y\\), perform a simple analysis and, to their surprise, find out that there is a strong negative correlation. This strange result can be explained by collider bias. Implicitly, as they only used data from their own university students, they also conditioned on a collider \\(Z\\), which is “accepted to university” and is fixed on a constant level of actually being accepted to the university.\nAgain, we have two paths from \\(X\\) to \\(Y\\), but one thing is different: the backdoor path is already closed because \\(Z\\) is a collider, both arrows point toward it.\nTo correct their results they would need to have data from the whole population and not only students from a university that puts focus on either grade point averages or musical talent. Having a full population as a sample would probably lead to the result that there is no correlation between \\(X\\) and \\(Y\\).\nThis case (collider) is different to the case before (confounding), where conditioning was the correct solution. But if you have a collider in you DAG, make sure not to condition on it as it creates a dependence between \\(X\\) and \\(Y\\). You “open” a backdoor path that was closed before, just due to the presence of the collider.\nCollider bias often arises when your sample is not very representative of the population you are making claims about. How that could change your result can be seen in an example Berkson’s paradox. While there is no correlation for the whole population, for smaller subgroups there are. And therefore, it is crucial that you clearly state what effect you are interested in.\n\n# Berkson's paradox ----\n\n# Simulate data\n# number of observations\nn <- 1e+03\n# draw and create data with specific dependencies\nability     <- rnorm(n)\nmotivation  <- rnorm(n)\naptitude    <- 1/2 * ability + 1/2 * motivation + rnorm(n, 0, .1)\n\n# create tibble\ndf <- tibble(\n  ability    = ability,\n  motivation = motivation,\n  aptitude   = aptitude,\n  student    = ifelse(aptitude > 0, \"student\", \"no_student\")\n)\n\n# Not conditioning on student\nberk_not_cond <- ggplot(df, aes(x = motivation, y = ability)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on student  \nberk_cond <- ggplot(df, aes(x = motivation, y = ability,\n                            color = student, \n                            alpha = student)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_color_manual(values = c(\"student\" = ggthemr::swatch()[4],\n                                \"no_student\" = ggthemr::swatch()[5])) +\n  scale_alpha_manual(values = c(\"student\" = 1, \"no_student\" = 0.2)) +\n  theme(legend.position = \"right\")\n\n# Plot both\nberk_not_cond\nberk_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#d-separation",
    "href": "content/fundamentals/04_dag.html#d-separation",
    "title": "Directed Acyclic Graphs",
    "section": "d-separation",
    "text": "d-separation\nOne concept, that we have not named yet but implicitly used, is d-separation. If an effect of \\(X\\) on \\(Y\\) is d-separated, there is no statistical association that can flow between \\(X\\) and \\(Y\\) except for the direct effect. In fact, d-separation determines conditional independence.\nD-separation formalizes what we have already learned when going through the tree types of association.\nPractically (http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html) , it is easier to define the opposite, d-connection:\n\nRule 1: unconditional separation: \\(X\\) and \\(Y\\) are d-connected if there is an unblocked path between them. (As an example, imagine a confounder, that is not conditioned on.)\nRule 2: blocking by conditioning: \\(X\\) and \\(Y\\) are d-connected, conditioned on a set of \\(Z\\) nodes, if there is a collider-free path between \\(X\\) and \\(Y\\) that traverses no member. (Think of a mediated effect that takes away parts from the direct effect.)\nRule 3: conditioning on colliders: If a collider is a member of conditioning set \\(Z\\), or has a descendant in \\(Z\\), then it no longer blocks any path that traces this collider. (Image the collider example in the previous section.)\n\n!!! MAYBE CHANGE TO https://de.wikipedia.org/wiki/D-Separation\nKnowing these rules and having mapped our assumptions into the DAG allows us to treat observational data like experimental data and simulate interventions as we would have conducted an experiment. However, it is not a silver bullet as in many cases data availability will stop you from isolating the causal effect. For example, if you do not observer a confounder, you cannot control for it. All empirical work requires theory and with observational data we need to be extra careful to make sure to actually extract the effects we are interested in."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "href": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "title": "Directed Acyclic Graphs",
    "section": "Backdoor/Frontdoor Criterion",
    "text": "Backdoor/Frontdoor Criterion\nSpecial cases that are derived from d-separation rules are the backdoor and frontdoor criterion/adjustment.\nTo satisfy the backdoor criterion, we have to make sure all backdoors are closed, which, as already mentioned, differs for confounders and colliders.\n\n\n\n\n\nAgain, we want to block all other paths between \\(X\\) (treatment) and \\(Y\\) (outcome). So depending on the structure of the DAG, the following can block a path:\n\na chain or a fork whose middle node is in \\(Z\\)\na collider that is not conditioned on, which means it is not in \\(Z\\)\n\nThe frontdoor criterion, which is actually a consecutive application of the backdoor criterion, is a bit more complicated and we will leave it out for now, but in a the section about instrumental variables we will deal with it extensively."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "href": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "title": "Directed Acyclic Graphs",
    "section": "Algorithms to identify causally valid estimates",
    "text": "Algorithms to identify causally valid estimates\nIf DAGs become more complex because there are a lot of variables that are somehow related, we can make use of algorithms to check all the rules for us.\nOne application to help in such cases is http://dagitty.net/dags.html, where you can draw your DAG, define what is the treatment and outcome, which variables are observed and unobserved and many other things. Then it will show you what kind of adjustment is necessary to estimate the causal effect of interest.\n\n\n\n\n\nTry to build this graph on dagitty.net and look what useful information you can get from the site.\ndagitty is also implemented in R and combined with ggdag you are also able to plot your DAGs in a easy manner and obtain information needed for designing your research.\nFirst, let’s just plot the DAG. When we define exposure, which is a different term for intervention or treatment, and the outcome, they are highlighted in another color.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model <- 'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nD -> Y\nD -> Z3\nZ1 -> D\nZ1 -> Y\nZ2 -> Y\nZ2 -> Z3\nZ3 -> Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\nLet’s check what paths there are from treatment \\(D\\) to \\(Y\\). Of course, there is a direct path (the causal path) and there are two other paths, of which one is closed.\n\n# find all paths\npaths(dag_model)\n\n$paths\n[1] \"D -> Y\"             \"D -> Z3 <- Z2 -> Y\" \"D <- Z1 -> Y\"      \n\n$open\n[1]  TRUE FALSE  TRUE\n\n\nWe can also plot the open paths. One path is already blocked by a collider (remember: we do not want to open that path).\n\n# plot paths\nggdag_paths(dag_model) +\n  theme_dag_cds()\n\n\n\n\n\n\n\n\nTo see what we have to adjust for to isolate the causal effect, we use adjustmentsSets(). As you might have figured out already, it is \\(Z1\\) that needs to be conditioned on.\n\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n\n{ Z1 }\n\n\nA very concise summary plot is returned by the function ggdag_adjustment_set(), which shows what needs to be adjusted, the open paths and the whole DAG.\n\n# plot adjustment sets\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\nBut what does it actually mean in practice? How do we block a path or condition on a variable?\nIf you use a linear regression, including a variable as an independent variable is the sames as conditioning on it. If you use other models, you might have to use subsets and average them or do some kind of matching where you only compare units that have the same value for the variables that has to be conditioned on.\nIn the following chapters, we will deal with a large variety of techniques and figure out clever ways to isolate causal effects."
  },
  {
    "objectID": "content/fundamentals/03_caus.html",
    "href": "content/fundamentals/03_caus.html",
    "title": "Causality",
    "section": "",
    "text": "Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.\nHowever, there are many other contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure.\n\n\n\nDifference between Prediction and Explanation\n\n\nInstead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to understand characteristics of the data-generating process, i.e. the system of cause and effect and extract useful information from the data. That is what science is about, explaining why things are happening.\n\n\n\nExample 1 - confounding factors:\nA simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.\nBut what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. Ideally, we would want to look at a hotel at one specific point in time where all factors are fixed and then observe the number of bookings for different prices. In practice, this is impossible but causal methods try to get as close as possible to that scenario. Only then, we can extract valid estimates and are able to understand the underlying mechanism which help us and businesses to take the right actions and decisions.\nExample 2 - direction of causation:\nAnother example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong. Many machine learning models cannot take such prior knowledge into consideration when building models and will therefore yield wrong estimates. Causal methods, however, built on a research and identification strategy to include prior knowledge.\nBased on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains, that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "href": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "title": "Causality",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nNow, let’s think again about the research question. How can we find out what the benefit of having parking spots is? Ideally, we would be able to compute the individual treatment effect (\\(ITE\\)) of each store \\(i\\). That means, for each store, we would know what the sales would be with and without parking spots. Then we could take the difference of those two outcomes and we would know what part of the sales would be only attributable to having parking spots. This is called the individual treatment effect (\\(ITE\\)):\n\\[\n\\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\n\\(Y_{i1}\\) are sales when there are parking spots at store \\(i\\) and \\(Y_{i0}\\) are sales when there are no parking spots at store \\(i\\). However, observing both outcomes is impossible.\nTo compute the individual treatment effect we would have to know the amount of sales that would have happened in case the treatment was not assigned to e.g. store \\(A\\). Not being able to observe an observation unit in both states (= with and without treatment) is called the fundamental problem of causal inference, essentially a missing data problem.\nThis is why technically the outcomes \\(Y_{i1}\\) and \\(Y_{i0}\\) are potential outcomes. To come from potential outcomes to the observed outcome, we can use the switching equation. For example for store \\(A\\):\n\\[\n\\begin{align}\nY_A &= D_AY_{A1} + (1-D_A)Y_{A0} \\\\\n&= 0*Y_{A1} + 1*Y_{A0} \\\\\n&= Y_{A0}\n\\end{align}\n\\]\nWe are able to observe \\(Y_{A0}\\), the sales for store \\(A\\) having no parking spots, but we are not able to observe \\(Y_{A1}\\), the state in which store \\(A\\) would have parking spots. But to estimate a individual causal effect, we would have to know what happens when we intervene and when we don’t intervene.\n\\(Y_{A1}\\) and \\(Y_{A0}\\) are potential outcomes, of which the one actually happened is called factual and the one that did not happen is called counterfactual. Note, that they describe outcomes for the same unit and although we cannot observe one of them, we can still define it mathematically."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#average-treatment-effect",
    "href": "content/fundamentals/03_caus.html#average-treatment-effect",
    "title": "Causality",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nFor now, we will leave the ITE behind and focus on a metric that is more accessible in analyses, the average treatment effect (ATE). The average treatment is defined as\n\\[\n\\text{ATE} = E[Y_1 - Y_0] \\,\\,,\n\\]\nthe expected difference in outcomes under both states. So the causal effect is defined as a comparison between two states of the world, the actual or factual state compared to the never observed counterfactual world.\nOther forms of average treatment effects are the average treatment effect on the treated (ATT) and the average effect on the untreated (ATU).\n\\[\n\\begin{align}\nATT = E[Y_1 - Y_0|D = 1] \\\\\nATU = E[Y_1 - Y_0|D = 0]\n\\end{align}\n\\]\nNow let’s ignore the fundamental problem of causal inference for a minute and imagine the impossible scenario that we would be able to observe all outcomes for all stores for all different states. That means, we would be able to magically know the sales of each stores with and without parking spots. Just for illustration, the unobserved outcomes are crossed out, but we’ll still use them for computation.\n\n\n\n\\(i\\)\n\\(Y_{i0}\\)\n\\(Y_{i1}\\)\n\\(D_i\\)\n\\(Y_i\\)\n\\(\\text{ITE}\\)\n\n\n\n\n\\(A\\)\n135\n145\n0\n135\n+10\n\n\n\\(B\\)\n121\n125\n0\n121\n+4\n\n\n\\(C\\)\n74\n102\n1\n102\n+28\n\n\n\\(D\\)\n68\n94\n1\n94\n+26\n\n\n\nKnowing all states, we would be able to easily compute the average treatment effect by averaging the last column \\(ITE\\). The \\(ATE\\) is the average of all \\(ITE\\) and in this impossible scenario, we actually know the true estimate:\n\\[\n\\text{ATE} = \\frac{1}{4}(28 + 26 + 10 + 4)= 17\n\\]\nWe can already see that for the treated stores, the ones with parking spots, the treatment effect is way higher. We can show that by calculating the average treatment effect for the treated (\\(D_i = 1\\)) and for the untreated (\\(D_i=0\\)).\n\\[\nATT = \\frac{1}{2}(28+26) = 27 \\\\\nATU = \\frac{1}{2}(10+4) = 7\n\\]\nBut again, we cannot see the table as it is shown above but instead, what we would see is the following table.\n\n\n\nStore\n\\(y_0\\)\n\\(y_1\\)\n\\(d\\)\n\\(y\\)\n\\(\\text{ITE}\\)\n\n\n\n\n\\(A\\)\n135\n-\n0\n135\n-\n\n\n\\(B\\)\n121\n-\n0\n121\n-\n\n\n\\(C\\)\n-\n102\n1\n102\n-\n\n\n\\(D\\)\n-\n94\n1\n94\n-\n\n\n\nOne idea you could come up with is to compare the mean of treated units to the mean of untreated units and take the difference as the ATE. Treated units are called the treatment group while untreated units are called control group. Knowing the true average treatment effect from our hypothetical table above, let’s see how it works.\n\\[\n\\text{ATE} = E[Y|D=1] -E[Y|D=0] = \\frac{102+94}{2} - \\frac{135+121}{2} = -30\n\\]\nThis would leave us with an average treatment effect of \\(-30\\), which is is very far away from our true estimate of \\(+27\\). In fact, it even goes in the other direction. This is why we need to be extremely careful when attempting to prove causal effects. Naive estimations and simple methods might not only under- or overestimate the effect or not identify a true effect, but they could get it even completely wrong."
  },
  {
    "objectID": "content/toolbox/05_rct.html",
    "href": "content/toolbox/05_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "This chapter will be unlocked on 30/11/22."
  },
  {
    "objectID": "content/toolbox/08_iv.html",
    "href": "content/toolbox/08_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "This chapter will be unlocked on 30/11/22."
  },
  {
    "objectID": "content/toolbox/06_match.html",
    "href": "content/toolbox/06_match.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "This chapter will be unlocked on 30/11/22."
  },
  {
    "objectID": "content/toolbox/07_did.html",
    "href": "content/toolbox/07_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "This chapter will be unlocked on 30/11/22."
  },
  {
    "objectID": "content/toolbox/09_rdd.html",
    "href": "content/toolbox/09_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "This chapter will be unlocked on 30/11/22."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Content",
    "section": "",
    "text": "Welcome to “Causal Data Science for Business Analytics”!\nIn this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nAccording to a large global survey1 conducted among data science practitioners in industry in 2020, 83% of respondents consider causal inference increasingly important for data-driven decision making. Moreover, 44% state that, in their data science projects, causal inference already is important."
  },
  {
    "objectID": "content/index.html#mattermost",
    "href": "content/index.html#mattermost",
    "title": "Content",
    "section": "Mattermost",
    "text": "Mattermost\nIn the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you have any questions about the class content, coding problems and other challenges, please use our Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\nFollow these steps to join the channel:\n\nGo on https://communicating.tuhh.de/\nClick Click here to sign in\nClick the Button GitLab\nYou may need to login to GitLab with your Kerberos/LDAP data (e.g. cba1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\nAfter accessing Mattermost, join the team W-11 students\nJoin Causal Data Science Channel (you might need to wait a bit, as I first have to add you)\n\nThere, and in the classes of course, I will try to help you as much as possible.\nIn order to keep the discussion efficient and manageable it is necessary that we all follow some basic rules:\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\n\n \n```r\nx %>% sum()\n```\n\n\n\n**Error:**\nError in x %>% sum() : could not find function \"%>%\"\n\n\n\n\nMinimalistic example of how you could post an error in Mattermost. See how little formatting makes the code very nice to read."
  },
  {
    "objectID": "content/index.html#installing-r-rstudio-ide",
    "href": "content/index.html#installing-r-rstudio-ide",
    "title": "Content",
    "section": "Installing R & RStudio IDE",
    "text": "Installing R & RStudio IDE\nBefore we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -> Global options -> Appearance and switch theme in -> Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab."
  },
  {
    "objectID": "content/index.html#introduction-to-r",
    "href": "content/index.html#introduction-to-r",
    "title": "Content",
    "section": "Introduction to R",
    "text": "Introduction to R\nOne last note before you start coding: don’t be intimidated by the errors, warnings and other messages that you (and everyone else) will without doubt receive. There is no reason to panic just because you see red text in your console and in fact, what is returned will often times already help you to solve the problem.\nThere are three different types of texts:\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\nMessages: these are just friendly texts that provide you with useful information.\n\nInteractive Tutorials:\nBut let’s no more talk about it but instead start coding because the best way to get familiar with R and to code is to just start.\nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console.\nFeel free to complete as many tutorials as you want, but for this class, the following tutorial is of particular use: The R Programming Environment (Chapter 2-12)\nswirl()does not come with R by default but is an optional package. R packages are extensions of the base functionality implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nInstall the package (one). As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nLoad the package (always). By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later.\n\nswirl()\n\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")\n\nYou don’t need to submit anything from this step. Just focus on getting familiar with R by completing the tutorial!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "",
    "text": "lv3060Winter term 2022/2023Institute of EntrepreneurshipTUHH"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "INSTRUCTOR",
    "text": "INSTRUCTOR\n Oliver Mork\n oliver.mork@tuhh.de\n https://www.startupengineer.io/"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "COURSE DETAILS",
    "text": "COURSE DETAILS\n 23.11. / 30.11. / 07.12.\n 09:00 - 17:00\n HS28 - 1.006"
  }
]