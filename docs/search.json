[
  {
    "objectID": "content/fundamentals/04_dag.html",
    "href": "content/fundamentals/04_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "We have already learned that correlation and causation can easily be confused. Now, we will define concepts and acquire tools that help us in developing credible identification strategies to separate correlation from causation. One essential part is graphically modeling your theoretical knowledge about the data-generating process.\nIn causal inference, directed acylic graphs (DAGs) do the graphic modeling part. They are the foundation of any analysis strategy and moreover communicate your research plan.\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nA DAG should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?\n\nBesides being helpful in guiding your analysis and identification strategy, DAGs also show your research design to your audience.\nA simple example of a DAG could be the effect of having an university degree on future salary: at first, it might be intuitive to say that future salary increases when you get an university degree.\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# without confounder\nschooling_1 <- dagify(\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3),\n                y = c(uni_degree = 1, salary = 1))\n)\n\n# Plot DAG\nggdag(schooling_1, use_labels = \"name\", text = F) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n# with confounder\nschooling_2 <- dagify(\n  uni_degree ~ ability,\n  salary ~ ability,\n  salary ~ uni_degree,\n  coords = list(x = c(uni_degree = 1, salary = 3, ability = 2),\n                y = c(uni_degree = 1, salary = 1, ability = 2))\n)\n\n# Plot DAG\nggdag(schooling_2, use_labels = \"name\", text = F) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut zooming out and thinking about why a university degree is correlated with higher salaries could lead you to the idea that both university degree and salary are influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree.\nIt is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#chain",
    "href": "content/fundamentals/04_dag.html#chain",
    "title": "Directed Acyclic Graphs",
    "section": "Chain",
    "text": "Chain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\nCode# Chain\nchain <- dagify(\n  Y ~ Z,\n  Z ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 0, X = 0))\n)\n\n# Plot DAG\nggdag(chain) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nAn example of such a causal mechanism (page 37, Pearl) could be the effect of work hours on training and training on race time. In the DAG, the variables would be:\n\n\\(X\\): work hours\n\\(Z\\): training\n\\(Y\\): race time\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nIn terms of dependencies,\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow.\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on the training amount, that means we hold training amount fixed at a particular level, then there is no effect from work hours to race time as there is no direct effect, but only an effect through the amount of training. In other words, for individuals that differ in the hours they work but still have the same amount of training, there is no association between working hours and race time.\n\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#fork",
    "href": "content/fundamentals/04_dag.html#fork",
    "title": "Directed Acyclic Graphs",
    "section": "Fork",
    "text": "Fork\nAnother mechanism is the fork, also called common cause.\n\nCode# Fork\nfork <- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0))\n)\n\n# Plot DAG\nggdag(fork) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nAgain, let’s check in term of dependencies:\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\n\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collision",
    "href": "content/fundamentals/04_dag.html#collision",
    "title": "Directed Acyclic Graphs",
    "section": "Collision",
    "text": "Collision\nThe last mechanism is the collision, which is also called common effect.\n\nCode# Collider\ncollider <- dagify(\n  Z ~ X,\n  Z ~ Y,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1))\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\\(X\\): attractiveness\n\\(Y\\): talent\n\\(Z\\): celebrity\n\nFirst of all, in the general population, there is no correlation between attractiveness and talent (3rd dependency). Second, being either attractive or having a talent will help you to become a celebrity (1st and 2nd dependency).\nBut what about the last dependency? Why are attractiveness and talent suddenly correlated when conditioned on e.g. being a celebrity? That is because when you know someone is a celebrity and has no talent, the likelihood that he/she is attractive increases because otherwise he/she would likely not be a celebrity. Vice versa, if you know someone is a celebrity and is not attractive, he/she is probably talented in some form.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\))."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#confounding",
    "href": "content/fundamentals/04_dag.html#confounding",
    "title": "Directed Acyclic Graphs",
    "section": "Confounding",
    "text": "Confounding\nA very common problem when trying to prove causal effects is confounding.\n\nCode# Confounder\nconfounding <- dagify(\n  X ~ Z,\n  Y ~ Z,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0)),\n  labels = list(X = \"university degree\",\n                Y = \"salary\",\n                Z = \"ability\")\n)\n\n# Plot DAG\nggdag(confounding, use_labels = \"label\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nIt is the same DAG as in the introduction. So just recall and imagine \\(X\\) was university degree, \\(Y\\) future salary and \\(Z\\) ability.\nThere are two paths from \\(X\\) to \\(Y\\):\n\ndirect path \\(X \\rightarrow Y\\)\nbackdoor path \\(X \\leftarrow Z \\rightarrow Y\\)\n\nFirst of all, it is important to clarify what effect we are actually interested in and that is the direct causal effect of university degree on future salary, \\(X\\) on \\(Y\\).\nThe indirect effect is not causal but is only spurious correlation induced by \\(Z\\) which we are generally not interested in. It is called backdoor path.\nSo how do we proceed to extract that effect? We need to somehow remove the association between \\(X\\) and \\(Y\\) that is only due to variation in \\(Z\\). And from the previous section we know how to do that. \\(X\\) and \\(Y\\) need to be independent conditional on \\(Z\\) and block the path from \\(X\\) to \\(Y\\) over \\(Z\\).\nBut what does blocking the path mean? It means that we have to condition on \\(Z\\), to keep it at a fixed level. Then, the variations in \\(X\\) that cause \\(Y\\) to vary are not due to \\(Z\\) because it does not vary at all and cannot have an impact on either \\(X\\) or \\(Y\\). Doing that we closed the backdoor and are able to retrieve the causal effect.\nNot blocking the path would falsify our results and is what is called the omitted variable bias. That is why \\(Z\\) is called confounder, because it confounds the ability to measure the causal effect.\nHowever, the main problem is that in many cases you might not be able to block the path for two different reasons:\n\nYou are aware of the confounder, but you did not collect data for it\nYou are not aware of the confounder (and probably did not collect data for it)\n\nThis stresses the importance of theoretical knowledge about the phenomenon you are researching. Without it, it is very unlikely that you can prove truly causal effects. The risk of not paying attention to confounders gets clear when we look at graphs visualizing an example of Simpson’s paradox: accounting for a third variable reverses the sign of correlation.\nWe can illustrate it with an imaginary example. Let’s assume you want to measure how a specific characteristic affects salary. So you start to collect data about both variables, throw them into a regression and your result tells you that there is a positive correlation. But what happens if you take the education level into account? You can see how the lines show a positive correlation on the left and a negative correlation on the right. When you include a third variable, the relationship reverses.\nIt is neither wrong or right to always include or exclude variables, but it depends on the application and question you want to answer. This is why causal reasoning is so important.\n\nCode# Simpson's paradox ----\n\n# Simulate data\n# number of observations\nn <- 1e+03\n\n# draw and create variables with specific dependencies\neducation <- rbinom(n, 2, 0.5)\ncharacter <- rnorm(n) + education\nsalary <- education * 2 + rnorm(n) - character * 0.3\n\n# rescale to realistic values\nsalary <- sample(10000:11000,1) + scales::rescale(salary, to = c(0, 100000))\ncharacter <- scales::rescale(character, to = c(0, 7))\neducation <- factor(education, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# create tibble\ndf <- tibble(\n  salary,\n  character,\n  education\n)\n\n# Not conditioning on education\nsimps_not_cond <- ggplot(df, aes(x = character, y = salary)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on education  \nsimps_cond <- ggplot(df, aes(x = character, y = salary, color = education)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsimps_not_cond\nsimps_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collider",
    "href": "content/fundamentals/04_dag.html#collider",
    "title": "Directed Acyclic Graphs",
    "section": "Collider",
    "text": "Collider\nYou probably noticed that the confounding example was related to the common cause in the previous section. The next example is related to the common effect mechanism.\n\nCode# Collider   \ncollider <- dagify(\n  Z ~ X,\n  Z ~ Y,\n  Y ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1)),\n  labels = list(X = \"GPA\",\n                Y = \"musical talent\", \n                Z = \"accepted\")\n)\n\n# Plot DAG\nggdag(collider, use_labels = \"label\") +\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nNow consider the following example: a research group wants to examine if there is a causal relationship between grade point averages (\\(X\\)) and musical talent (\\(Y\\)). They can use data from their own university where both is measured for every student as scoring high in one of these characteristics substantially increases your chance of being accepted.\nNow the researchers, who hypothesized a positive or no correlation between \\(X\\) and \\(Y\\), perform a simple analysis and, to their surprise, find out that there is a strong negative correlation. This strange result can be explained by collider bias. Implicitly, as they only used data from their own university students, they also conditioned on a collider \\(Z\\), which is “accepted to university” and is fixed on a constant level of actually being accepted to the university.\nAgain, we have two paths from \\(X\\) to \\(Y\\), but one thing is different: the backdoor path is already closed because \\(Z\\) is a collider, both arrows point toward it.\nTo correct their results they would need to have data from the whole population and not only students from a university that puts focus on either grade point averages or musical talent. Having a full population as a sample would probably lead to the result that there is no correlation between \\(X\\) and \\(Y\\).\nThis case (collider) is different to the case before (confounding), where conditioning was the correct solution. But if you have a collider in you DAG, make sure not to condition on it as it creates a dependence between \\(X\\) and \\(Y\\). You “open” a backdoor path that was closed before, just due to the presence of the collider.\nCollider bias often arises when your sample is not very representative of the population you are making claims about. How that could change your result can be seen in an example Berkson’s paradox. While there is no correlation for the whole population, for smaller subgroups there are. And therefore, it is crucial that you clearly state what effect you are interested in.\n\n# Berkson's paradox ----\n\n# Simulate data\n# number of observations\nn <- 1e+03\n# draw and create data with specific dependencies\nability     <- rnorm(n)\nmotivation  <- rnorm(n)\naptitude    <- 1/2 * ability + 1/2 * motivation + rnorm(n, 0, .1)\n\n# create tibble\ndf <- tibble(\n  ability    = ability,\n  motivation = motivation,\n  aptitude   = aptitude,\n  student    = ifelse(aptitude > 0, \"student\", \"no_student\")\n)\n\n# Not conditioning on student\nberk_not_cond <- ggplot(df, aes(x = motivation, y = ability)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on student  \nberk_cond <- ggplot(df, aes(x = motivation, y = ability,\n                            color = student, \n                            alpha = student)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_color_manual(values = c(\"student\" = ggthemr::swatch()[4],\n                                \"no_student\" = ggthemr::swatch()[5])) +\n  scale_alpha_manual(values = c(\"student\" = 1, \"no_student\" = 0.2)) +\n  theme(legend.position = \"right\")\n\n# Plot both\nberk_not_cond\nberk_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#d-separation",
    "href": "content/fundamentals/04_dag.html#d-separation",
    "title": "Directed Acyclic Graphs",
    "section": "d-separation",
    "text": "d-separation\nOne concept, that we have not named yet but implicitly used, is d-separation. If an effect of \\(X\\) on \\(Y\\) is d-separated, there is no statistical association that can flow between \\(X\\) and \\(Y\\) except for the direct effect. In fact, d-separation determines conditional independence.\nD-separation formalizes what we have already learned when going through the tree types of association.\nPractically (http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html) , it is easier to define the opposite, d-connection:\n\nRule 1: unconditional separation: \\(X\\) and \\(Y\\) are d-connected if there is an unblocked path between them. (As an example, imagine a confounder, that is not conditioned on.)\nRule 2: blocking by conditioning: \\(X\\) and \\(Y\\) are d-connected, conditioned on a set of \\(Z\\) nodes, if there is a collider-free path between \\(X\\) and \\(Y\\) that traverses no member. (Think of a mediated effect that takes away parts from the direct effect.)\nRule 3: conditioning on colliders: If a collider is a member of conditioning set \\(Z\\), or has a descendant in \\(Z\\), then it no longer blocks any path that traces this collider. (Image the collider example in the previous section.)\n\n!!! MAYBE CHANGE TO https://de.wikipedia.org/wiki/D-Separation\nKnowing these rules and having mapped our assumptions into the DAG allows us to treat observational data like experimental data and simulate interventions as we would have conducted an experiment. However, it is not a silver bullet as in many cases data availability will stop you from isolating the causal effect. For example, if you do not observer a confounder, you cannot control for it. All empirical work requires theory and with observational data we need to be extra careful to make sure to actually extract the effects we are interested in."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "href": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "title": "Directed Acyclic Graphs",
    "section": "Backdoor/Frontdoor Criterion",
    "text": "Backdoor/Frontdoor Criterion\nSpecial cases that are derived from d-separation rules are the backdoor and frontdoor criterion/adjustment.\nTo satisfy the backdoor criterion, we have to make sure all backdoors are closed, which, as already mentioned, differs for confounders and colliders.\n\n\n\n\nAgain, we want to block all other paths between \\(X\\) (treatment) and \\(Y\\) (outcome). So depending on the structure of the DAG, the following can block a path:\n\na chain or a fork whose middle node is in \\(Z\\)\na collider that is not conditioned on, which means it is not in \\(Z\\)\n\nThe frontdoor criterion, which is actually a consecutive application of the backdoor criterion, is a bit more complicated and we will leave it out for now, but in a the section about instrumental variables we will deal with it extensively."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "href": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "title": "Directed Acyclic Graphs",
    "section": "Algorithms to identify causally valid estimates",
    "text": "Algorithms to identify causally valid estimates\nIf DAGs become more complex because there are a lot of variables that are somehow related, we can make use of algorithms to check all the rules for us.\nOne application to help in such cases is http://dagitty.net/dags.html, where you can draw your DAG, define what is the treatment and outcome, which variables are observed and unobserved and many other things. Then it will show you what kind of adjustment is necessary to estimate the causal effect of interest.\n\n\n\n\nTry to build this graph on dagitty.net and look what useful information you can get from the site.\ndagitty is also implemented in R and combined with ggdag you are also able to plot your DAGs in a easy manner and obtain information needed for designing your research.\nFirst, let’s just plot the DAG. When we define exposure, which is a different term for intervention or treatment, and the outcome, they are highlighted in another color.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model <- 'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nD -> Y\nD -> Z3\nZ1 -> D\nZ1 -> Y\nZ2 -> Y\nZ2 -> Z3\nZ3 -> Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nLet’s check what paths there are from treatment \\(D\\) to \\(Y\\). Of course, there is a direct path (the causal path) and there are two other paths, of which one is closed.\n\n# find all paths\npaths(dag_model)\n\n$paths\n[1] \"D -> Y\"             \"D -> Z3 <- Z2 -> Y\" \"D <- Z1 -> Y\"      \n\n$open\n[1]  TRUE FALSE  TRUE\n\n\nWe can also plot the open paths. One path is already blocked by a collider (remember: we do not want to open that path).\n\n# plot paths\nggdag_paths(dag_model) +\n  theme_dag_cds()\n\n\n\n\n\n\n\nTo see what we have to adjust for to isolate the causal effect, we use adjustmentsSets(). As you might have figured out already, it is \\(Z1\\) that needs to be conditioned on.\n\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n\n{ Z1 }\n\n\nA very concise summary plot is returned by the function ggdag_adjustment_set(), which shows what needs to be adjusted, the open paths and the whole DAG.\n\n# plot adjustment sets\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nBut what does it actually mean in practice? How do we block a path or condition on a variable?\nIf you use a linear regression, including a variable as an independent variable is the sames as conditioning on it. If you use other models, you might have to use subsets and average them or do some kind of matching where you only compare units that have the same value for the variables that has to be conditioned on.\nIn the following chapters, we will deal with a large variety of techniques and figure out clever ways to isolate causal effects."
  },
  {
    "objectID": "content/fundamentals/03_caus.html",
    "href": "content/fundamentals/03_caus.html",
    "title": "Causality",
    "section": "",
    "text": "Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.\nHowever, there are many other contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure.\n\n\nDifference between Prediction and Explanation\n\n\nInstead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to understand characteristics of the data-generating process, i.e. the system of cause and effect and extract useful information from the data. That is what science is about, explaining why things are happening.\n\nExample 1 - confounding factors:\nA simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.\nBut what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. Ideally, we would want to look at a hotel at one specific point in time where all factors are fixed and then observe the number of bookings for different prices. In practice, this is impossible but causal methods try to get as close as possible to that scenario. Only then, we can extract valid estimates and are able to understand the underlying mechanism which help us and businesses to take the right actions and decisions.\nExample 2 - direction of causation:\nAnother example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong. Many machine learning models cannot take such prior knowledge into consideration when building models and will therefore yield wrong estimates. Causal methods, however, built on a research and identification strategy to include prior knowledge.\nBased on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains, that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "href": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "title": "Causality",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nNow, let’s think again about the research question. How can we find out what the benefit of having parking spots is? Ideally, we would be able to compute the individual treatment effect (\\(ITE\\)) of each store \\(i\\). That means, for each store, we would know what the sales would be with and without parking spots. Then we could take the difference of those two outcomes and we would know what part of the sales would be only attributable to having parking spots. This is called the individual treatment effect (\\(ITE\\)):\n\\[\n\\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\n\\(Y_{i1}\\) are sales when there are parking spots at store \\(i\\) and \\(Y_{i0}\\) are sales when there are no parking spots at store \\(i\\). However, observing both outcomes is impossible.\nTo compute the individual treatment effect we would have to know the amount of sales that would have happened in case the treatment was not assigned to e.g. store \\(A\\). Not being able to observe an observation unit in both states (= with and without treatment) is called the fundamental problem of causal inference, essentially a missing data problem.\nThis is why technically the outcomes \\(Y_{i1}\\) and \\(Y_{i0}\\) are potential outcomes. To come from potential outcomes to the observed outcome, we can use the switching equation. For example for store \\(A\\):\n\\[\n\\begin{align}\nY_A &= D_AY_{A1} + (1-D_A)Y_{A0} \\\\\n&= 0*Y_{A1} + 1*Y_{A0} \\\\\n&= Y_{A0}\n\\end{align}\n\\]\nWe are able to observe \\(Y_{A0}\\), the sales for store \\(A\\) having no parking spots, but we are not able to observe \\(Y_{A1}\\), the state in which store \\(A\\) would have parking spots. But to estimate a individual causal effect, we would have to know what happens when we intervene and when we don’t intervene.\n\\(Y_{A1}\\) and \\(Y_{A0}\\) are potential outcomes, of which the one actually happened is called factual and the one that did not happen is called counterfactual. Note, that they describe outcomes for the same unit and although we cannot observe one of them, we can still define it mathematically."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#average-treatment-effect",
    "href": "content/fundamentals/03_caus.html#average-treatment-effect",
    "title": "Causality",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nFor now, we will leave the ITE behind and focus on a metric that is more accessible in analyses, the average treatment effect (ATE). The average treatment is defined as\n\\[\n\\text{ATE} = E[Y_1 - Y_0] \\,\\,,\n\\]\nthe expected difference in outcomes under both states. So the causal effect is defined as a comparison between two states of the world, the actual or factual state compared to the never observed counterfactual world.\nOther forms of average treatment effects are the average treatment effect on the treated (ATT) and the average effect on the untreated (ATU).\n\\[\n\\begin{align}\nATT = E[Y_1 - Y_0|D = 1] \\\\\nATU = E[Y_1 - Y_0|D = 0]\n\\end{align}\n\\]\nNow let’s ignore the fundamental problem of causal inference for a minute and imagine the impossible scenario that we would be able to observe all outcomes for all stores for all different states. That means, we would be able to magically know the sales of each stores with and without parking spots. Just for illustration, the unobserved outcomes are crossed out, but we’ll still use them for computation.\n\n\n\\(i\\)\n\\(Y_{i0}\\)\n\\(Y_{i1}\\)\n\\(D_i\\)\n\\(Y_i\\)\n\\(\\text{ITE}\\)\n\n\n\n\\(A\\)\n135\n145\n0\n135\n+10\n\n\n\\(B\\)\n121\n125\n0\n121\n+4\n\n\n\\(C\\)\n74\n102\n1\n102\n+28\n\n\n\\(D\\)\n68\n94\n1\n94\n+26\n\n\n\nKnowing all states, we would be able to easily compute the average treatment effect by averaging the last column \\(ITE\\). The \\(ATE\\) is the average of all \\(ITE\\) and in this impossible scenario, we actually know the true estimate:\n\\[\n\\text{ATE} = \\frac{1}{4}(28 + 26 + 10 + 4)= 17\n\\]\nWe can already see that for the treated stores, the ones with parking spots, the treatment effect is way higher. We can show that by calculating the average treatment effect for the treated (\\(D_i = 1\\)) and for the untreated (\\(D_i=0\\)).\n\\[\nATT = \\frac{1}{2}(28+26) = 27 \\\\\nATU = \\frac{1}{2}(10+4) = 7\n\\]\nBut again, we cannot see the table as it is shown above but instead, what we would see is the following table.\n\n\nStore\n\\(y_0\\)\n\\(y_1\\)\n\\(d\\)\n\\(y\\)\n\\(\\text{ITE}\\)\n\n\n\n\\(A\\)\n135\n-\n0\n135\n-\n\n\n\\(B\\)\n121\n-\n0\n121\n-\n\n\n\\(C\\)\n-\n102\n1\n102\n-\n\n\n\\(D\\)\n-\n94\n1\n94\n-\n\n\n\nOne idea you could come up with is to compare the mean of treated units to the mean of untreated units and take the difference as the ATE. Treated units are called the treatment group while untreated units are called control group. Knowing the true average treatment effect from our hypothetical table above, let’s see how it works.\n\\[\n\\text{ATE} = E[Y|D=1] -E[Y|D=0] = \\frac{102+94}{2} - \\frac{135+121}{2} = -30\n\\]\nThis would leave us with an average treatment effect of \\(-30\\), which is is very far away from our true estimate of \\(+27\\). In fact, it even goes in the other direction. This is why we need to be extremely careful when attempting to prove causal effects. Naive estimations and simple methods might not only under- or overestimate the effect or not identify a true effect, but they could get it even completely wrong."
  },
  {
    "objectID": "content/fundamentals/01_b_stats.html",
    "href": "content/fundamentals/01_b_stats.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Now we will talk about some statistical concepts, that are the foundation for modeling processes in both statistical and causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to show here.\nRandom Variable\nFor starters, let’s again define what a random variable is. Often represented by letters such as \\(X\\), a random variable has a set of values, also called sample space, of which any could be the outcome if we draw a random sample from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die) or continuous values (e.g. average height of individuals).\nExpected value\nA random variable is a real-valued function and can with more than one possible outcomes. This is why we cannot represent it as a scalar. The expected value of random variable, however, is a scalar and represents something like a “summary” of the random variable. Knowing the possible values (from the sample space) and the probability distribution, we can compute the expected value.\n\n\n\n\n\n\nTip\n\n\n\nThe summation operator \\(\\sum\\), denoted by the Greek capital Sigma is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, \\(x_1, x_2, …, x_n\\) to a shorter and more readable form\n\\[\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n\\]\nwith the arbitrary index of summation \\(i\\) being the lower limit and \\(n\\) the upper limit.\nBy basic math rules, the following simplifications are possible, where \\(c\\) is a constant:\n\\[\n\\sum_{i=1}^nc=nc\n\\]\nand\n\\[\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n\\]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe expected value, also called population mean, of a variable \\(X\\) is defined as the weighted average of possible values the random variable can take, where the weight is equal to the probability of the random variable taking on a specific value.\n\n\nIf we want to compute it for a discrete random variable, we make use of the summation operator. Considering a finite list of potential values \\(x_1, x_2, …, x_k\\) with probabilities \\(p_1, p_2, …, p_k\\), the expectation of \\(X\\) can be computed by\n\\[\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n\\]\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\\[\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n\\]\nProbabilities could also be different, as long as they sum to 1. For continuous random variables, we need a function returning the probabilities for each value. We leave that out here as we are only interested in understanding the intuition behind the expected value. However, it is very similar.\nLet’s type in the probabilities and outcomes of rolling a die and see what the expected value is.\n\n\n\n\n\n\nNote\n\n\n\nTo replicate values and create a vector rep() can be used. It takes the value and number of times it should be replicated.\n\n\n\n# Vector of probabilities (all equal)\np <- rep(1/6, 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n\n[1] 3.5\n\n\nAs you might have expected, it’s 3.5. In this case, we know the probabilities, but if we had not known them beforehand, what we could have done to get an estimate of the expected value is to roll the die a lot of times, store the results in an object and use the mean() function. It would yield an expected value close to 3.5.\n\n\n\n\n\n\nTip\n\n\n\nAdditional rules regarding the calculation of expected values that can be useful are:\n\\[\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n\\]\n\n\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.\nConditional Expected Value\nThe conditional expected value is the expected value conditioned on some other value. Given the value \\(x\\) of \\(X\\), the expected value for \\(Y\\) obtains as\n\\[\nE[Y|X = x]\n\\]\nand is a function of \\(x\\). In other words, the conditional expected value is the best guess for \\(Y\\) knowing only that \\(X=x\\).\nAs a simple example, consider we take a representative random sample from the world population and want to compute the expected value for \\(height\\). Denoting height with \\(Y\\), the expected value for the whole population is the expected value \\(E[Y]\\) over all individuals in your sample.\nThe conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.\n\\[\nE[Y] \\neq E[Y|age < 10] \\neq E[Y|age >= 10]\n\\]\nVariance\nBefore we define variance, let’s see why it is important to know. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.\n\n\n\n\n\n\nNote\n\n\n\nseq(from, to, by) or seq(from, to, length.out) returns a vectors with a sequence as specified by the arguments.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrnorm(n, mean, sd) samples values from the normal distribution. n specifies the number of values, mean and sd define the parameters of the normal distribution.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmap() is a very useful function when you want to apply a function to each element of a list or a vector.\n\n\n\n\n\n\n\n\nNote\n\n\n\npivot_longer() changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need pivot_wider().\n\n\n\nCode# Load tidyverse package\nlibrary(tidyverse)\n\n# 100 step-wise values from 1 to 5\nX <- seq(1, 5, length.out = 100)\n\n# For each value x_i of X sample from normal distribution with mean equal to x_i\n# Note the different values for standard deviation\nY_lv <- map(X, function(i) rnorm(1, i, 0.25)) %>% unlist()\nY_hv <- map(X, function(i) rnorm(1, i, 1.5)) %>% unlist()\n\n# Create tibble with X and YY values for low and high variance\ndf <- tibble(X = X, \n             Y_low_variance = Y_lv,\n             Y_high_variance = Y_hv) %>%\n  pivot_longer(cols = c(Y_low_variance, Y_high_variance))\n\n# Plot both X~Y relations as scatter plot\nggplot(df, aes(X, value)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  facet_wrap(~name)\n\n\n\n\n\n\n\nMathematically, the variance is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The sample variance indicates how far a set of observed values spread out from their average value and is an estimate of the full population variance, that in most cases cannot be directly observed due to lack of data of the whole population.\nMathematically, the population variance is defined as\n\\[\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n\\]\nand the sample variance results as\n\\[\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n\\]\nYou might have noticed the term \\((n-1)^{-1}\\) is different from what you probably expected (\\(n^{-1}\\)). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.\n\n# High variance\nvar(Y_hv)\n\n[1] 3.8\n\n\n\n# Low variance\nvar(Y_lv)\n\n[1] 1.5\n\n\nThe standard deviation obtains as the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\n\n# High standard deviation\nsd(Y_hv)\n\n[1] 2\n\n\n\n# Low standard deviation\n# High standard deviation\nsd(Y_lv)\n\n[1] 1.2\n\n\n\n\n\n\n\n\nTip\n\n\n\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of \\(a\\), then the variance will increase by \\(a^2\\).\n\\[\nVar(aX+b)=a^2V(X)\n\\]\nYou can also conveniently compute a variance for the sum of two random variables\n\\[\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n\\]\nwhich in case of independence reduces to the sum of the individual variances due to the fact that \\(E(XY) = E(X)E(Y)\\).\n\n\nCovariance\nCovariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.\n\n\n\n\n\n\nNote\n\n\n\nas_tibble() or as.data.frame() can be used to change e.g. matrices or lists into tables.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrbind() or bind_rows() are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are cbind() and bind_cols().\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo rename columns, you can use rename(new_name = old_name).\n\n\nIf you’re interested in how to create correlated variable, you can open the following code chunk.\n\nCode# Create the variance covariance matrix\nsigma <- rbind(c(1, 0.8, 0), c(0.8, 1, -0.5), c(0, -0.5, 1))\n# Create the mean vector\nmu <- c(0, 0, 0) \n\n# Generate the multivariate normal distribution\ndf <- as_tibble(MASS::mvrnorm(n = 1e+3, mu = mu, Sigma = sigma)) %>% \n  rename(X1 = V1, X2 = V2, X3 = V3)\n\n\nWe defined the following covariance matrix:\n\n# Show variance-covariance matrix\nsigma\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.8  0.0\n[2,]  0.8  1.0 -0.5\n[3,]  0.0 -0.5  1.0\n\n\nFrom left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.\n\nCodeggplot(df, aes(x = X1, y = X2)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"Positive correlation\")\nggplot(df, aes(x = X2, y = X3)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"Negative correlation\")\nggplot(df, aes(x = X1, y = X3)) +\n  geom_point() +\n  stat_ellipse(level = .99, color = ggthemr::swatch()[3]) +\n  labs(title = \"No correlation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the equality \\(E(XY) = E(X)E(Y)\\) holds, then it implies a covariance of 0 between the variables \\(X\\) and \\(Y\\). Covariance is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.\nAs a matter of form, the formula for the covariance of the random variables \\(X\\) and \\(Y\\) is\n\\[\nCov(X,Y) = E(XY) - E(X)E(Y)\n\\]\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the correlation which can be derived from the covariance if the individual variances are known.\n\\[\n\\text{Corr}(X,Y) = \\dfrac{C(X,Y)}{\\sqrt{V(X)V(Y)}}\n\\]\nThe correlation is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.\n\n\n\n\n\n\nNote\n\n\n\nTo compute correlation, variance and covariance, respectively, you can use cor(), var() and cov().\n\n\nConclusion\nMany of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.\nAssignments\nFrom the data folder, load the table random_vars.rds (by using table_name <- readRDS(\"your_download_folder/random_vars.rds\") or finding the file in the Files pane and selecting it). It contains draws from two random variables, age and income. By the way, after you loaded the table, you can have a look at the whole table using View(random_vars).\n\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nexpected value\nvariance\nstandard deviation\n\n\nExplain, if it makes sense to compare the standard deviations.\nThen, examine the relationship between both variables and compute:\n\ncovariance\ncorrelation\n\n\nWhat measure is easier to interpret? Please discuss your interpretation.\nCompute the conditional expected value for:\n\n\\(E[income|age <= 18]\\)\n\\(E[income|age \\in [18, 65)]\\)\n\\(E[income|age >= 65]\\)\n\n\n\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to successfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions."
  },
  {
    "objectID": "content/fundamentals/02_reg.html",
    "href": "content/fundamentals/02_reg.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Statistical inference aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores and a lot of other business-related units.\nIn practice, it is often impossible to collect data about the whole population, which is why we draw (ideally random) samples from the whole population and use statistical inference to draw conclusions about the whole population using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#r",
    "href": "content/fundamentals/02_reg.html#r",
    "title": "Regression and Statistical Inference",
    "section": "R",
    "text": "R\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in R. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\nBefore we start, we load the R package tidyverse, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\n# Load tidyerse package\nlibrary(tidyverse)\n\nWe create a tibble, which is a table containing data, with two columns, \\(x\\) and \\(y\\). For \\(x\\), we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable \\(y\\), we make dependent from \\(x\\), i.e. for each unit \\(i\\), we create a \\(y_i\\) as\n\\[\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n\\]\nwhere \\(\\epsilon_i\\) is random noise.\nLet’s have a look at how the data is presented. We have a table containing 100 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n# Simluate data\n\n# number of observations\nn <- 10\n\n# Create tibble\nlm_dat <- tibble(\n  # draw from normal distribution\n  x = rnorm(n, mean = 3, sd = 1),\n  # y depends on x and noise from normal distribution\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\n# Show data\nlm_dat\n\n\n\n  \n\n\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between \\(X\\) and \\(Y\\). However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\nLet’s have a look at some random lines.\n\n\n\n\n\n\n\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the residuals are zero.\n\n\n\n\n\n\n\n\nThe resulting line is highlighted in blue.\nIf we want to mathematically compute the line in R, we have to use the lm() function and provide data and the assumed functional relationship as arguments. lm() is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: lm_mod), which we can interpret best when using summary() to show the resulting coefficients and other statistical information.\n\n\n\n\n\n\nNote\n\n\n\nlm() is a function that fits a linear model. You have to provide data and a regression equation in the form of for example outcome ~ regressor_1 + regressor_2 or outcome ~ ., if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call summary().\n\n\nLooking at the regression summary, we see that the line is modeled by \\(y = -0.1918 + 0.3354*x\\). It means that for the fitted model, an increase of one unit in \\(x\\) is related to an 0.3354 increase in \\(y\\). That is relatively close to what we simulated (0.3) and deviates due to the added random noise.\n\n# Fit model and print summary\nlm_mod <- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = y ~ x, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18597 -0.05210 -0.00783  0.06584  0.24115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1918     0.1300   -1.48     0.18    \nx             0.3354     0.0443    7.57  6.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 8 degrees of freedom\nMultiple R-squared:  0.878, Adjusted R-squared:  0.862 \nF-statistic: 57.3 on 1 and 8 DF,  p-value: 6.48e-05\n\n\nNow, let’s check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations \\(y_i\\) and predictions \\(\\hat{y_i}\\) for each \\(i\\) or plotting the residuals \\(r_i = y_i - \\hat{y_i}\\) and comparing it to the \\(x\\)-axis.\n\n# Add fitted values and residuals to data\nlm_dat_fit <- lm_dat %>% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")"
  },
  {
    "objectID": "content/fundamentals/02_reg.html#math",
    "href": "content/fundamentals/02_reg.html#math",
    "title": "Regression and Statistical Inference",
    "section": "Math",
    "text": "Math\nMathematically, the best line is found by the ordinary least squares (OLS) method.\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\nGiven \\(n\\) samples of observed pairs of dependent and independent variables \\(\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}\\), we plug any of them into the equation\n\\[\ny_i=\\beta_0+\\beta_1x_i+u_i\n\\]\nand together with our assumptions \\(E(u) = 0\\) and \\(E(u|x)=0\\) we obtain the equations to be solved to retrieve estimates for \\(\\beta_0\\) and \\(\\beta_1\\).\nFrom the independence of \\(x\\) and \\(u\\) and our understanding of probabilities and expectations, we also know that the expected value of the product of \\(x\\) and \\(u\\) has to be zero: \\(E(xu)=0\\). Substituting \\(u\\) with \\(y-\\beta_0-\\beta_1\\), we obtain the two conditions that when being solved give us the optimal estimates for our \\(\\beta\\) parameters.\n\\[\n\\begin{align}\nE(y-\\beta_0-\\beta_1x) = E\\Big(x[y-\\beta_0-\\beta_1x]\\Big) = 0\n\\end{align}\n\\]\nTranslated into its sample counterpart:\n\\[\n\\begin{align}\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\end{align}\n\\]\nLooking at the sample equations, we know our sample size \\(n\\), our sampled values \\(y_i\\) and \\(x_i\\). The coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\nSkipping a few transformation steps, we obtain\n\\[\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n\\]\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our \\(\\beta_1\\) is defined as the covariance of \\(X\\) and \\(Y\\) divided by the variance of \\(X\\). It also shows that the variance of \\(X\\) has to be greater than zero, which means that not all values of \\(x_i\\) can be the same. You need to observe different values to be able to estimate how \\(y_i\\) reacts to \\(x_i\\).\n\\(\\beta_0\\) follows directly by plugging \\(\\beta_1\\) into \\(\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta}1\\overline{x}\\). A bar above a variable always represents the sample value of that particular variable. Thus,\\(\\beta_0\\) is (as expected) constant and linear in \\(\\beta_1\\).\nKnowing the equation for the regression line, we can compute fitted values \\(y_i\\) for all \\(i\\)\n\\[\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n\\]\nIn almost all cases however, \\(\\widehat{y}_i\\) won’t be equal to \\(y_i\\) but there will be a prediction error, commonly referred to as residual \\(\\widehat{u}_i\\). Make sure that you don’t mix it up with \\(u\\), the error term, which is always unobserved.\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\n\n\n\n\n\n\nInfo\n\n\n\nSquares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n\n\n\\[\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n\\]\nAgain, most of the residuals won’t be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#interpretation",
    "href": "content/fundamentals/02_reg.html#interpretation",
    "title": "Regression and Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\nSo let’s run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. We will see in later chapters why that might be dangerous.\n\n# Include all potential regressors\nlm_all <- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n\n\nCall:\nlm(formula = expected_cost ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-406.5 -123.9  -26.4  102.7  942.0 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 450.51998   28.77380   15.66   <2e-16 ***\nregionE       8.57388   16.25854    0.53   0.5981    \nregionS      -1.99015   16.66160   -0.12   0.9049    \nregionW       1.65634   16.93503    0.10   0.9221    \nsex          -9.38273   11.77360   -0.80   0.4257    \nsmoking     205.70391   13.21526   15.57   <2e-16 ***\nage           9.29488    0.51525   18.04   <2e-16 ***\nincome       -0.05113    0.00384  -13.30   <2e-16 ***\nbmi          -2.07654    0.76406   -2.72   0.0067 ** \nchildren     -9.25879   14.55410   -0.64   0.5248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185 on 990 degrees of freedom\nMultiple R-squared:  0.404, Adjusted R-squared:  0.398 \nF-statistic: 74.4 on 9 and 990 DF,  p-value: <2e-16\n\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\nThe estimate is what we know as regression coefficient from before, typically denoted by \\(\\widehat{\\beta}_i\\) or other Greek letters. As it is the estimated version, it has a hat. The estimated for \\(\\beta_i\\) It tells you by how much the dependent variable \\(\\widehat{y}\\) varies when a particular variable is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect of \\(x_i\\) ceteris paribus, Latin for “the others equal”. It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as controlling for the other variables."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#statistical-significance",
    "href": "content/fundamentals/02_reg.html#statistical-significance",
    "title": "Regression and Statistical Inference",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate.\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error. Most widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data.\nLet’s take the example of the correlation between smoking and expected_cost from the regression above. The null hypothesis states that there is no correlation between the dependent variable expected_cost and the independent variable smoking. The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis.The alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable. Because the p-value is low, we reject the null hypothesis and the alternative hypothesis (significant correlation) is true.\nStatistical significance can directly be derived from the p-value and an arbitrary significance level \\(\\alpha\\). However, the most widely used level of \\(\\alpha\\) is \\(0.05\\). Less often used are levels of \\(.1\\), \\(.01\\) or \\(.001\\).\nAn estimate with a p-value less than \\(\\alpha\\) is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level \\(\\alpha\\). Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that \\(age\\), \\(income\\), \\(bmi\\), and \\(smoking\\) are statistically significant (at different levels though, indicated by the number of stars).\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A \\((1-\\alpha)\\) confidence interval has a probability of \\((1-\\alpha)*100 \\%\\) to contain the true value of our estimated coefficient. That means, if we would sample \\(100\\) times, \\(\\beta_i\\) would be contained in the sample \\((1-\\alpha)*100\\) times.\n\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n\n              2.5 %  97.5 %\n(Intercept) 394.055 506.985\nregionE     -23.331  40.479\nregionS     -34.686  30.706\nregionW     -31.576  34.889\nsex         -32.487  13.721\nsmoking     179.771 231.637\nage           8.284  10.306\nincome       -0.059  -0.044\nbmi          -3.576  -0.577\nchildren    -37.819  19.302\n\n\n\n# alpha = 0.9\nconfint(lm_all, level = 0.90)\n\n                5 %    95 %\n(Intercept) 403.147 497.893\nregionE     -18.194  35.342\nregionS     -29.422  25.441\nregionW     -26.225  29.538\nsex         -28.767  10.001\nsmoking     183.946 227.461\nage           8.447  10.143\nincome       -0.057  -0.045\nbmi          -3.334  -0.819\nchildren    -33.221  14.703\n\n\nAn estimate whose interval is either completely positive or completely negative is different from zero and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the positive coefficient."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#model-selection",
    "href": "content/fundamentals/02_reg.html#model-selection",
    "title": "Regression and Statistical Inference",
    "section": "Model selection",
    "text": "Model selection\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\nLooking at the spread of residuals (\\(\\widehat{u}_i = y_i - \\widehat{y}_i\\)) we want them to be spread evenly around zero.\n\n# Plot histogram of residuals\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n\n\n\n\n\n\n\nWe can see that the residuals are in fact almost normally distributed.\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit is \\(R^2\\). It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\\[\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n\\]\nConveniently, \\(R^2\\) is always between 0 and 1 and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high \\(R^2\\) can even point to a biased model while a model with a low \\(R^2\\) can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of \\(R^2\\) does also not change the interpretation of the regression coefficients.\nA problem with \\(R^2\\) is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the \\(\\text{Adjusted} \\, R^2\\). It includes a term for the number of independent variables used.\n\\[\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n\\]\nwhere \\(n\\) is the sample size and \\(p\\) the number of independent variables. This way, you can compare models and account for their scarcity.\nLet’s build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n# Include only significant regressors\nlm_imp <- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + smoking, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-481.5 -138.9  -27.2  112.5  931.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  392.959     27.560   14.26   <2e-16 ***\nage            7.198      0.478   15.05   <2e-16 ***\nbmi           -2.071      0.824   -2.51    0.012 *  \nsmoking      198.401     14.283   13.89   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201 on 996 degrees of freedom\nMultiple R-squared:  0.296, Adjusted R-squared:  0.294 \nF-statistic:  140 on 3 and 996 DF,  p-value: <2e-16\n\n\nExcept for \\(bmi\\), coefficients are very similar. We’ll look into that in just a second. But first let us compare both models with regard to \\(\\text{(Adjusted)} \\, R^2\\).\nTo elegantly print variables in a specified format, you can use sprintf().\n\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.40\"\n\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.29\"\n\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike’s Information Criteria (AIC) and Bayesian Information Criteria (BIC). Both AIC and BIC penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\n\n[1] \"AIC: 13294.02\"\n\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n\n[1] \"AIC: 13447.79\"\n\n\nFor both criteria, the model with the lowest value is preferred.\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\nFor example, is it correct to assume a linear relationship between \\(bmi\\) and the outcome \\(expected\\_cost\\)? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term “linear”, so what can we do about it?\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. \\(bmi^2\\).\n\n# Include quadratic term for BMI\nlm_sq <- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + I(bmi^2) + smoking, \n    data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-389.6  -80.3   -2.6   90.2  350.2 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1964.4792    41.7901    47.0   <2e-16 ***\nage            7.8004     0.2919    26.7   <2e-16 ***\nbmi         -122.6122     2.9778   -41.2   <2e-16 ***\nI(bmi^2)       2.0803     0.0507    41.1   <2e-16 ***\nsmoking      184.4309     8.7113    21.2   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122 on 995 degrees of freedom\nMultiple R-squared:  0.739, Adjusted R-squared:  0.738 \nF-statistic:  704 on 4 and 995 DF,  p-value: <2e-16\n\n\nFrom the summary, we can see that including the square term significantly improves the model fit.Check for yourself the metrics \\(R^2\\) and \\(AIC\\) and plot the histogram of residuals.\nWe can take from it, that it is extremely important to rely on theoretical considerations when building models."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#assumptions",
    "href": "content/fundamentals/02_reg.html#assumptions",
    "title": "Regression and Statistical Inference",
    "section": "Assumptions",
    "text": "Assumptions\n\nLinearity: Relationship between \\(X\\) and \\(Y\\) is linear.\nHomoscedasticity: Variance of residual is the same for any value of \\(X\\).\nIndependence: Observations are independent of each other. Residuals are independent of each other.\nNormality: For any fixed value of \\(X\\), \\(Y\\) is normally distributed. Residuals of the model are normally distributed."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html",
    "href": "content/fundamentals/01_a_prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Before we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part.\n\n\n\n\n\n\nAssignments\n\n\n\nPlease note: in this chapter, there are two assignments in between. In the other chapters, you’ll usually find the assignments at the end."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "href": "content/fundamentals/01_a_prob.html#basic-rules-of-probability",
    "title": "Probability Theory",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nConsider the most simple example: flipping coins. We define the flip of a coin as a random variable as we don’t know the outcome. To express our uncertainty, we make us of probability theory.\nFlipping the coin, we will see how the coin has landed and our random variable can take on of the two possible events \\(\\{H, T\\} \\subseteq \\Omega\\). It will be either Head or Tail.\nSo we have already defined two terms: random variable and events. Now what is a probability? A probability is always linked to an event typically denoted by a capital letter, here either \\(H\\) and \\(T\\), and expresses how likely this event is to happen. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\\[\nP(H) = P(T) = 0.5\n\\]\nExtreme cases: If an event \\(A\\) is impossible, its probability is \\(P(A) = 0\\) and if it is certain to occur, it is \\(P(A)=1)\\).\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 1: Probability is a real number greater or equal to 0.\n\n\nWe can also introduce the compliment \\(\\overline{A}\\), which is what happens when \\(A\\) does not happen and consequently, \\(P(A) + P(\\overline{A}) = 1\\). \\(A\\) and \\(\\overline{A}\\) are mutually exclusive, by definition. But there could also be two events \\(A\\) and \\(B\\) that are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\\[\nP(H \\cup T) = P(H) + P(T) = 1\n\\]\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 2: Total probability is equal to 1.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 3: Probability of mutually exclusive events is the sum of the probabilities.\n\n\nTo understand what not mutually exclusive events are, consider events \\(studying\\) and \\(working\\). For a random person, we don’t know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\nThen, the probability of both events happening is calculated by\n\\[\nP(A \\cup B) = P(A) + P(B) + P(A \\cap B)\n\\]\nwith \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the addition rule.\n\n\n\n\nFor mutually exclusive events:\n\\[\nP(A \\cup B) = P(A) + P(B) + P(A \\cap B) = P(A) + P(B)\n\\]\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule,\n\\[\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n\\]\nwhere \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened. It is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\nUsing the example with workers and students: without knowing exact numbers, we can almost safely assume that students are less likely to work than individuals who are not studying.\n\\[\nP(working|studying) < P(working|\\overline{studying})\n\\]\nEssentially, we are looking at probabilities restricted to a subset of the sample.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways:\n\\[\nP(A \\mid B) = P(A)\n\\]\nA typical example of independence is to roll a die twice. The second roll does not depend on the first one and each outcome is as likely independent of the first roll. This also results in\n\\[\nP(A \\cap B) = P(A) \\ P(B)\n\\]"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#probability-tree",
    "href": "content/fundamentals/01_a_prob.html#probability-tree",
    "title": "Probability Theory",
    "section": "Probability Tree",
    "text": "Probability Tree\nAn intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events has to happen. The probability of two consecutive events is obtained by multiplying the probabilities.\nConsider the following example: you are project manager and based on your are interested in th probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Based on your historical data about past projects, you come up with the following tree.\n\n\nProbability tree"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#assignment-1",
    "href": "content/fundamentals/01_a_prob.html#assignment-1",
    "title": "Probability Theory",
    "section": "Assignment 1",
    "text": "Assignment 1\nDefine being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\). Then, compute the following probabilities:\n\n\\(P(T \\cap S)\\)\n\\(P(T \\cap \\overline{S})\\)\n\\(P(\\overline{T} \\cap S)\\)\n\\(P(\\overline{T} \\cap \\overline{S})\\)\n\nWhat is the sum of all four probabilities?\nHint: Check here, if you are not sure what is shown in the probability tree.\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to sucessfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions."
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#set-theory",
    "href": "content/fundamentals/01_a_prob.html#set-theory",
    "title": "Probability Theory",
    "section": "Set Theory",
    "text": "Set Theory\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams building on set theory. A very simple one we actually already used above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\nLet’s use an example to understand the other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices, smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current distribution.\nWe simulate the collection process here. If you are interested how to do it in R, check out the code. But you don’t have to. And don’t worry, if it looks too complicated at this point, just move on.\n\n\n\n\n\n\nNote\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R.\n\n\n\n\n\n\n\n\nNote\n\n\n\ntibble() is the most convenient way to create tablets. You specify column name and content and assign your tibble to an object to store it.\n\n\n\n\n\n\n\n\nNote\n\n\n\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\n\n\n\n\n\n\n\n\nNote\n\n\n\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmutate() is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %>% mutate(new_variable = existing_var / 100).\n\n\n\nCode# Load tidyverse package\nlibrary(tidyverse)\n\n# Nmber of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  rowwise() %>% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer.\n\n\n\n\n\n\nNote\n\n\n\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\n\n\n# Show first ten lines\nhead(app_usage, 10)\n\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n\n\n\n\n\nNote\n\n\n\nSumming all values by column is done by colSums(table). For rows, you would use rowSums(table).\n\n\n\n# Show column sums\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n\n\nSumming the \\(user\\_id\\) does not make any sense. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n\n\n\n\n\nNote\n\n\n\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\n\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n\nsmartphone     tablet   computer \n       589        389        226 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n\nGeneric Venn diagram with three sets\n\n\n\n\n\n\n\n\nNote\n\n\n\nwhich() checks a condition and returns the indices.\n\n\n\n# Set of phone, tablet and computer users\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all <- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\nUsing the Venn diagram, we are able to answer questions like the following:\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\nTry to answer these questions yourself.\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\n0.5%\n7.3% + 3.3% + 8.8% + 0.5%\n10% + 42.3% + 27.8%\n\n\n\n\nWe can also use the example to go through the basic probability rules defined above.\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - P(T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)"
  },
  {
    "objectID": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "href": "content/fundamentals/01_a_prob.html#bayes-theorem",
    "title": "Probability Theory",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMath\nA very important theorem in probability theory is Bayes theorem that we get by reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nUsing the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem expresses a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before.\n\n\n\n\n\n\nTip\n\n\n\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]\n\n\nApplication\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) and consequently \\(P(\\overline{B}|A) = 0.03\\)\n\\(P(B|\\overline{A}) = 0.01\\) and consequently \\(P(\\overline{B}|\\overline{A}) = 0.99\\)\n\\(P(A) = 0.04\\) and consequently \\(P(\\overline{A}) = 0.96\\)\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B)\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nAssignment 2\nCompute\n\n\n\\(P(\\overline{A}|B)\\) (1)\n\n\n\n\\(P(A|B)\\) (2)\n\nand fill the gaps in the following sentence:\nThese results show that in case the alarm is triggered, there is a possibility of about __% that the product is flawless and a probability of __% that the product is faulty.\n\n\n\n\n\n\nHow to submit your solutions!\n\n\n\nPlease see here how you have to sucessfully submit your solutions. I would recommend you to solve the assignments first in .R scripts and in the end convert them to the required format as explained in the submission instructions."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Content",
    "section": "",
    "text": "Welcome to “Causal Data Science for Business Analytics”!\nIn this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nAccording to a large global survey1 conducted among data science practitioners in industry in 2020, 83% of respondents consider causal inference increasingly important for data-driven decision making. Moreover, 44% state that, in their data science projects, causal inference already is important."
  },
  {
    "objectID": "content/index.html#mattermost",
    "href": "content/index.html#mattermost",
    "title": "Content",
    "section": "Mattermost",
    "text": "Mattermost\nIn the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you have any questions about the class content, coding problems and other challenges, please use our Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\nFollow these steps to join the channel:\n\nGo on https://communicating.tuhh.de/\nClick Click here to sign in\nClick the Button GitLab\nYou may need to login to GitLab with your Kerberos/LDAP data (e.g. cba1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\nAfter accessing Mattermost, join the team W-11 students\nJoin Causal Data Science Channel (you might need to wait a bit, as I first have to add you)\n\nThere, and in the classes of course, I will try to help you as much as possible.\nIn order to keep the discussion efficient and manageable it is necessary that we all follow some basic rules:\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\n\n \n```r\nx %>% sum()\n```\n\n\n\n**Error:**\nError in x %>% sum() : could not find function \"%>%\"\n\n\n\nMinimalistic example of how you could post an error in Mattermost. See how little formatting makes the code very nice to read."
  },
  {
    "objectID": "content/index.html#installing-r-rstudio-ide",
    "href": "content/index.html#installing-r-rstudio-ide",
    "title": "Content",
    "section": "Installing R & RStudio IDE",
    "text": "Installing R & RStudio IDE\nBefore we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -> Global options -> Appearance and switch theme in -> Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab."
  },
  {
    "objectID": "content/index.html#introduction-to-r",
    "href": "content/index.html#introduction-to-r",
    "title": "Content",
    "section": "Introduction to R",
    "text": "Introduction to R\nOne last note before you start coding: don’t be intimidated by the errors, warnings and other messages that you (and everyone else) will without doubt receive. There is no reason to panic just because you see red text in your console and in fact, what is returned will often times already help you to solve the problem.\nThere are three different types of texts:\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\nMessages: these are just friendly texts that provide you with useful information.\n\nInteractive Tutorials:\nBut let’s no more talk about it but instead start coding because the best way to get familiar with R and to code is to just start.\nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console.\nFeel free to complete as many tutorials as you want, but for this class, the following tutorial is of particular use: The R Programming Environment (Chapter 2-12)\nswirl()does not come with R by default but is an optional package. R packages are extensions of the base functionality implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nInstall the package (one). As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nLoad the package (always). By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later.\n\nswirl()\n\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")\n\nYou don’t need to submit anything from this step. Just focus on getting familiar with R by completing the tutorial!"
  },
  {
    "objectID": "content/toolbox/05_rct.html",
    "href": "content/toolbox/05_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Let’s recall the fundamental problem of causal inference: we are not able to observe individual treatment effects. Only one potential outcome can be observed because there is only one state of the world.\nArguably, the most promising way to deal with this problem is randomization of the observation units and in particular randomized experiments, also known as randomized controlled trials (RCTs). Due to their statistical rigor and simplicity, RCTs are called the gold standard of causal inference.\nRCTs do not solve the fundamental problem of only observing one potential outcome but instead treatment and control group are randomized such that both groups are expected to be almost equal. Having similar groups that either received or not received treatment, we can calculate a valid causal estimate, the Average Treatment Effect (ATE). But it is only due to the randomization of observation units (e.g. individuals) that we are able to interpret it causally. If you remember the example with parking spots from a previous chapter, the computed \\(ATE\\) can also return invalid estimates."
  },
  {
    "objectID": "content/toolbox/05_rct.html#identification",
    "href": "content/toolbox/05_rct.html#identification",
    "title": "Randomized Controlled Trials",
    "section": "Identification",
    "text": "Identification\nTwo assumptions are crucial for the ATE to be interpreted causally.\nIndependence assumption\nWe have to assume independence between the potential outcomes and the treatment assignment, i.e. treatment assignment to a unit hast nothing to do with the size of treatment effect for a unit. In other words, it is not only those in the treatment group who benefit the most or the least from the treatment.\n\\[\nD_i \\perp (Y_{i0}, Y_{i1})\n\\]\nThis is where we exploit randomization. We can actually ensure that there is no association between potential outcomes and treatment by randomly assigning observation units to control and treatment group.\nThis way, both groups will be very similar on average, both in observed and in unobserved characteristics. They will only differ in their treatment status and possibly in the observed outcome, which makes the estimation of a causal effect possible.\nPlease make sure, that you understand the formula correctly. It does not mean that there is no treatment effect. The (potential) outcome under \\(D=0\\) or \\(D=1\\) is not affected by whether a particular observation unit does or does not receive the treatment. However, the observed outcome \\(Y_i\\) might depend on \\(D_i\\), and in fact, that is the effect we are interested in.\nA related way to express it is\n\\[\nE[Y_0|D=0] = E[Y_0|D=1]\n\\]\nRegardless of the treatment value a unit receives, the expected (but not always observed) potential outcome is the same in both treatment (\\(D=1\\)) and control group (\\(D=0\\)). The mean potential outcome is equal for both groups.\nThis does also imply equality of ATE and ATT, as there is no bias and the association we see is equal to the causation. Remember, we saw that \\(ATE \\neq ATT\\) when there is selection bias, i.e. observation units chose to be treated or not to be treated. But in case of randomization, by definition, selection bias cannot occur.\nWhen is the independence assumption violated?\nAn example, where the independence between treatment and potential outcomes is not given is if the treatment assignment is not randomized but people are able to self-select into on of the groups.\nThen, it could happen that for e.g. more motivated people would choose the treatment and when motivation had an impact on the potential outcome, e.g. more motivated people have a higher outcome for both potential outcomes compared to less motivated people, that are more likely to be in the control group. Under these circumstances, the independence assumption would be violated.\nSUTVA\nThe second assumption that needs to be fulfilled is the stable unit treatment value assumption (SUTVA).\nIt ensures that there is no interference between units. In other words, one unit’s treatment does not affect outcomes of other units. If unit \\(i\\) received a treatment, than this treatment of unit \\(i\\) should have no effect on another unit.\nImplicitly, the assumption states that there are only two potential outcomes for each unit and they only depend on a unit’s own treatment status.\nWhen is the SUTVA violated?\nIn situations where observation units are somehow clustered like e.g. in classrooms, departments or other kind of groups, violations of SUTVA can occur.\nAs an example, imagine you are running a company and select a few of your employees to participate in a program that teaches them about safety measures. After the program, it is very likely that they share some of the program content with their colleagues in their department, who might not have been selected for participation. Then, there are spillover effects.\nTo deal with violations of SUTVA you could change your selection process or change the level of analysis (analyzing clusters instead of individuals)."
  },
  {
    "objectID": "content/toolbox/05_rct.html#randomization",
    "href": "content/toolbox/05_rct.html#randomization",
    "title": "Randomized Controlled Trials",
    "section": "Randomization",
    "text": "Randomization\nIn practice, randomization is done automatically by software programs but to get an intuition, you could also think of it as e.g. flipping a coin for each observation unit or individual and assigning units that get head to the treatment group, while units that get tail are assigned to the control group (or the other way around).\nIn fact, that is already a special case because the probability of being treated and being not treated is 50% for both cases. But treatment probabilities could also take different values for a variety of reasons, for example because treatment is costly. However, you need to ensure that both groups are large enough to be comparable in order to fulfill the independence assumption.\nLet’s see what that means. We assume that we have a population of 10’000 individuals which we want to learn something about. Using runif() and rbinom(), we synthetically generate this population with random values for the characteristics \\(age\\) and \\(sex\\).\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Define population size\nn <- 1e+4 \n\n# Create population with two characteristics\nX <- tibble(\n  age = runif(n, 18, 65), # draw random values from uniform distribution\n  sex = rbinom(n, 1, 0.5) # draw random values from binomial distribution\n)\n\n# Show table\nX\n\n\n\n  \n\n\n\nUntil now, we have not assigned units to treatment and control group and actually, we do not want to assign our whole population to any group. As a matter of fact, in many applications, you are just able to draw a sample from the population and almost never the whole population.\nRemember, randomization of treatment should achieve that we are able to interpret the average treatment effect causally and for that, both groups need to be as similar as possible. The image illustrates the randomization process. Try to think what could happen if you have just very few units in both groups. How likely is is that they are very similar regarding their characteristics? You can probably already sense that this might not be sufficient to make groups comparable.\n\n\nIllustration of randomization process. Sample of population is randomly divided into treatment and control group.\n\n\nBut let’s try it out and see how average group characteristics develop when we change the sample size.\nIn R, it is very easy to generate a random vector that we can use for randomization. Here, we want to have a random vector that contains either 1 (treatment group) or 0 (control group) with a treatment probability of 50%. We can make use of the rbinom() function that can randomly generate outcomes of a Bernoulli trial, which you can just imagine as flipping the coin \\(n\\) times.\nAs we have 10’000 people in our population, we will vary sample sizes from 10 to 10’000 to understand the impact sample size has.\n\n\n\n\n\n\nNote\n\n\n\nTo perform the same step multiple times with just one variable changing (in this case: sample size), we can use for loops. First we have to define what to iterate through (a vector of sample sizes). Then, we initialize a few list that we store objects in that are created during an iteration:\n\navg_tbl_age_lst(): To store average age for specific sample size\navg_tbl_sex_lst(): To store average sex (proportion) for specific sample size\ntbl_sampled_lst(): To store actual table for sample size (for later use)\n\nTo store data from an iteration, we assign it to a list by accessing the list with two square brackets: lst[[\"name\"]] <- object_to_store.\nThere are several other ways to do it (even without loops). But for loops are an intuitive way and often times sufficient. You can read more about for loops here.\n\n\n\n# Define vector of sample sizes\nsss <- c(10, 50, 500, seq(1000, 1e+4, 500))\n\n# Create empty lists to store average tables in\navg_tbl_age_lst <- list()\navg_tbl_sex_lst <- list()\ntbl_sampled_lst <- list()\n\n# Iterate through sample sizes\n# ... for sample size in sample sizes ...\nfor (ss in sss) {\n  # sample from population\n  X_sampled <- sample_n(X, ss)\n  \n  # perform random assignment\n  D <- rbinom(ss, 1, 0.5)\n  \n  # combine characteristics and assignment in one table\n  tbl_sampled <- X_sampled %>% mutate(treatment = D)\n  \n  # store in list\n  tbl_sampled_lst[[paste(ss)]] <- tbl_sampled\n  \n  # get average characteristics ...\n  # ... for age\n  avg_tbl_age <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_age = mean(age)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"age\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_age) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_age_lst[[paste(ss)]] <- avg_tbl_age\n  \n  # ... for sex\n  avg_tbl_sex <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_sex = mean(sex)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"sex\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_sex) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_sex_lst[[paste(ss)]] <- avg_tbl_sex\n}\n\nAs you can see in the plot below, group average characteristics converge with increasing sample size. The more units are assigned to either group, the less differences are between the groups and thus, the independence assumption, stating that groups only differ by their treatment status, is fulfilled. But although you need a minimum amount of units, there is not much improvement after increasing the sample size way beyond that (also, the difference is already really small).\n\n# Combine tables to one larger table\navg_age <- avg_tbl_age_lst %>% bind_rows()\navg_sex <- avg_tbl_sex_lst %>% bind_rows()\navgs_tbl <- avg_age %>% bind_rows(avg_sex)\n\n# Plot convergence\nggplot(avgs_tbl, aes(x = sample_size, y = delta_abs)) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(x = \"Sample size\", y = \"Absolute difference\") +\n  ggtitle(\"Absolute difference of characteristics\\n between groups by sample size\")\n\n\n\nWith increasing sample size, differences between treatment and control group get smaller or are already so small, that they can be ignored."
  },
  {
    "objectID": "content/toolbox/05_rct.html#average-treatment-effect",
    "href": "content/toolbox/05_rct.html#average-treatment-effect",
    "title": "Randomized Controlled Trials",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nJust because we can, let’s just use all of them. That means there should be about ~5’000 units per group. There are many suggested rules and guidelines to choose the right sample size, but for now, we will disregard it as our data is simulated and therefore, we do not have any data problems.\nSo far, we have just looked at the covariate balance but have not included the outcome variable. Let’s do that now. In the background we simulated the outcome after treatment and added the column outcome to our table.\n\n# Show data with outcome variable\ndf_out\n\n\n\n  \n\n\n\nAs already mentioned, having balanced baseline characteristics between treatment and control group allows us to estimate the average treatment effect.\nBut how do we calculate the average treatment effect? We can just take a simple difference in means to estimate it. By the way, groups can be of different group size. It is only important, that they are comparable in their characteristics.\nLet’s compute the average outcome per group. We see that there seems to be a difference, the average outcome in the treatment group is higher.\n\n# Group by treatment group and compute average outcome\ndf_out %>% \n  group_by(treatment) %>% \n  summarise(mean_outcome = mean(outcome))\n\n\n\n  \n\n\n\nGenerally, it is recommendable to use a linear regression to get an estimate of the treatment effect. You don’t have to manually compute the difference and additionally, the output of lm() and summary() yields information to be used for statistical inference. Then, we see that this effect is in fact highly statistically significant. The effect is equal to the difference of the two values just seen above. Check it out (small differences possible due to rounding)!\n\n# Compute ATE with linear regression\nlm_ate <- lm(outcome ~ treatment, data = df_out)\nsummary(lm_ate)\n\n\nCall:\nlm(formula = outcome ~ treatment, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.317  -2.064  -0.009   2.094  14.981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.9525     0.0432    45.2   <2e-16 ***\ntreatment     1.1589     0.0607    19.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3 on 9998 degrees of freedom\nMultiple R-squared:  0.0351,    Adjusted R-squared:  0.035 \nF-statistic:  364 on 1 and 9998 DF,  p-value: <2e-16\n\n\nOne way to present your results to your audience could be a boxplot that on the one hand shows the difference of regressors by group and on the other hand the difference of outcomes. Here we will show the 95% confidence intervals for our estimates and it can be seen that there is a substantial difference between both groups. However, for our independent variables, age and sex , both groups are very similar.\n\n# Plot independent and and depdent difference\n# age (independent)\ncompare_age <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = age, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Age\", title = \"Difference in age\")\n\n# sex (independent)\ncompare_sex <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = sex, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Sex\", title = \"Difference in sex\")\n\n# outcome (dependent)\ncompare_outcome <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = outcome, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n\n# Plot age, sex and outcome differences for both groups\ncompare_age\ncompare_sex\ncompare_outcome\n\n\n\n\nTreatment and control group are similar regarding their characteristics but for the outcome, they are different. Due to the similarity in characteristics, we can ascribe this difference to the treatment.\n\n\n\n\nTreatment and control group are similar regarding their characteristics but for the outcome, they are different. Due to the similarity in characteristics, we can ascribe this difference to the treatment.\n\n\n\n\nTreatment and control group are similar regarding their characteristics but for the outcome, they are different. Due to the similarity in characteristics, we can ascribe this difference to the treatment.\n\n\n\n\n\nBut why did we not include \\(age\\) and \\(sex\\) into our regression? Because they are similarly distributed across both groups it should not change the treatment effect. But still, they might have an impact on the outcome, as well. Although being similarly distributed in both groups, they can still vary within each group. So let’s see what happens if we include them.\nBoth regressors turn out to be significant. However, as expected, the treatment effect is almost unchanged.\n\n# Include other regressors\nlm_all <- lm(outcome ~ treatment + age + sex, data = df_out)\nsummary(lm_all)\n\n\nCall:\nlm(formula = outcome ~ treatment + age + sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.590  -2.059   0.015   2.056  15.298 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.28786    0.10636   12.11  < 2e-16 ***\ntreatment    1.14359    0.06046   18.92  < 2e-16 ***\nage          0.00944    0.00224    4.22  2.5e-05 ***\nsex          0.55944    0.06046    9.25  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3 on 9996 degrees of freedom\nMultiple R-squared:  0.0451,    Adjusted R-squared:  0.0448 \nF-statistic:  157 on 3 and 9996 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/05_rct.html#subgroup-analysis",
    "href": "content/toolbox/05_rct.html#subgroup-analysis",
    "title": "Randomized Controlled Trials",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nThe significance of \\(age\\) and \\(sex\\) could also indicate that there are different treatment effects across different levels of both covariates. Then, a so called interaction/moderation effect would be covered behind the statistical coefficients.\nA moderation effect expresses different strengths of the treatment for different subgroups. For example older women might benefit relatively more from the treatment and younger males relatively less.\nIn R, we include interaction effects by using either using a product x1*x2 or a colon x1:x2. In fact, when we do that, the interactions are significant and the treatment effect changes substantially.\nNote, that now the treatment effect differs for everyone dependent on their age and sex. Now it requires a bit of addition to obtain the treatment effect. Moreover, it is not the \\(ATE\\) anymore, but instead the conditional average treatment effect \\(CATE\\) as it depends on other covariates.\n\n# Include interaction\nlm_mod <- lm(outcome ~ treatment * age + treatment * sex, data = df_out)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = outcome ~ treatment * age + treatment * sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.354  -2.046  -0.001   2.065  15.035 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.77909    0.14589   12.19  < 2e-16 ***\ntreatment      0.17574    0.20401    0.86   0.3890    \nage            0.00322    0.00321    1.00   0.3153    \nsex            0.08105    0.08574    0.95   0.3445    \ntreatment:age  0.01193    0.00447    2.67   0.0075 ** \ntreatment:sex  0.94445    0.12052    7.84  5.1e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3 on 9994 degrees of freedom\nMultiple R-squared:  0.0517,    Adjusted R-squared:  0.0512 \nF-statistic:  109 on 5 and 9994 DF,  p-value: <2e-16\n\n\nNote also that \\(R^2\\) has increased with each addition to the regression. But why do you think the main treatment effect is not significant anymore?\nAs the data is simulated, we can check what the true data-generating process is and based on that evaluate what regression equation provides the best solution. Check for yourself what model should be used.\n\\[\noutcome = 0.2*treatment + 0.01*treatment*age + treatment*sex + \\epsilon\n\\]\nAgain, it shows how crucial theoretical knowledge of the phenomenon you are studying is. Imagine a situation with a high number of regressors. Testing out all potential variables as moderators requires some effort and might even lead to results just due to chance. You should therefore plan your research design and hypotheses beforehand. As a matter of fact, many scientific publications therefore have to define a pre-analysis plan."
  },
  {
    "objectID": "content/toolbox/09_rdd.html",
    "href": "content/toolbox/09_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "The next tool we introduce is called regression discontinuity design (RDD). Fist used in 1960 (Campbell), it did not play a large role until 1999 and since then has experienced growing acceptance due to the advance of more rigorous requirements regarding credibility and causality in social sciences. Another factor that made many researchers use RDDs is the increased availability of digitized administrative data that is often linked to arbitrary rules that can be exploited to capture “as-if” randomization processes for treatment assignment.\nAssuming a data-generating process, where we have a variable \\(X\\) that is a confounder as it has an impact on treatment assignment \\(D\\) and the outcome \\(Y\\). Additionally, we could have an unobserved confounders between \\(X\\) and \\(Y\\).\nThen, as can be seen in the second DAG, the regression discontinuity design exploits the fact that \\(X\\) determines \\(D\\) and data is filtered such that there are only observations that were close to a cut-off value determining their treatment status. This way, treated and untreated units are very similar and comparable and RDD is able to eliminate selection bias for that sub-population. Note that the treatment effect you calculate using this method is an average treatment effect for a subgroup rather than for the whole population. Like with IV, it is the local average treatment effect (\\(LATE\\)).\n\\(X\\) is called the running variable and is a continuous variable assigning units to treatment \\(D\\) based on a cut-off score \\(c_0\\). Because it has an impact on \\(Y\\) as well, it is a confounder and opens a backdoor path. Now, the problem is that due to the cut-off determining the treatment \\(D\\), the backdoor cannot be closed with regular ways like e.g. matching as there is no overlap, i.e. there are no treated and untreated units for all levels of \\(X\\).\n\n\n\n\n\n\nImportant\n\n\n\n\nRunning/forcing variable: variable that determines or strongly influences treatment assignment.\nCut-off/threshold: rule-based value that divides units into treatment an control group.\n\n\n\n\n\n\n\nRelationships when using full sample.\n\n\n\n\nRelationships when restricting sample to units close to cut-off.\n\n\n\n\nTherefore, as the second graph shows, the causal effect is identified by analyzing only observations that are in close neighborhood to \\(c_0\\). The identified treatment effect is the local average treatment effect (LATE).\n\\[\nLATE_{RDD} = E[Y_1 - Y_0| X \\rightarrow c_0]\n\\]"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#identification-strategy",
    "href": "content/toolbox/09_rdd.html#identification-strategy",
    "title": "Regression Discontinuity",
    "section": "Identification Strategy",
    "text": "Identification Strategy\nRDDs are quite intuitive and very graphical. For this reason, we will go through estimation and inference using an application and explain at each step what has to be considered.\nOur application is as follows: we are interested how sending a coupon to customers incentivizes their purchase behavior. We could run a randomized controlled trial and only send a coupon to a random selection. However, an already conducted campaign can actually be exploited. Last year 5€ coupons were sent to customers who had not bought within the previous 60 days.\nThat is an arbitrary cut-off rule that we have discussed in the introduction. We can exploit it because we have customers around the cut-off that should be very similar. However, we could not convincingly argue that customers that had bought within the last few days are similar to those who have bought for example more than three months before.\nThus, our cut-off value is \\(c0 = 60\\).\nLet’s have a look at what the data looks like. We know when the customers has bought the last time and based on that received a coupon or did not receive anything and we know the purchases he/she made after the coupon distribution.\n\n# Read data. You probably have to add the correct path\ndf <- readRDS(\"coupon.rds\")\n\n\n# Show data\ndf\n\n\n\n  \n\n\n\nIllustrating the relationships in a DAG, we can see that for our running variable days_since_last being close to the cut-off value, we can estimate a local average treatment effect.\n\n# Running variable does not affect purchases itself as we only\n# estimate a local average treatment effect. Also, there is no non-random\n# heaping at cut-off.\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Directed Acyclic Graph\n# Define\nrdd <- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ X,\n  exposure = \"D\",\n  outcome = \"Y\",\n  latent = \"U\",\n  coords = list(x = c(Y = 1, D = 0, X = 0, U = 1),\n                y = c(Y = 0, D = 0, X = 1, U = 1)),\n  labels = list(X = \"Days since last\\naround cut-off value\",\n                Y = \"Purchases after\",\n                U = \"Unobserved\",\n                D = \"Coupon\")\n)\n\n# Plot\nggdag_status(rdd, text = T, use_labels = \"label\") +\n  theme_dag_cds() +\n  guides(color = \"none\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\nAt first glance, it looks like all restaurants below the cut-off don’t have a “very good” label (indicated by FALSE) and the restaurants above cut-off do have it. We can visualize if that applies to all restaurants. As we have expected, to the left of the cut-off and to the right of the cut-off, there is always just one label type. It means, we are dealing with a sharp cut-off.\nBecause the coupons were assigned by a computer, we deal with a sharp cut-off. This means every customer with no purchase history within the last 60 days did receive the coupon and every customer with purchases made within this period did not receive it.\n\n# [2] Visualization ----\n# [2.1] Compliance ----\n# As expected, perfect \"compliance\" and sharp cutoff. All \n# customers below the cutoff get no coupon, while all customers above\n# the cutoff get a coupon.\ncompl <- \n  ggplot(df, aes(x = days_since_last, y = coupon, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.2, position = position_jitter()) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"No coupon\", \"Coupon\"))+\n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\")\ncompl\n\n\n\nSharp cut-off. All eligible customer were sent an coupon and not eligibles did not receive it."
  },
  {
    "objectID": "content/toolbox/09_rdd.html#random-assignment",
    "href": "content/toolbox/09_rdd.html#random-assignment",
    "title": "Regression Discontinuity",
    "section": "Random assignment",
    "text": "Random assignment\nAs already mentioned, for RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is no decline or incline at the cut-off and therefore can assume that the continuity assumption holds.\n\n# [2.2] Random assignment test ----\n# identifying assumption: random assignment to either side of cut-off\n# Manual plot\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", boundary = c0, alpha = .6) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\nNumber of customers for given days since last purchase. As there are many customers who buy regularly and also a number of one-time purchasers, the distribution has such a decreasing curve.\n\n\n\n\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package. It relies on a test to check the assumption. as the p-value is large, we can reject the null hypothesis that the number of units at either side are different.\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd <- rddensity(df$days_since_last, c = c0)\nsummary(rddd)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       5000\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 60                Left of c           Right of c          \nNumber of obs         3854                1146                \nEff. Number of obs    1486                734                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           31.203              31.915              \n\nMethod                T                   P > |T|             \nRobust                1.1559              0.2477              \n\n\nWarning in summary.CJMrddensity(rddd): There are repeated observations. Point estimates and standard errors have been adjusted. Use option\nmassPoints=FALSE to suppress this feature.\n\n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          <c     >=c    P>|T|\n0.300                       8      14    0.2863\n0.600                      20      20    1.0000\n0.900                      25      29    0.6835\n1.200                      33      36    0.8099\n1.500                      41      45    0.7465\n1.800                      48      55    0.5546\n2.100                      70      68    0.9322\n2.400                      84      83    1.0000\n2.700                      95      93    0.9419\n3.000                     107     103    0.8361\n\n\nIt can also be shown graphically, where you can see that the confidence intervals overlap. If they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to get valid results.\n\n# Visually check continuity at running variable\nrdd_plot <- rdplotdensity(rddd, df$days_since_last, plotN = 100)"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#visualization",
    "href": "content/toolbox/09_rdd.html#visualization",
    "title": "Regression Discontinuity",
    "section": "Visualization",
    "text": "Visualization\nHaving checked potential violations of the continuity assumptions, we can move on and estimate the treatment effect. We start with selecting a bandwidth, i.e. we select what and how many observations should be compared. The larger the bandwidth, the more observations are taken into consideration but it also reduces the comparability. On the other hand, choosing a small bandwidth results in a lower number of observations while ensuring similarity and comparability.\nThere is no safe rule how to best select the bandwidth, although there are algorithms attempting to look for the optimal bandwidth. For now, we just use common sense and select a bandwidth of 5 days, resulting in an analysis window \\([55, 65]\\), which still leaves us with about 300 observations.\n\n# [3] Dependent variable ----\n# [3.1] Average Treatment Effect ----\n# Plot regression lines for full and specified bandwidth.\n# Specify bandwidth\nbw <- c0 + c(-5, 5)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(days_since_last %>% between(bw[1], c0))\ndf_bw_above <- df %>% filter(days_since_last %>% between(c0, bw[2]))\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n[1] 326   4\n\n\nTo illustrate the difference between using only a small window and all data, we plot the resulting regression lines. You can see that both approaches would lead to different results.\n\n# Plot dependent variable vs running variable\ndep_var <-\n  ggplot(df, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.1, size = 0.2) +\n  # add lines for the full range\n  geom_smooth(data = filter(df, days_since_last <= c0), \n              method = \"lm\", se = F, linewidth = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(df, days_since_last > c0), \n              method = \"lm\", se = F, linewidth = 1, linetype = \"dashed\") +\n  # add lines for specified bandwidth\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, \n              color = ggthemr::swatch()[7], linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, \n              color = ggthemr::swatch()[7], linewidth = 2) +\n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var\n\n\n\nVisual comparison of purchases after coupon allocation.\n\n\n\n\nFrom the plot above, it is hard to see what the difference between observations close to the cut-off is. So what we can do is to compute to regressions, one for the observations in \\([55, 65)\\) and another one for the observations in \\([55, 65]\\).\nThen, using the resulting coefficients, we compute what both models predict for the cut-off value \\(c0\\) and take the difference. The difference is the local average treatment effect (LATE).\n\n# [3.2] Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above <- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 <- predict(model_bw_below, tibble(days_since_last = c0))\ny1 <- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate <- y1 - y0\nlate\n\n1 \n8 \n\n\nIt’s a bit messy when we plot all observations, so let’s zoom in to see if we can detect the local average treatment effect graphically. Not surprisingly, it is equal to what we have just computed.\n\n# Minimum and maximum for y-axis limits\nmin_y <- min(df_bw$purchase_after)\nmax_y <- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw <- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5], size = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dashed\", color = ggthemr::swatch()[4]) +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dashed\", color = ggthemr::swatch()[4]) +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           color = ggthemr::swatch()[4], fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ndep_var_bw\n\n\n\nTreatment effect for customers within +- 5 days."
  },
  {
    "objectID": "content/toolbox/09_rdd.html#estimation",
    "href": "content/toolbox/09_rdd.html#estimation",
    "title": "Regression Discontinuity",
    "section": "Estimation",
    "text": "Estimation\nWhat you will see most in studies is a regression to compute the LATE. Here, we prefer to use days_since_last_centered, which is the raw days_since_last variable centered, i.e. subtracted by the cut-off value \\(c0\\). That simplifies the interpretation, however, it does not change the coefficient of interest, the LATE.\nThe coefficient we are most interested in is the one for couponTRUE. It is equal to the effect in the plot above, but the regression summary also yields additional statistical information. We see that the LATE is statistically significant.\n\n# [4] Estimation ----\n# [4.1] Parametric ----\n# Compute coefficients for specified bandwidth.\nlm_bw <- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n\n\nCall:\nlm(formula = purchase_after ~ days_since_last_centered + coupon, \n    data = df_bw)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.128 -2.134 -0.121  2.105 10.283 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                11.075      0.366   30.29   <2e-16 ***\ndays_since_last_centered    0.254      0.119    2.14    0.033 *  \ncouponTRUE                  8.779      0.667   13.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3 on 323 degrees of freedom\nMultiple R-squared:  0.732, Adjusted R-squared:  0.73 \nF-statistic:  441 on 2 and 323 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/06_match.html",
    "href": "content/toolbox/06_match.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Almost always, the problem we are trying to solve in causal inference relates to the fundamental problem of causal inference, the fact that we cannot observe two states for one particular observation unit, e.g. we cannot see how a person’s health status changes after taking a specific drug and after not taking the same drug. Consequently, we cannot know the individual treatment effect.\nThus, we said we can make use of averages and try to estimate the average treatment effect by taking the difference between a group of treated observation units and a group of untreated observation units. Written in a formula, we can present it as\n\\[\nATE = E[Y_1| D = 1] - E[Y_0 | D = 0]\n\\]\nAssuming that individuals (or other kinds of observation units) were randomly assigned for example to take the drug, then this result gives us exactly what we want. It compares the outcomes of groups under treatment with groups not under treatment (control).\nBut the estimate hinges on the assumption that both groups are comparable, formally\n\\[\nE[Y_0|D=0] = E[Y_1|D=0]\n\\]\nwhich we cannot test but in randomized settings we have good reason to believe it to be true. Under these circumstances, average treatment effect (\\(ATE\\)) and the average treatment effect on the treated (\\(ATT\\)) are equal.\nHowever, if there are underlying group differences because the treatment assignment was not randomized and e.g. individuals were able to choose their treatment, we are not measuring the estimate that we are interested in. Then, the difference between \\(ATE\\) and \\(ATT\\) is what we call selection bias, an unmeasured factor representing systematic bias:\n\\[\nATE = ATT + \\text{selection bias}\n\\]\nGraphically, we can show an example of when the naive estimate would fail.\n\n\n\n\nEffect of D on Y is confounded by variation in Z. In other words, there are two paths from D to Y, one direct path and a backdoor path via Z.\n\n\n\n\nBy now, you might recognize what kind of problem the DAG depicts: confounding. A variable \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) and to estimate the causal effect of \\(X\\) on \\(Y\\), we need to close the backdoor path of \\(X\\) to \\(Y\\) via \\(Z\\)."
  },
  {
    "objectID": "content/toolbox/06_match.html#single-matching-variable",
    "href": "content/toolbox/06_match.html#single-matching-variable",
    "title": "Matching and Subclassification",
    "section": "Single Matching Variable",
    "text": "Single Matching Variable\nProbably the most simple method, though rarely applied in practice, is matching on a single variable. Its rare use in practice is due to the fact that matching on a single variable is only applicable if there is only one backdoor path that can be closed by this matching variable.\nBut for ease of explanation, we’ll have a look at it.\n\n\n\n\nGeneric DAG with matching. By fixing the level of confounder, backdoor is closed and valid estimation of causal treatment effect for that specific group at level of confounder.\n\n\n\n\nApplied to credit card example. Effect of being late on settling credict card debt in the past on being late again. Confounded by size of credit card debt. It could be suspected that customers with higher debts in the past are unlikely to be financially recovered by now."
  },
  {
    "objectID": "content/toolbox/06_match.html#coarsened-exact-matching",
    "href": "content/toolbox/06_match.html#coarsened-exact-matching",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching",
    "text": "(Coarsened) Exact Matching\nWhen you think of the previous credit card example, two things could come up in your mind:\n\nHow likely is it that when fixing a continuous variable such as previous bill, we can find matching treatment and control observations?\nHow do we control for more than one variable?\n\nTo address the first issue we can coarsen continuous variables. That means, we create different levels/bins for the size of the previous bill, which could be for example: 1-50€, 51-250€, 251-1000€ and so on. This increases the likelihood of finding matches substantially. However, we also lose a bit of precision. In general: the wider the bins, the more matches but the less precision.\nIf there is more than one backdoor, we can also match on more than one variable. Matched observations need to correspond on all variables that are selected as matching variables.\nEspecially if matching on a single variable, there might be more than one match for a particular observation. In this case, you would weight the matches. On the other hand, especially if trying to find (coarsened) exact matches on multiple variables, some observations might not be matched at all. That’s a situation where nearest-neighbor matching comes in handy."
  },
  {
    "objectID": "content/toolbox/06_match.html#nearest-neighbor-matching",
    "href": "content/toolbox/06_match.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching",
    "text": "Nearest-Neighbor Matching\nIt is less strict than exact matching as it does not require observations to match exactly in all (coarsened) variables. Instead, the distance between observations is computed using distance measures such as the distance or the Euclidean distance.\n\n\n\n\nInitial situation: six potential matches.\n\n\n\n\nFour best matches are selected.\n\n\n\n\nMatches are only accepted if they fall into specified vicinity.\n\n\n\n\nFor two observations, a scalar is returned calculated from the distances of all matching variables. Based on that value, \\(k\\) matching observations are selected to build the control group.\nIf \\(k \\neq 1\\), you also have to decide whether to weight the control observations on their distance. Weighting the observations makes the approach less sensitive to the choice of \\(k\\) because less importance observations are down-weighted. Eventually the weights goes to zero."
  },
  {
    "objectID": "content/toolbox/06_match.html#propensity-score",
    "href": "content/toolbox/06_match.html#propensity-score",
    "title": "Matching and Subclassification",
    "section": "Propensity score",
    "text": "Propensity score\nWith an increasing number of matching variables nearest-neighbors matching becomes unfeasible, as well. To reduce the dimensionality onto one dimension, for each observation a propensity score can be computed. The propensity score expresses the estimated probability of treatment. In absence of selection bias, propensity scores should be very similar across treatment and control group. Thinking back to our probability chapter, the propensity score is:\n\\[\nP(D_i = 1|X_i)\n\\]\nWe’ll discuss how to use it later in detail but essentially, we exploit that there are units in the treatment group that were unlikely to be treated and vice versa. The most recommended method that uses propensity scores as matching input is called inverse probability weighting and weights each observation is weighted by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight."
  },
  {
    "objectID": "content/toolbox/06_match.html#multiple-matching-variables",
    "href": "content/toolbox/06_match.html#multiple-matching-variables",
    "title": "Matching and Subclassification",
    "section": "Multiple Matching Variables",
    "text": "Multiple Matching Variables\nLet us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.\nNow you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age. Let’s assume for demonstration purposes that these are the only confounding factors. In practice, there might be more, however.\nWe can use a DAG to think about the correct identification strategy. Using dagitty and ggdag we see that we need to close two backdoor paths: initial health status and age.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Define DAG\ndag_model <- 'dag {\n  bb=\"0,0,1,1\"\n  \"Health Program\" [exposure,pos=\"0.25,0.2\"]\n  \"Initial Health Status\" [pos=\"0.35,0.25\"]\n  \"Sick Days\" [outcome,pos=\"0.35,0.2\"]\n  Age [pos=\"0.25,0.25\"]\n  \"Initial Health Status\" -> \"Health Program\"\n  \"Initial Health Status\" -> \"Sick Days\"\n  Age -> \"Health Program\"\n  Age -> \"Initial Health Status\"\n  Age -> \"Sick Days\"\n}'\n\n# DAG with adjustment sets (and custom layout)\nggdag_adjustment_set(dag_model, shadow = T, use_labels = \"name\", text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag_cds() +\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\nDAG showing what needs to be accounted for.\n\n\n\n\nLet’s load the data (you probably have to change the path) and have a glance at it.\n\n# Read data\ndf <- readRDS(\"../../datasets/health_program.rds\")\n\n# Show data \ndf\n\n\n\n\n\n\n\nTrue Treatment Effect\n\n\n\nBecause the data is simulated, we know the true treatment effect: 0.5\nCompare it to the estimates in the following sections!\n\n\nA naive estimate would be obtained by just regressing sick_days on health_program.\n\n# Naive estimation (not accounting for backdoors)\nmodel_naive <- lm(sick_days ~ health_program, data = df)\nsummary(model_naive)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.492 -1.492 -0.262  0.738 19.508 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          7.2623     0.0326   222.8   <2e-16 ***\nhealth_programTRUE   1.2299     0.0459    26.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 9998 degrees of freedom\nMultiple R-squared:  0.067, Adjusted R-squared:  0.0669 \nF-statistic:  718 on 1 and 9998 DF,  p-value: <2e-16\n\n\nBut we suspect that treatment assignment was not random, so let’s see if we can improve the validity of our estimation using matching."
  },
  {
    "objectID": "content/toolbox/06_match.html#coarsened-exact-matching-1",
    "href": "content/toolbox/06_match.html#coarsened-exact-matching-1",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching",
    "text": "(Coarsened) Exact Matching\nAgain, in case of exact matching, only observations that share the same values in (coarsened) matching variables are matched in. To perform Coarsened Exact Matching (CEM) you can use the MatchIt package in R. If you do not specify how to coarsen the data, it will be done automatically based on an algorithm. Other than that, we provide a formula containing our treatment dependent on the matching variables, the data, what method to use ('cem' = Coarsened Exact Matching) and what estimate we are interested in.\n\n# Load 'MatchIt' library\nlibrary(MatchIt)\n\n# Without specifying coarsening\n# (1) Matching\ncem <- matchit(health_program ~ age + sick_days_before,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n\nUsing the summary() function we can check how well balanced the covariates are compared to before. In this case, they are almost perfectly balanced.\n\n# Covariate balance\nsummary(cem)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\")\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.5            0.20       0.95     0.056     0.09\nsick_days_before           4.2           3.5            0.36       1.32     0.035     0.18\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.5          45.5           0.001       1.00     0.002    0.007             0.1\nsick_days_before           3.8           3.8           0.012       0.99     0.002    0.020             0.1\n\nSample Sizes:\n              Control Treated\nAll              4957    5043\nMatched (ESS)    4657    4757\nMatched          4947    5017\nUnmatched          10      26\nDiscarded           0       0\n\n\nNow, we can use the matched data and see how the coefficient changes. Actually, it changes quite a lot. Even when at first glance, the covariates were not too different before matching.\n\n# Use matched data\ndf_cem <- match.data(cem)\n\n# (2) Estimation\nmodel_cem <- lm(sick_days ~ health_program, data = df_cem, weights = weights)\nsummary(model_cem)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_cem, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-6.368 -1.455 -0.100  0.934 15.875 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          7.6024     0.0319   238.2   <2e-16 ***\nhealth_programTRUE   0.4808     0.0450    10.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 9962 degrees of freedom\nMultiple R-squared:  0.0113,    Adjusted R-squared:  0.0112 \nF-statistic:  114 on 1 and 9962 DF,  p-value: <2e-16\n\n\nInstead of letting the algorithm decide how to coarsen the data, we can also provide custom cut-points. Let’s do that and check if we can create matched data as balanced as by the algorithm.\nWe’ll see that we are able to decrease the imbalance, but not to the same degree as the algorithm did it.\n\n# Custom coarsening\n# (1) Matching\ncutpoints <- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))\ncem_coars <- matchit(health_program ~ age + sick_days_before,\n                     data = df, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n\n# Covariate balance\nsummary(cem_coars)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.5            0.20       0.95     0.056     0.09\nsick_days_before           4.2           3.5            0.36       1.32     0.035     0.18\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.4           0.018        1.0     0.006    0.019            0.47\nsick_days_before           3.9           3.7           0.118        1.1     0.012    0.088            0.58\n\nSample Sizes:\n              Control Treated\nAll              4957    5043\nMatched (ESS)    4798    4861\nMatched          4957    5038\nUnmatched           0       5\nDiscarded           0       0\n\n\nWe can also visualize the subsamples and see how data points are weighted. Weights depend on how many treated and control units there are in a specific subsample. You can see that in the top-right corner, for example. From the plot we can also see that the cut-points are too broad as matches could be way closer.\n\n# Use matched data\ndf_cem_coars <- match.data(cem_coars)\n\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sick_days_before,\n                         size = weights, color = as.factor(health_program))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sick_days_before),\n              mapping = aes(intercept = y, slope = 0), \n              linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  geom_vline(data.frame(y = cutpoints$age),\n              mapping = aes(xintercept = y),\n             linewidth = 1.5, color = ggthemr::swatch()[5]) +\n  theme(legend.position = \"none\")\n\n\n\nCoarsening: each subsample should contain only similar units.\n\n\n\n\nWith custom coarsening, we again get another coefficient. It could indicate that this way, backdoors are not properly closed.\n\n# (2) Estimation\nmodel_cem_coars <- lm(sick_days ~ health_program, data = df_cem_coars, \n                      weights = weights)\nsummary(model_cem_coars)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_cem_coars, \n    weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-6.255 -1.389 -0.243  0.812 18.988 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          7.5189     0.0327   230.2   <2e-16 ***\nhealth_programTRUE   0.6935     0.0460    15.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 9993 degrees of freedom\nMultiple R-squared:  0.0222,    Adjusted R-squared:  0.0221 \nF-statistic:  227 on 1 and 9993 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/06_match.html#nearest-neighbor-matching-1",
    "href": "content/toolbox/06_match.html#nearest-neighbor-matching-1",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor matching",
    "text": "Nearest-Neighbor matching\nFor nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular distance measures used to find so called nearest neighbors is the Mahalanobis distance.\nAgain, we use MatchIt to conduct the matching process. We just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching. The result differs from (coarsened) exact matching but again, we have almost perfect balance.\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn <- matchit(health_program ~ age + sick_days_before,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"nearest\", distance = \"mahalanobis\", replace = T)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.5            0.20       0.95     0.056     0.09\nsick_days_before           4.2           3.5            0.33       1.32     0.035     0.18\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       46.4          46.4          -0.000          1     0.001    0.004           0.008\nsick_days_before           4.2           4.1           0.001          1     0.000    0.001           0.001\n\nSample Sizes:\n              Control Treated\nAll              4957    5043\nMatched (ESS)    1819    5043\nMatched          2658    5043\nUnmatched        2299       0\nDiscarded           0       0\n\n\nAnd also the estimated average treatment effect is very similar to the one obtained by (default) CEM.\n\n# Use matched data\ndf_nn <- match.data(nn)\n\n# (2) Estimation\nmodel_nn <- lm(sick_days ~ health_program, data = df_nn, weights = weights)\nsummary(model_nn)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_nn, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-7.149 -1.492 -0.492  1.006 19.508 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          8.0198     0.0466   172.1  < 2e-16 ***\nhealth_programTRUE   0.4723     0.0576     8.2  2.8e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.4 on 7699 degrees of freedom\nMultiple R-squared:  0.00866,   Adjusted R-squared:  0.00853 \nF-statistic: 67.2 on 1 and 7699 DF,  p-value: 2.79e-16\n\n\n\n\n\n\n\n\nCurse of dimensionality\n\n\n\nWith exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don’t find any matches at all.\nRegarding exact matching, consider for example the situation with two covariates with each five different values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates ~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations."
  },
  {
    "objectID": "content/toolbox/06_match.html#inverse-probability-weighting",
    "href": "content/toolbox/06_match.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability weighting",
    "text": "Inverse Probability weighting\nOne way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let’s go into more detail.\nEstimating Propensity Score\nWe start by understanding what probability in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as propensity score.\nHere, we will use a logistic regression for prediction. A logistic regression, opposed to a linear regression, is designed for outcomes that are between 0 and 1, such as probabilities. The coefficients are a bit more difficult to interpret, so we’ll leave that for now. But what we see is, that age and sick_days_before are relevant and positive predictors for the probability of treatment.\n\n# (1) Propensity scores\nmodel_prop <- glm(health_program ~ age + sick_days_before,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n\nCall:\nglm(formula = health_program ~ age + sick_days_before, family = binomial(link = \"logit\"), \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-2.333  -1.125   0.633   1.166   1.527  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -1.2923     0.1002   -12.9  < 2e-16 ***\nage                0.0139     0.0021     6.6  4.1e-11 ***\nsick_days_before   0.1802     0.0115    15.6  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13862  on 9999  degrees of freedom\nResidual deviance: 13498  on 9997  degrees of freedom\nAIC: 13504\n\nNumber of Fisher Scoring iterations: 4\n\n\nFor each observation we can compute a probability and add it to a table. It is important to specify type = \"response\" in the predict() command to obtain probabilities.\n\n# Add propensities to table\ndf_aug <- df %>% mutate(propensity = predict(model_prop, type = \"response\"))\n\nHaving obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as propensity score matching. However, there are several reasons why this is not a good identification strategy1, mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that same covariate values indeed imply the same propensity score.\nWeighting by Propensity Score\nInstead inverse probability weighting (IPW) has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use the propensity score of an observation unit to in- or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\\[\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n\\]\nwhere only one of the terms is always active as \\(D_i\\) is either one or zero. Now we should better understand what “inverse probability weighting” actually means. It weights each observation by its inverse of its treatment probability. Let’s compute it for our data.\n\n# Extend data by IPW scores\ndf_ipw <- df_aug %>% mutate(\n  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw)\n\n\n\n  \n\n\n\nImagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. \\(D_i=1\\). Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let’s assume it was 0.8, for demonstration. Then his/her weight would equal \\(w_i = \\frac{1}{0.8} = 1.25\\).\nCompared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let’s say his/her probability of participating would be 0.2. Then, his/her weight would be \\(w_i = \\frac{1}{0.2} = 5\\). So we see, he/she would obtain a significantly higher weight.\nIn general, IPW upweights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group.\nRunning a linear regression with weights as provided by IPW yields a coefficient not as precise as the ones obtained by previous matching procedures.\n\n# (2) Estimation\nmodel_ipw <- lm(sick_days ~ health_program,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_ipw, weights = ipw)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n -8.99  -2.38  -0.22   1.25  51.43 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          7.8073     0.0352  221.73  < 2e-16 ***\nhealth_programTRUE   0.3385     0.0500    6.77  1.3e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 9998 degrees of freedom\nMultiple R-squared:  0.00457,   Adjusted R-squared:  0.00447 \nF-statistic: 45.9 on 1 and 9998 DF,  p-value: 1.34e-11\n\n\nA reason for that could be that there are some extreme and very atypical observations with either very high or low probabilities. Those observations than get a very high weight.\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nMost predicted treatment probabilities are between 0.4 and 0.6. Only a few outliers.\n\n\n\n\n\n# Looking for observations with highest weights\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw) %>% \n  arrange(desc(ipw))\n\n\n\n  \n\n\n\nA rule of thumb is to filter out all observations with a propensity score less than 0.15 and higher than 0.85. Doing that,the coefficient is very close to the true treatment effect.\n\n# Run with high weights excluded\nmodel_ipw_trim <- lm(sick_days ~ health_program,\n                data = df_ipw %>% filter(propensity %>% between(0.15, 0.85)),\n                weights = ipw)\nsummary(model_ipw_trim)\n\n\nCall:\nlm(formula = sick_days ~ health_program, data = df_ipw %>% filter(propensity %>% \n    between(0.15, 0.85)), weights = ipw)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-8.947 -2.164 -0.185  1.276 23.773 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          7.6391     0.0322   237.2   <2e-16 ***\nhealth_programTRUE   0.4796     0.0456    10.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.2 on 9961 degrees of freedom\nMultiple R-squared:  0.011, Adjusted R-squared:  0.0109 \nF-statistic:  111 on 1 and 9961 DF,  p-value: <2e-16\n\n\nOpposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the probability model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders."
  },
  {
    "objectID": "content/toolbox/06_match.html#comparison",
    "href": "content/toolbox/06_match.html#comparison",
    "title": "Matching and Subclassification",
    "section": "Comparison",
    "text": "Comparison\nComparing the methods, we see that some of them were yield results very close to the true treatment effect. The naive estimation, however, was far off. Also, using custom cut-points (that were too wide) for coarsening also reduces the accuracy.\n\n# Summary of naive and matching methods\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                \"CEM1\"  = model_cem,\n                                \"CEM2\"  = model_cem_coars,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n\n\n\n\n   \n    Naive \n    CEM1 \n    CEM2 \n    NN \n    IPW1 \n    IPW2 \n  \n\n\n (Intercept) \n    7.262 \n    7.602 \n    7.519 \n    8.020 \n    7.807 \n    7.639 \n  \n\n  \n    (0.033) \n    (0.032) \n    (0.033) \n    (0.047) \n    (0.035) \n    (0.032) \n  \n\n health_programTRUE \n    1.230 \n    0.481 \n    0.694 \n    0.472 \n    0.338 \n    0.480 \n  \n\n  \n    (0.046) \n    (0.045) \n    (0.046) \n    (0.058) \n    (0.050) \n    (0.046) \n  \n\n Num.Obs. \n    10000 \n    9964 \n    9995 \n    7701 \n    10000 \n    9963 \n  \n\n R2 \n    0.067 \n    0.011 \n    0.022 \n    0.009 \n    0.005 \n    0.011 \n  \n\n R2 Adj. \n    0.067 \n    0.011 \n    0.022 \n    0.009 \n    0.004 \n    0.011 \n  \n\n AIC \n    44992.4 \n    44660.3 \n    45177.5 \n    35819.7 \n    46918.3 \n    44825.9 \n  \n\n BIC \n    45014.1 \n    44681.9 \n    45199.1 \n    35840.6 \n    46939.9 \n    44847.5 \n  \n\n Log.Lik. \n    -22493.225 \n    -22327.154 \n    -22585.732 \n    -17906.857 \n    -23456.132 \n    -22409.937 \n  \n\n F \n    718.301 \n    114.225 \n    227.297 \n    67.240 \n    45.864 \n    110.820 \n  \n\n RMSE \n    2.29 \n    2.24 \n    2.29 \n    2.38 \n    2.34 \n    2.22"
  },
  {
    "objectID": "content/toolbox/07_did.html",
    "href": "content/toolbox/07_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Another quasi-experimental method from our toolbox is the difference-in-differences (DiD) approach. It is the most popular research design in quantitative and social sciences. As the name implies, the method captures differences by observing a treatment and a control group over time to estimate causal average effects.\nIn its simplest form, DiD compares two groups (control and treatment) at two points in time (before treatment and after) by observing if and how different both groups change. It is important to note, that both groups do not need to be equal before the treatment.\nBy taking two differences, two different kind of biases can be avoided. First, by comparing both groups at both points in time, any external effect that affects the outcome through time plays no role as it affects both groups. Secondly, taking only the difference of change in consideration, we can disregard selection bias. Potential outcomes can differ:\n\\[\nE[Y_0|D = 0] \\gtreqqless E[Y_0|D = 1]\n\\]\nWe don’t care whether initially treatment and control group are different. We only assume that they behave similarly in absence of treatment.\nAs can be seen in the table, the difference in outcome for the treatment group before and after treatment is \\(D + T\\), while for the control group it is only \\(T\\). The difference of these two differences then reduces to only \\(D\\), which is the treatment effect we want to estimate.\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\nWe can also break it down in our known notation:\n\\[\nD = ATE = \\bigg(E[Y_{D=1}|T=1] - E[Y_{D=1}|T=0] \\bigg)- \\bigg(E[Y_{D=0}|T=1] - E[Y_{D=0}|T=0]\\bigg)\n\\]\nBecause there are a lot of subscripts, it can also help to write down the formula in pseudo-math:\n\\[\nATE = (Y_{Treatment, After} - Y_{Treatment, before}) - (Y_{Control, After} - Y_{Control, Before})\n\\]\nAgain, opposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential outcomes \\(E[Y_0|D=1] = E[Y_0|D=1]\\) are equal. That is a big difference, because do not have to assume that observation units are similar in all their characteristics.\nInstead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but only the change of outcomes from before to after. It implies that there is no factor that has only an impact on just one of the groups. If units differ in characteristics, they are only allowed to have a constant effect. If the effect varies with time, the parallel trends assumption is violated."
  },
  {
    "objectID": "content/toolbox/07_did.html#parallel-trends",
    "href": "content/toolbox/07_did.html#parallel-trends",
    "title": "Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\nScenario A\nTo compute an estimated treatment effect, we filter the data to the two periods just around treatment and implement the formulas as in the introduction. Not surprisingly, we get an estimate that is very close to our true treatment effect.\n\n# [1.1.1] (A) Fulfillment ----\n# Scenario (A)\n# Only show last data point before and first data point after treatment.\ndf_A_zoom_in  <- df_A %>% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_A <- df_A_zoom_in %>%\n  filter(treat == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_A <- df_A_zoom_in %>%\n  filter(treat == 1, after == 0) %>% \n  pull(sales)\n\ndiff_before_A <- before_treatment_A - before_control_A\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_A <- df_A_zoom_in %>%\n  filter(treat == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_A <- df_A_zoom_in %>%\n  filter(treat == 1, after == 1) %>% \n  pull(sales)\n\ndiff_after_A <- after_treatment_A - after_control_A\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate may vary from \n# true treatment effect, as we also include some noise in the data generating \n# process.\ndiff_diff_A <- diff_after_A - diff_before_A\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_A, delta)\n\n[1] \"Estimate: 1.05, True Effect: 1.00\"\n\n\nLooking at the last period before and the first period after treatment, the impact of treatment can clearly be seen. The dashed line represents the counterfactual value for the treated group, i.e. the value it would have if it had not been treated. This value is not observed, but by the parallel trends assumptions, it would have developed like the value for the untreated group.\n\n\n\n\n\n\nPlots in this chapter\n\n\n\nGenerating the plots for this chapter is a bit tricky as they contain a lot of annotations and other extensions. I left the code for those who want to replicate it. But you do not worry if you cannot reproduce them.\n\n\n\nPlot parallel trends assumption# Compute counterfactual sales for treated group\ncf_treat_A <- df_A[!df_A$treat == 1, \"sales\"] + diff_before_A\ndf_A[df_A$treat == 1, \"sales_cf\"] <- cf_treat_A\n\n# Add to zoomed in table\ndf_A_zoom_in <- df_A_zoom_in %>% left_join(df_A)\n\n# Plot DiD with parallel trends assumption\nggplot(df_A_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A_zoom_in %>% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_A - (diff_diff_A / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_A + 1.1*diff_before_A + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A:\\nParallel Trends Assumption\")\n\n\n\nIf parallel trends assumption can be assumed, treatment effect is valid. Counterfactual line shows how outcome would have been evolved in absence of treatment.\n\n\n\n\nScenario B\nRepeating the steps for scenario B yields an unexpected result. The estimated treatment effect is different from what we would have expected.\n\n# [1.1.2] (B) Violation ----\n# Scenario (B)\n# Only show last data point before and first data point after treatment.\ndf_B_zoom_in  <- df_B %>% filter(period %in% (P/2):(P/2+1))\n\n# Manually compute differences\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control_B <- df_B_zoom_in %>%\n  filter(treat == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_B <- df_B_zoom_in %>%\n  filter(treat == 1, after == 0) %>% \n  pull(sales)\n\ndiff_before_B <- before_treatment_B - before_control_B\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control_B <- df_B_zoom_in %>%\n  filter(treat == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_B <- df_B_zoom_in %>%\n  filter(treat == 1, after == 1) %>% \n  pull(sales)\n\ndiff_after_B <- after_treatment_B - after_control_B\n\n# Step 3: Difference-in-differences. Unbiased estimate if parallel trends is \n# correctly assumed and there is no hidden confounding. Estimate varies from \n# true treatment effect due to confounding and added noise.\ndiff_diff_B <- diff_after_B - diff_before_B\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_B, delta)\n\n[1] \"Estimate: 1.23, True Effect: 1.00\"\n\n\nAgain, the picture is very similar. Having only four data points, treatment before and after and control before and after, there is no way to test the parallel trends assumption which leaves room for doubt. So how can we check whether we made a mistake or the parallel trends assumption is violated?\n\nPlot parallel trends assumption# Compute counterfactual sales for treated group\ncf_treat_B <- df_B[!df_B$treat == 1, \"sales\"] + diff_before_B\ndf_B[df_B$treat == 1, \"sales_cf\"] <- cf_treat_B\n\n# Add to zoomed in table\ndf_B_zoom_in <- df_B_zoom_in %>% left_join(df_B)\n\n# Plot DiD with parallel trends assumption\nggplot(df_B_zoom_in, aes(x = period, y = sales, color = as.factor(treat))) +\n  # Geographic elements\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B_zoom_in %>% filter(treat == 1),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], alpha = .8, linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  annotate(geom = \"label\", x = (P/2) + 0.9, \n           y = after_treatment_B - (diff_diff_B / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.7, \n           y = before_control_B + 1.1*diff_before_B + .1, \n           label = \"Counterfactual\", size = 4, \n           angle = 3) +\n  # Custom scaling and legend\n  scale_x_continuous(name =\"\", breaks=c(5, 5.5, 6),\n                     labels = c(\"Before\\n Treatment\", \n                                \"Treatment\",\n                                \"After\\n Treatment\")) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B:\\nParallel Trends Assumption\")\n\n\n\nIf parallel trends assumption can be assumed, treatment effect is valid. Counterfactual line shows how outcome would have been evolved in absence of treatment."
  },
  {
    "objectID": "content/toolbox/07_did.html#event-study",
    "href": "content/toolbox/07_did.html#event-study",
    "title": "Difference-in-Differences",
    "section": "Event Study",
    "text": "Event Study\nMany researchers therefore try to increase the validity of their results by providing an event study. Comparing trends before treatment across treatment and control group, it should show that there was no difference. Because if there was no difference before treatment, why should there be difference after the treatment (if not for the treatment?\nHowever, event studies cannot provide full certainty about the parallel trends assumption. There still might be other unobserved factors that could affect the treatment. But still, it is a good way to argue that treatment and control group are comparable.\n\nCode: Event Study A# [1.2] Event study ----\n# To provide evidence of the credibility in assuming parallel trends, \n# researchers often perform an event study, if possible. Instead of only\n# looking at the last period before and the first period after treatment,\n# further periods are included to examine the validity of the parallel trends\n# assumption and the treatment effect estimate.\n\n# [1.2.1] (A) Fulfillment ----\n# Zoom out and show that parallel trend assumption is fulfilled in scenario (a)\n\n# Compute difference in control group\ndiff_control_A <- after_control_A - before_control_A\n\n# Plot event study\nev_stdy_A <- ggplot(df_A, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_A %>% filter(treat == 1, period >= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[3], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_A, yend = after_treatment_A - diff_diff_A,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(breaks = 1:P) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario A\\nEvent Study\")\n\n\n\nCode: Event Study B# [1.2.2] (B) Violation----\n# Zoom out and show that parallel trend assumption is violated in scenario (b)\n\n# Compute difference in treatment group\n# Before treatment\ndiff_control_B <- after_control_B - before_control_B\n\n# Increase from t0 to before treatment\ninit_treatment_B <- df_B %>%\n  filter(treat == 1, period == 1) %>%\n  pull(sales)\ndiff_treatment_B <-  (before_treatment_B - init_treatment_B) / (P/2)\n\n# Plot event study\nev_stdy_B <- ggplot(df_B, aes(x = period, y = sales, color = as.factor(treat))) +\n  geom_line() +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[5]) + \n  geom_line(data = df_B %>% filter(treat == 1, period >= P/2),\n            aes(x = period, y = sales_cf),\n            color = ggthemr::swatch()[2], linetype = \"dashed\") +\n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_B,\n           yend = before_treatment_B + (P/2)*(diff_treatment_B),\n           linetype = \"dashed\", color = ggthemr::swatch()[3]) +\n  # Estimated treatment effect\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_B, yend = after_treatment_B - diff_diff_B,\n           linetype = \"dashed\", color = ggthemr::swatch()[4], linewidth = 1) +\n  # Custom scaling and legend\n  scale_x_continuous(name   = \"Period\", breaks = 1:P) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Scenario B\\nEvent Study\")\n\n\n\n\n\n\nFor treatment and control group, we see the same trend over time. This lends credibility to the parallel trends assumption and consequently, to the validity of the causal treatment effect.\n\n\n\n\nOther than in scenario A, the parallel trends assumption does not seem to hold. The estimated treatment effect is larger than the actual treatment effect. This is due to different trends in both groups. The treatment group has a more positive trend even without treatment and the groups would have further diverged after treatment (see dashed red line). The difference between the dashed red and dashed blue line is attributable to this trend and should not be part of the treatment effect."
  },
  {
    "objectID": "content/toolbox/07_did.html#modeling",
    "href": "content/toolbox/07_did.html#modeling",
    "title": "Difference-in-Differences",
    "section": "Modeling",
    "text": "Modeling\nA more typical situation is usually that there is more than one unit in the treatment and control group. You could e.g. imagine that you are managing more than two stores and are implementing an ad campaign in a specific region.\nTo simulate such a scenario, we generate data for 3’000 stores that are split evenly into two regions. In one region, the ad campaign will be run (treatment region) and in the other there will be no campaign (control region). The variable relationships as defined in the previous section still hold.\n\n# [1.4] Linear regression ----\n# Now assume that there are more than two stores and treatment is performed\n# e.g. in a specific region which are, depending on scenario (A) and (B) \n# different.\n# Generate a bunch of samples and combine in one table. Here, we choose a higher\n# standard deviation.\nn_stores <- 3e+3\n\n# Scenario A\ndf_A_lm  <- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"A\")}) %>%\n    bind_rows() %>%\n  filter(period %in% (P/2):(P/2+1))\n\n# Scenario B\ndf_B_lm  <- lapply(1:n_stores, function(R) {\n  generate_data(sd = 1, scenario = \"B\")}) %>%\n  bind_rows() %>%\n  filter(period %in% (P/2):(P/2+1))\n\nScenario A\nSo how do we compute the average treatment effect? Previously in this chapter, we just used basic math calculations (particularly subtraction). But there is an easier way: we can use regression again. This is because the average treatment effect is the coefficient of the interaction of group and time.\n\\[\ny_i = \\beta_0 + \\beta_1 * Period_i + \\beta_2 * Treatment_i + \\beta_3 * (Time_i * Treatment_i) + \\epsilon_i\n\\]\n\\(Time\\) indicates whether the period is before or after the treatment and \\(Treatment\\) whether an observation was treated or not. Then, the coefficient we are interested in is \\(\\beta_3\\), because its term is only active for the treated group after treatment.\n!!! x1: maybe purchase power in region\nFor scenario A, we can see that there is no need to adjust for the covariate \\(x1\\). If you check the formulas again, your will notice that \\(x1\\) has a constant and time-invariant effect on sales and therefore it does not violate the paralell trends assumption.\nIncluding or leaving out \\(x1\\) in the regression yields the a similar unbiased estimate (close to defined true size) for our variable of interest \\(store:after\\), the parameter of interest.\n\n# [1.4.1] (A) ----\n# (a): Due to the construction of the data set, we expect interaction\n# coefficient to be significant as well as the covariate and period. However, as\n# the covariate does not have a time-varying effect, it is not a confounder and\n# interaction coefficient should be unbiased even if not adjusting for the\n# covariate.\nsummary(lm(sales ~ treat*after , data = df_A_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.176 -0.952  0.002  0.956  6.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.9610     0.0257 1981.43   <2e-16 ***\ntreat         1.0372     0.0364   28.52   <2e-16 ***\nafter         0.2642     0.0364    7.26    4e-13 ***\ntreat:after   0.9813     0.0514   19.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 11996 degrees of freedom\nMultiple R-squared:  0.284, Adjusted R-squared:  0.284 \nF-statistic: 1.58e+03 on 3 and 11996 DF,  p-value: <2e-16\n\nsummary(lm(sales ~ treat*after + x1, data = df_A_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after + x1, data = df_A_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.947 -0.680  0.000  0.673  4.204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.98128    0.01821 2800.25   <2e-16 ***\ntreat        0.03914    0.02732    1.43     0.15    \nafter        0.22876    0.02575    8.88   <2e-16 ***\nx1           0.98721    0.00903  109.31   <2e-16 ***\ntreat:after  0.97808    0.03641   26.86   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11995 degrees of freedom\nMultiple R-squared:  0.641, Adjusted R-squared:  0.641 \nF-statistic: 5.36e+03 on 4 and 11995 DF,  p-value: <2e-16\n\n\nScenario B\nIn scenario B, the effect of x1 is different because it has a time-varying effect. Therefore it violates the parallel trends assumption, leading to a biased estimate if x1 is not included (e.g. because it is unobserved).\nBecause we constructed the data set ourselves, we are able to see that the bias in fact is quite large and the treatment effect seems to include the actual treatment effect plus the effect of x1. Even with including \\(x1\\) and as a main effect and moderator, we cannot fully reconstruct the true treatment effect.\n\n# [1.4.2] (B) ----\n# (b): Due to the construction of the data set, we expect interaction coefficient\n# to be significant and accurate only when adjusting for the time-varying effect\n# of the covariate and main effects for period and covariate.\nsummary(lm(sales ~ treat*after, data = df_B_lm))\n\n\nCall:\nlm(formula = sales ~ treat * after, data = df_B_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.025  -2.331   0.008   2.293  16.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  51.0835     0.0643  794.08   <2e-16 ***\ntreat         3.6321     0.0910   39.92   <2e-16 ***\nafter         0.0682     0.0910    0.75     0.45    \ntreat:after   1.4315     0.1287   11.13   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 11996 degrees of freedom\nMultiple R-squared:  0.287, Adjusted R-squared:  0.287 \nF-statistic: 1.61e+03 on 3 and 11996 DF,  p-value: <2e-16\n\nsummary(lm(sales ~ treat*after + after*x1 + treat*x1, data = df_B_lm)) # best\n\n\nCall:\nlm(formula = sales ~ treat * after + after * x1 + treat * x1, \n    data = df_B_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.084 -0.682  0.001  0.679  4.348 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 51.00199    0.01818 2804.68  < 2e-16 ***\ntreat        0.00336    0.03012    0.11     0.91    \nafter        0.16597    0.02571    6.45  1.1e-10 ***\nx1           2.63804    0.01591  165.81  < 2e-16 ***\ntreat:after  1.00018    0.04063   24.62  < 2e-16 ***\nafter:x1     0.36083    0.01823   19.80  < 2e-16 ***\ntreat:x1     1.03128    0.01823   56.57  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11993 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.943 \nF-statistic: 3.31e+04 on 6 and 11993 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/08_iv.html",
    "href": "content/toolbox/08_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "The method we introduce in this chapter is called instrumental variables estimation (IV), a quasi-experimental method heavily used in economics to identify causal effects in observational studies with unobserved confounders. Thus, it can control for omitted variable bias which is present when we were either not able to collect all relevant data or we are not aware of some confounders.\nDue to not observing particular confounders, we cannot close the backdoor path by for example matching or regression. IV provides an alternative by introducing an additional variable (the instrumental variable/instrument), which affects the outcome only through the treatment variable.\nThe instrumental variable is exogenous, i.e. there is no other variable in the model influencing the value of the instrument (no arrow pointing to the instrument). Thus, it mimics an experiment by exploiting the exogenous variation in treatment due to the instrument. Endogenous variation from unobserved confounders can be disregarded as we are only considering the variation in treatment caused by the instrument.\nIV can be illustrated using DAGs. On the left, there is a potential initial situation you could find yourself in: you want to examine the effect of \\(D\\) on \\(Y\\), but unfortunately, there is an unobserved confounder (=omitted variable) that you would have to adjust for to identify the direct effect. As \\(U\\) is unobserved, there is no way to close the backdoor path by methods like matching, regression etc. This is where IV comes to rescue.\nAs you can see on the right, now \\(D\\) mediates between an instrument \\(Z\\) and outcome \\(Y\\). There is no direct path between \\(Z\\) and \\(Y\\) and therefore \\(Z\\) affects \\(Y\\) only through \\(D\\). For the instrument validity, there are a few assumptions that need to be fulfilled, which we discuss later in detail but summarizing, we need\n\nRelevance: \\(Z \\rightarrow D, \\,\\,\\, Cor(Z,D) \\neq 0\\) (testable)\nExcludability: \\(Z \\rightarrow D \\rightarrow Y,\\,\\, Z \\not\\to Y, \\,\\,\\, Cor(Z, Y|D) =0\\) (partly testable)\nExogeneity: \\(U \\not\\to Z, \\,\\,\\, Cor(Z, U)=0\\) (not testable)\n\n\n\n\n\nOmitted variable bias\n\n\n\n\nInstrumental variable estimation"
  },
  {
    "objectID": "content/toolbox/08_iv.html#exploration",
    "href": "content/toolbox/08_iv.html#exploration",
    "title": "Instrumental Variables",
    "section": "Exploration",
    "text": "Exploration\nFor the sake of explanation, we generate a synthetic data set with the variables \\(Z\\), \\(D\\) and \\(Y\\) as defined above. We also include the unobserved variable \\(U\\). This way, we can better explain where the bias comes from and how it affects the estimated treatment effect. The true treatment effect, which is the direct effect of \\(D\\) on \\(Y\\), is set to 1. We also add some random noise, so relationships are not perfect and what you will see is closer to reality.\n\n# Show data\ndf\n\n\n\n  \n\n\n\nFrom the table, you already have an idea what the data structure and types look like. But when developing our analysis strategy, we are mainly interested in relationships between the variables, so let’s have a look at the correlation matrix. Some requirements for a valid IV strategy can already be checked with it. Please also note, that we include \\(U\\) here, but in general, you do not observe \\(U\\), which was the reason why we need to use IV in the first place. It is just for the purpose of explanation that we include it here.\nFrom the correlation matrix, we can take a few important insights. There is a significant negative correlation between distance and program participation, which is also called first-stage and confirms the relevance of our instrument. The distance to the next training location does affect decision to participate in a training. This is assumption (4) from above.\nHaving a synthetic data set, we can also see that our instrument is uncorrelated with the unobserved variable motivation, which means there is no confounding as stated in assumption (2). Usually, however, we would not be able to test this assumption due to an unobservable variable not being observed. Moreover, there could be additional confounders. With merely statistical concepts, we cannot prove that the effect of \\(Z\\) on \\(Y\\) goes only through \\(D\\). We have to argue why that is the case.\n\n# Correlation matrix\ncor(df) %>% round(2)\n\n           distance program motivation   kpi\ndistance       1.00   -0.56       0.01 -0.25\nprogram       -0.56    1.00       0.39  0.50\nmotivation     0.01    0.39       1.00  0.30\nkpi           -0.25    0.50       0.30  1.00\n\n\nCorresponding to our DAG, we also see that both program participation and KPI are correlated with the (unobserved) motivation, which thus opens another path between treatment and outcome. Being unable to close this path, we actually need an instrument in this situation.\nIn general, it is very useful to also plot the data to check relationships between variables. It confirms what we have seen in the correlation matrix."
  },
  {
    "objectID": "content/toolbox/08_iv.html#modeling",
    "href": "content/toolbox/08_iv.html#modeling",
    "title": "Instrumental Variables",
    "section": "Modeling",
    "text": "Modeling\nConfounding\nWhen you plausibly argued that your instrument is valid, you would usually perform 2SLS, short for Two Stage Least Squares. We’ll come to that shortly, but because we have all the data, we can also check what the bias would be in case we would not use an instrument and ignore the confounder.\nRemember, the true treatment effect is 1 in this case. Therefore, when we regress \\(Y\\) on \\(D\\) and \\(U\\), we should be able to recover this effect. And when you look at the regression output, in fact, we do. It is not exactly equal 1, but that is due to sampling noise.\n\n# First of all, let's look at the coefficients of the \"full\" (but unobservable)\n# model. It is unobservable, as it includes motivation, which in reality is \n# a variable that is very hard to collect or measure.\n# Coefficients are expected to be close to what he have defined in the data\n# generation section.\nmodel_full <- lm(kpi ~ program + motivation, data = df)\nsummary(model_full)\n\n\nCall:\nlm(formula = kpi ~ program + motivation, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.548 -0.680 -0.008  0.680  3.538 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.0694     0.0562   -1.23     0.22    \nprogram       1.0469     0.0282   37.18   <2e-16 ***\nmotivation    1.1017     0.1078   10.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5997 degrees of freedom\nMultiple R-squared:  0.261, Adjusted R-squared:  0.26 \nF-statistic: 1.06e+03 on 2 and 5997 DF,  p-value: <2e-16\n\n\nBut what would happen if we ignored the confounder \\(motivation\\) and only regress \\(Y\\) on \\(D\\)? We do not close the backdoor path and consequently, the effect is overestimated.\nWe can see that the coefficient for program participation is higher than expected, i.e. it has an upward bias. This is because, it takes some of the variation that is actually attributable to motivation into the coefficient of program participation. This confirms the need to include an instrument to model causal effects when there is no way to include the confounder.\n\n# Modeling the data without the unobservable variable, i.e. only including \n# program participation in this case, returns a biased coefficient as the \n# relationship between program and the outcome is biased by a collider.\nmodel_biased <- lm(kpi ~ program, data = df)\nsummary(model_biased)\n\n\nCall:\nlm(formula = kpi ~ program, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.611 -0.689 -0.006  0.671  3.565 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4736     0.0185    25.7   <2e-16 ***\nprogram       1.1602     0.0261    44.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5998 degrees of freedom\nMultiple R-squared:  0.248, Adjusted R-squared:  0.248 \nF-statistic: 1.98e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\n2SLS\nTwo Stage Least Square is the estimation technique for IV and consists, as it names suggest, of two stages. In the first stage, the treatment variable is regressed on the instrument and in the second stage, the estimated values of the first stage are used as a regressors for the outcome. It sounds a little bit confusing, so let’s write it down. Remember, \\(D\\) is our treatment, \\(Z\\) the instrument and \\(Y\\) the outcome.\nFirst stage:\n\\[\nD_i = \\gamma_0 + \\gamma_1Z_i + \\nu_i\n\\]\nSecond stage:\n\\[\nY_i = \\beta_0 +\\beta_1\\widehat{D_i} +\\epsilon_i\n\\]\nLower cases indicate single observations, so \\(i\\) indicates for example the row in our data set. What is important to note, is that in the second stage, we do not use \\(D_i\\), but instead \\(\\widehat{D_i}\\). The hat indicates, that these are fitted values from the first stage.\nWe can do 2SLS manually in R, but for reasons I will get to later, it is recommended to use libraries built to run 2SLS. However, for purpose of explanation, we’ll do it also manually here.\nFirst stage: As already discussed, regress treatment variable on instrument and obtain the fitted model. The model coefficient returned by the model summary should be significant, otherwise there is reason to doubt the relevance and validity of the instrument. You can also look at the F-statistic, which should be above 10.\n\n\n\n\n\n\nUse linear regression\n\n\n\nAlthough program is a binary variable, we use linear regression instead of logistic regression. Otherwise, 2SLS does not provide correct results. A linear regression predicting binary outcomes is called linear probability model (LPM).\n\n\nHere, the instrument is highly significant. The higher the distance, the lower the likelihood of participation.\n\n# First stage\nfirst_stage <- lm(program ~ distance, data = df)\nsummary(first_stage)\n\n\nCall:\nlm(formula = program ~ distance, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1263 -0.3322 -0.0372  0.3486  1.0201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.4672     0.0191    76.7   <2e-16 ***\ndistance     -1.6193     0.0308   -52.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.41 on 5998 degrees of freedom\nMultiple R-squared:  0.316, Adjusted R-squared:  0.316 \nF-statistic: 2.77e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\nLet’s look at the fitted values from the first stage. The fitted values is what you get when you use the calculated coefficients and for each observation compute what the model predicts as an expected value. So, they are most likely a different from the actual values. How different they are depends on the goodness of fit of your model. This is why it is important that your instrument has a good explanatory value for the treatment variable.\nLet’s see how well we can explain the decision to participate using the distance to the training location. For employees that actually participated, predicted probabilities are on average higher than for those who have not participated. However, there is a large amount of overlap between both groups. By the way, because we used a linear probability model, not all probabilities are between 0 and 1.\n\n# Predicted 'probabilities' from first stage\npred_fs <- predict(first_stage)\n\n# Create table with predictions and actual decisions\npred_vs_actl <- tibble(\n  pred = pred_fs,\n  actl = df$program\n)\n\n# Plot predictions vs original\nggplot(pred_vs_actl, aes(x = pred, y = actl, color = as.factor(actl))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n    theme(legend.title = element_blank())\n\n\n\nPredicted ‘probabilities’ for treated and untreated units.\n\n\n\n\nNow we continue to use the fitted values from the first stage and plug it in the second stage to get the local average treatment effect. We see that the coefficient for the effect is close to one as constructed and we were able to eliminate the omitted variable bias.\n\n# Second stage\nsecond_stage <- lm(df$kpi ~ first_stage$fitted.values)\nsummary(second_stage)\n\n\nCall:\nlm(formula = df$kpi ~ first_stage$fitted.values)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.758 -0.765 -0.005  0.761  3.844 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 0.5290     0.0297    17.8   <2e-16 ***\nfirst_stage$fitted.values   1.0493     0.0518    20.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.1 on 5998 degrees of freedom\nMultiple R-squared:  0.064, Adjusted R-squared:  0.0639 \nF-statistic:  410 on 1 and 5998 DF,  p-value: <2e-16\n\n\nHowever, it is recommended to use functions, like e.g. iv_robust() from the estimatr package, as it yields correct standard errors. You see that the coefficient is the same but the standard errors slightly differ.\n\n# Using our instrument (distance to training location), we try to eliminate the\n# bias induced by the omitted variable. If all assumptions regarding the validity\n# of our instrument are met, the resulting coefficient should be\n# close to what we have defined above.\nlibrary(estimatr)\nmodel_iv <- iv_robust(kpi ~ program | distance, data = df)\nsummary(model_iv)\n\n\nCall:\niv_robust(formula = kpi ~ program | distance, data = df)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(>|t|) CI Lower CI Upper   DF\n(Intercept)    0.529     0.0271    19.5  1.73e-82    0.476    0.582 5998\nprogram        1.049     0.0475    22.1 4.68e-104    0.956    1.142 5998\n\nMultiple R-squared:  0.245 ,    Adjusted R-squared:  0.245 \nF-statistic:  488 on 1 and 5998 DF,  p-value: <2e-16"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "",
    "text": "lv3060Winter term 2022/2023Institute of EntrepreneurshipTUHH"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "Course description",
    "text": "Course description\n\nDistinguishing causal relationships from simple correlation is what commonly used approaches in business analytics often fall short of. In this course, we will provide you with the skill set to answer questions like\n\nwhat happens to \\(Y\\) if we do \\(X\\)?\nwas it \\(X\\) that caused \\(Y\\) to change?\n\nIntroducing you to causal inference with the help of data science will allow you to carry out state-of-the-art causal analyses by yourself and extrapolate causal knowledge across different business contexts and various management areas."
  },
  {
    "objectID": "index.html#website-structure",
    "href": "index.html#website-structure",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "Website Structure",
    "text": "Website Structure\n\nBrowse all the class content, assignments and examples by going to Content. For instruction how to pass the course go to Submission. If you have problems regarding the content, solving the assignments or any other questions, use the chat icon to directly go to our Mattermost discussion channel."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "Instructor",
    "text": "Instructor\n   Oliver Mork\n   oliver.mork@tuhh.de\n   StartupEngineer"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "<b>CAUSAL DATA SCIENCE FOR BUSINESS ANALYTICS</b>",
    "section": "Course Details",
    "text": "Course Details\n   23.11. / 30.11. / 07.12.\n   09:00 - 17:00\n   HS28 - 1.006"
  },
  {
    "objectID": "submission/submission.html",
    "href": "submission/submission.html",
    "title": "Submission of the Assignments",
    "section": "",
    "text": "To submit your solutions to the problem set, you’ll have to run your code in .qmd (quarto markdown format) and render your files to HTML-files.\nI’ll guide you through the necessary steps.\n\n\n\n\n\n\nYour solutions\n\n\n\nYour solutions don’t need to be completely free from errors but you should have an attempt at each assignment. Please make sure, that your code is running and generating output.\n\n\n\n\nTo conveniently organize your files, scripts and data sets you should use an R project. It helps keeping all files in the same place without the hustle of moving files and changing paths all the time. To create a new project just follow the steps. I am working on macOS, but it should be very similar on other operating systems.\n\n\n\n1) File -> New Project\n\n\n\n\n\n2) New Directory. If you already have a folder that you would to be the project directory, you can click Existing Directory.\n\n\n\n\n\n3) New Project\n\n\n\n\n\n4) Specify the directory and give your project a name (whatever you like, I chose Causal_Data_Science. Then click on Create Project.\n\n\nNow, you will see that in your folder, there is a file with the file extension .Rproj. This is your project file that contains information about the location of your directory and other project settings. Every time you want to open your project, just click on it and it will open RStudio and your project.\n\n\n\nNow that you have set up a project, it should be already open. You can check that by looking at the top right and see if you see the name of your project. If not you have to open it.\nIn this section, I’ll show you how to create a .qmd documents. These documents can generate various kinds of output documents to present your results like PDF, HTML, Word etc. We will focus on HTML documents. These will contain all your code needed to solve the assignments and will also display the results.\nSo, let’s see what we have to do.\n\n\n\n1) Because your project is open, you are in your project folder. Here, you can now create a new .qmd file by clicking on New Blank File and Quarto Doc.\n\n\n\n\n\n2) Name the file accordingly, for example chapter_1.qmd for the assignments you have to submit in chapter 1.\n\n\n\n\n\n3) First, to define what output format you want to have and to give your document a title, insert a YAML Block. Please make sure that you have activated the Visual editor (instead of Source). This one is easier to handle and gives you a preview of what your final document will look like.\n\n\n\n\n\n4) Modify your YAML Block as shown in the picture. Below, you can start writing regular text which can be formatted with Format. Other elements can be inserted with Insert. The button, where the mouse is located creates a code chunk which is used to run code.\n\n\nYou can just copy an example of a YAML block from here. Please note that you still have to Insert a block and then copy the code from below between the lines.\n\n\nYAML_block_example\n\ntitle: \"Chapter 1 - Assignment\"\ntoc: true\nformat:\n  html:\n    embed-resources: true\noutput: html\n\n\n\n\n5) Just like in your R scripts, you can run code in these chunks. Note that it is common practice to only have one output per chunk (for example a plot or a result). Just create a new chunk for the next output. To run your code and check if it works, there are several options on the top right of each chunk or at the top right of the source editor.\n\n\n\n\n\n6) As a last step, when all your code needed to solve the assignment is in code chunks, you have to click Render, which will create a .html document in the same folder. This document contains your solution and will later be submitted. If you are asked to install necessary packages, you have to install them.\n\n\nFor example, the document just created looks like this. There are so many more options that you can use in .qmd files that we cannot cover here. However, if you are interested have a look at the documentation and check some features out. By the way, this whole website is generated based on .qmd files.\n\n\n\nWhen you have generated all your .html document, you’ll see them in your project folder. Check if all of your code has run and all your output is shown correctly. If you have any problems, please don’t hesitate to ask in class or in the Mattermost channel.\nPlease move them to one one folder that you name: cds_YourKerberosID, for example: cds_cxy0428, if your KerberosID is cxy0428.\nThen, proceed by going to the StudIP course for this class and click on the files tab. There you’ll find a folder called Submission. In this folder you have to upload your final folder with your HTML-files. Don’t forget to name it correctly so I can match it with you.\n\n\n\n\n\n\nSummary: How to successfully submit\n\n\n\n\nWrite your solutions down in a .qmd file and render an HTML document with the YAML-block as given above.\nPut all your HTML-documents in one folder and name it: cds_YourKerberosID, for example: cds_cxy0428. Convert your folder to an archive file format (.zip, .rar, .gz etc.).\nUpload your compressed folder to StudIP (Folder: Submission)"
  }
]