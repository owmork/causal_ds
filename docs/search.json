[
  {
    "objectID": "content/toolbox/09_rdd.html",
    "href": "content/toolbox/09_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "The next tool we introduce is called regression discontinuity design (RDD). Fist used in 1960 (Campbell), it did not play a large role until 1999 and since then has experienced growing acceptance due to the advance of more rigorous requirements regarding credibility and causality in social sciences. Another factor that made many researchers use RDDs is the increased availability of digitized administrative data that is often linked to arbitrary rules that can be exploited to capture “as-if” randomization processes for treatment assignment.\nAssuming a data-generating process, where we have a variable \\(X\\) that is a confounder as it has an impact on treatment assignment \\(D\\) and the outcome \\(Y\\). Additionally, we could have an unobserved confounders between \\(X\\) and \\(Y\\).\nThen, as can be seen in the second DAG, the regression discontinuity design exploits the fact that \\(X\\) determines \\(D\\) and data is filtered such that there are only observations that were close to a cut-off value determining their treatment status. This way, treated and untreated units are very similar and comparable and RDD is able to eliminate selection bias for that subpopulation. Note that the treatment effect you calculate using this method is an average treatment effect for a subgroup rather than for the whole population.\n\\(X\\) is called the running variable and is a continuous variable assigning units to treatment \\(D\\) based on a cut-off score \\(c_0\\). Because it has an impact on \\(Y\\) as well, it is a confounder and opens a backdoor path. Now, the problem is that due to the cut-off determining the treatment \\(D\\), the backdoor cannot be closed with regular ways like e.g. matching as there is no overlap, i.e. there are no treated and untreated units for all levels of \\(X\\).\n!!! Summarizing:\n\nRunning/forcing variable: …\nCut-off/threshold: …\n\n\nlibrary(tidyverse)\nlibrary(ggdag)\nlibrary(dagitty)\n# Initial situation\ndag_model_1 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> X\nU -> Y\nX -> D\nX -> Y\n}\n')\n \n# Directed Acyclic Graph\ndag_1 <- ggdag_status(dag_model_1) +\n  guides(color = \"none\") +\n  theme_dag()\n# Exploiting cut-off\ndag_model_2 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX_c [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> Y\nX_c -> D\n}\n')\n \n# Directed Acyclic Graph\ndag_2 <- ggdag_status(dag_model_2) +\n  guides(color = \"none\") +\n  theme_dag()\ngridExtra::grid.arrange(dag_1, dag_2, ncol = 2)\n\n\n\n\n\n\n\n\nTherefore, as the second graph shows, the causal effect is identified by analyzing only observations that are in close neighborhood to \\(c_0\\). The identified treatment effect is the local average treatment effect (LATE).\n\\[\nLATE_{RDD} = E[Y_1 - Y_0| X \\rightarrow c_0]\n\\]"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#research-design",
    "href": "content/toolbox/09_rdd.html#research-design",
    "title": "Regression Discontinuity",
    "section": "Research Design",
    "text": "Research Design\nRDDs are quite intuitive and very graphical. For this reason, we will go through estimation and inference using an application and explain at each step what has to be considered.\nIn the application, we want to analyze the effect of being graded “very good” instead of “good” on a restaurant review site. Customers leave reviews at this site for restaurants they have visited and the resulting average score (from 1 to 10) is phrased with a label. Restaurants with a score \\([8,9)\\) will receive the label “good”, while restaurants with a score of \\([9, 10]\\) receive the label “very good”.We will only focus on restaurants with a score larger than 8.\nYou might already see how we can use RDD in this application to identify the effect of a “very good” label on revenue. We can exploit the rule that there is a sharp cut-off for restaurants having a score just above or below 9. That means our cut-off value is \\(c0 = 9\\).\nLet’s have a look at what the data looks like.\n!!! INCLUDE COVARIATES?\n\nhead(df)\n\n\n\n  \n\n\n\n!!! DAG?\nAt first glance, it looks like all restaurants below the cut-off don’t have a “very good” label (indicated by FALSE) and the restaurants above cut-off do have it. We can visualize if that applies to all restaurants. As we have expected, to the left of the cut-off and to the right of the cut-off, there is always just one label type. It means, we are dealing with a sharp cut-off.\n\n# [2] Visualization ----\n# [2.1] Compliance ----\n# As expected, perfect \"compliance\" and sharp cutoff. All \n# restaurants below the cutoff get a \"good\" rating, while all restaurants above\n# the cutoff get a \"very good\" rating.\ncompl <- \n  ggplot(df, aes(x = user_rating, y = rating_label, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[4]) +\n  geom_point(alpha = 1000/n, position = position_jitter(width = NULL, height = 0.05)) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Good\", \"Very Good\"))+\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"\") +\n  theme(legend.title = element_blank())\ncompl"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#random-assignment",
    "href": "content/toolbox/09_rdd.html#random-assignment",
    "title": "Regression Discontinuity",
    "section": "Random assignment",
    "text": "Random assignment\nAs already mentioned, for RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is no decline or incline at the cut-off and therefore can assume that the continuity assumption holds.\n\n# [2.2] Random assignment test ----\n# identifying assumption: random assignment to either side of cut-off\n# Manual plot\nggplot(df, aes(x = user_rating, fill = label)) +\n  geom_histogram(binwidth = .1, color = \"white\", boundary = c0, alpha = .6) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5], size = 2, linetype = \"solid\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package. It relies on a test to check the assumption. as the p-value is large, we can reject the null hypothesis that the number of units at either side are different.\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd <- rddensity(df$user_rating, c = c0)\nsummary(rddd)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       5000\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 9                 Left of c           Right of c          \nNumber of obs         2488                2512                \nEff. Number of obs    709                 840                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.178               0.199               \n\nMethod                T                   P > |T|             \nRobust                0.5829              0.5599              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          <c     >=c    P>|T|\n0.003                       8      12    0.5034\n0.006                      18      22    0.6358\n0.008                      27      35    0.3742\n0.011                      37      46    0.3800\n0.014                      50      58    0.5008\n0.017                      60      72    0.3384\n0.019                      79      85    0.6963\n0.022                      89      96    0.6592\n0.025                     100     111    0.4913\n0.028                     118     122    0.8465\n\n\nIt can also be shown graphically, where you can see that the confidence intervals overlap. If they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to get valid results.\n\n# Visually check continuity at running variable\nrdd_plot <- rdplotdensity(rddd, df$user_rating, plotN = 100)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\nℹ The deprecated feature was likely used in the lpdensity package.\n  Please report the issue to the authors."
  },
  {
    "objectID": "content/toolbox/09_rdd.html#plot-treatment-effect",
    "href": "content/toolbox/09_rdd.html#plot-treatment-effect",
    "title": "Regression Discontinuity",
    "section": "Plot Treatment Effect",
    "text": "Plot Treatment Effect\nHaving checked potential violations of the continuity assumptions, we can move on and estimate the treatment effect. We start with selecting a bandwidth, i.e. we select what and how many observations should be compared. The larger the bandwidth, the more observations are taken into consideration but it also reduces the comparability. On the other hand, choosing a small bandwidth results in a lower number of observations while ensuring similarity and comparability.\nThere is no safe rule how to best select the bandwidth, although there are algorithms attempting to look for the optimal bandwidth. For now, we just use common sense and select a bandwidth of 0.15, resulting in an analysis window \\([8.85, 9.15]\\), which still leaves us with about 1000 observations.\n\n# [3] Dependent variable ----\n# [3.1] Average Treatment Effect ----\n# Plot regression lines for full and specified bandwidth.\n# Specify bandwidth\nbw <- c0 + c(-0.15, 0.15)\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(user_rating %>% between(bw[1], c0))\ndf_bw_above <- df %>% filter(user_rating %>% between(c0, bw[2]))\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n[1] 1268    6\n\n\nTo illustrate the difference between using only a small window and all data, we plot the resulting regression lines. You can see that both approaches would lead to different results.\n\n# Plot dependent variable vs running variable\ndep_var <-\n  ggplot(df, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_vline(xintercept = bw[1], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_vline(xintercept = bw[2], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_point(alpha = 1000/n, size = 0.5) +\n  # add lines for the full range\n  geom_smooth(data = filter(df, user_rating <= c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(df, user_rating > c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  # add lines for specified bandwidth\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom the plot above, it is hard to see what the difference between observations close to the cut-off is. So what we can do is to compute to regressions, one for the observations in \\([8.85, 9)\\) and another one for the observations in \\([9,9.15]\\).\nThen, using the resulting coefficients, we compute what both models predict for the cut-off value \\(c0\\) and take the difference. The difference is the local average treatment effect (LATE).\n\n# [3.2] Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(revenue ~ user_rating, df_bw_below)\nmodel_bw_above <- lm(revenue ~ user_rating, df_bw_above)\ny0 <- predict(model_bw_below, tibble(user_rating = c0))\ny1 <- predict(model_bw_above, tibble(user_rating = c0))\nlate <- y1 - y0\nlate\n\n 1 \n25 \n\n\nIt’s a bit messy when we plot all observations, so let’s zoom in to see if we can detect the local average treatment effect graphically. Not surprisingly, it is equal to what we have just computed.\n\n# Minimum and maximum for y-axis limits\nmin_y <- min(df_bw$revenue)\nmax_y <- max(df_bw$revenue)\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw <- \n  ggplot(df_bw, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.2, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  theme(legend.position = \"bottom\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y0, yend = y0),\n             linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  annotate(\"text\", x = c0 - 0.05, y = mean(c(y1, y0)),\n           label = sprintf(\"Difference: %.2f\", late), fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#estimate-treatment-effect",
    "href": "content/toolbox/09_rdd.html#estimate-treatment-effect",
    "title": "Regression Discontinuity",
    "section": "Estimate Treatment Effect",
    "text": "Estimate Treatment Effect\n\nParametric Estimation\nWhat you will see most in studies is a regression to compute the LATE. Here, we prefer to use user_rating_ct, which is the raw user_rating variable centered, i.e. subtracted by the cut-off value \\(c0\\). That simplifies the interpretation, however, it does not change the coefficient of interest, the LATE.\nThe coefficient we are most interested in is the one for rating_labelTRUE. It is equal to the effect in the plot above, but the regression summary also yields additional statistical information. We see that the LATE is statistically significant.\n\n# [4] Estimation ----\n# [4.1] Parametric ----\n# Compute coefficients for specified bandwidth.\nparam_bw <- lm(revenue ~ user_rating_ct + rating_label, df_bw)\nsummary(param_bw)\n\n\nCall:\nlm(formula = revenue ~ user_rating_ct + rating_label, data = df_bw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-106.67  -28.59    0.19   28.25  103.00 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         90.00       2.36   38.18  < 2e-16 ***\nuser_rating_ct       2.84      25.62    0.11     0.91    \nrating_labelTRUE    24.96       4.31    5.80  8.5e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38 on 1265 degrees of freedom\nMultiple R-squared:   0.1,  Adjusted R-squared:  0.0987 \nF-statistic: 70.4 on 2 and 1265 DF,  p-value: <2e-16\n\n\n\n\nNon-parametric Estimation\nYou can also use non-parametric estimation techniques, i.e. we do not fit a line through the data but instead a curve. The R package rdrobust is a good resource that is easy to use. We just have to provide the dependent variable, the running variable and the cut-off value.\nBy default, the rdrobust() function automatically selects a bandwidth and uses a kernel weighting function (triangular kernel).\n\n# [4.2] Non-parametric ----\nlibrary(rdrobust)\nnparam_bw <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, c = c0)\nsummary(nparam_bw)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             214          207\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.048        0.048\nBW bias (b)                   0.076        0.076\nrho (h/b)                     0.634        0.634\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    29.892     8.322     3.592     0.000    [13.582 , 46.203]    \n        Robust         -         -     3.111     0.002    [11.487 , 50.600]    \n=============================================================================\n\n\nTo check how the curve goes through the data, we can use rdplot(), which splits the data into bins and for each bin shows an average data point. We see that the curve fits the data very well.\n\nrdplot(y = df$revenue, x = df$user_rating, c = c0)\n\n\n\n\n\n\n\n\nThere are a lot of options with the rdrobust() function, e.g. we can change the kernel and see how it affects the result.\n\n# Use other kernel\nnparam_bw_kernel <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, \n                             c = c0, kernel = \"epanechnikov\")\nsummary(nparam_bw_kernel)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Epanechnikov\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             204          199\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.046        0.046\nBW bias (b)                   0.075        0.075\nrho (h/b)                     0.606        0.606\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    30.892     8.286     3.728     0.000    [14.652 , 47.132]    \n        Robust         -         -     3.257     0.001    [12.846 , 51.681]    \n============================================================================="
  },
  {
    "objectID": "content/toolbox/08_iv.html",
    "href": "content/toolbox/08_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "The method we introduce in this chapter is called instrumental variables estimation (IV) and is a non-experimental methods used to identify causal effects in observational studies with unobserved confounders.\nHeavily used in economics and probably one of the most important research designs, it controls for omitted variable bias. Not being able to close the backdoor path through the omitted variable between treatment and outcome due to the omitted variable not being measured, IV introduces an additional variable, the so called instrument or instrumental variable,which affects the outcome only through the treatment variable.\nThe instrumental variable is exogenous, i.e. there is no other variable in the mode influencing the value of the instrument. Thus, it mimics an experiment by exploiting the exogenous variation in treatment due to the instrument and disregarding endogenous variation from unobserved confounders.\nIV can be illustrated using DAGs. On the left, there is a potential initial situation you could find yourself in: you want to examine the effect of \\(D\\) on $Y$, but unfortunately, there is an unobserved confounder (=omitted variable) that you would have to adjust for to identify the direct effect. As \\(U\\) is unobserved, there is no way to close the backdoor path by methods like matching, regression etc. This is where IV comes to rescue.\nAs you can see on the right, now \\(D\\) mediates between an instrument \\(Z\\) and outcome \\(Y\\). There is no direct path between \\(Z\\) and \\(Y\\) and therefore \\(Z\\) affects \\(Y\\) only through \\(D\\). For the instrument validity, there are a few assumptions that need to be fulfilled, which we discuss later in detail but summarizing, we need\n\nRelevance: \\(Z \\rightarrow D, \\,\\,\\, Cor(Z,D) \\neq 0\\) (testable)\nExcludability: \\(Z \\rightarrow D \\rightarrow Y,\\,\\, Z \\not\\to Y, \\,\\,\\, Cor(Z, Y|D) =0\\) (partly testable)\nExogeneity: \\(U \\not\\to Z, \\,\\,\\, Cor(Z, U)=0\\) (not testable)\n\n\n\n\n\n\n\n\n\n\nThat are the main requirements of IV. To go through them in-depth and also explain how to estimate the effects, we’ll go through an example in the following section.."
  },
  {
    "objectID": "content/toolbox/08_iv.html#exploration",
    "href": "content/toolbox/08_iv.html#exploration",
    "title": "Instrumental Variables",
    "section": "Exploration",
    "text": "Exploration\nFor the sake of explanation, we generate a synthetic data set with the variables \\(Z\\), \\(D\\) and \\(Y\\) as defined above. We also include the unobserved variable \\(U\\). This way, we can better explain where the bias comes from and how it affects the estimated treatment effect. The true treatment effect, which is the direct effect of \\(D\\) on \\(Y\\), is set to 1. We also add some random noise, so relationships are not perfect and what you will see is close to practice.\n\nhead(df)\n\n\n\n  \n\n\n\nFrom the table, you already have an idea what the data structure and types look like. But when developing our analysis strategy, we are mainly interested in relationships between the variables, so let’s have a look at the correlation matrix. Some requirements for a valid IV strategy can already be checked with it. Please also note, that we include \\(U\\) here, but in general, you do not observe \\(U\\), which is the reason why researchers came up with the idea of IV in the first place. It is just for the purpose of explanation that we include it here.\nFrom the correlation matrix, we can take a few important insights. There is a significant negative correlation between distance and program participation, which is also called first-stage and confirms the relevance of our instrument. The distance to the next training location does affect decision to participate in a training. This is assumption (4) from above.\nHaving a synthetic data set, we can also see that our instrument is uncorrelated with the unobserved variable motivation, which means there is no confounding as stated in assumption (2). Usually, however, we would not be able to test this assumption due to an unobservable variable not being observed. Moreover, there could be additional confounders. With merely statistical concepts, we cannot prove that the effect of \\(Z\\) on \\(Y\\) goes only through \\(D\\).\nCorresponding to our DAG, we also see that there both program participation and KPI are correlated with motivation, which thus opens another path between treatment and outcome. Being unable to close this path, we actually need an instrument in this situation.\n\n# Correlation matrix\ncor(df) %>% round(2)\n\n           distance program motivation   kpi\ndistance       1.00   -0.56      -0.02 -0.23\nprogram       -0.56    1.00       0.41  0.48\nmotivation    -0.02    0.41       1.00  0.30\nkpi           -0.23    0.48       0.30  1.00\n\n\nIn general, it is very useful to also plot the data to check relationships between variables. It confirms what we have seen in the correlation matrix."
  },
  {
    "objectID": "content/toolbox/08_iv.html#modeling",
    "href": "content/toolbox/08_iv.html#modeling",
    "title": "Instrumental Variables",
    "section": "Modeling",
    "text": "Modeling\n\nConfounding\nWhen you plausibly argued that your instrument is valid, you would usually perform 2SLS, which is short for Two Stage Least Squares. We’ll come to that shortly, but because we have all the data, we can also check what the bias would be in case we would not use an instrument and ignore the confounder.\nRemember, the true treatment effect is 1 in this case. Therefore, when we regress \\(Y\\) on \\(D\\) and \\(U\\), we should be able to recover this effect. And when you look at the regression output, in fact, we do. It is not exactly equal 1, but that is due to sampling noise.\n\n# First of all, let's look at the coefficients of the \"full\" (but unobservable)\n# model. It is unobservable, as it includes motivation, which in reality is \n# a variable that is very hard to collect or measure.\n# Coefficients are expected to be close to what he have defined in the data\n# generation section.\nmodel_full <- lm(kpi ~ program + motivation, data = df)\nsummary(model_full)\n\n\nCall:\nlm(formula = kpi ~ program + motivation, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.572 -0.680 -0.005  0.674  3.491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.0590     0.0564   -1.05      0.3    \nprogram       0.9898     0.0284   34.91   <2e-16 ***\nmotivation    1.1313     0.1086   10.41   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5997 degrees of freedom\nMultiple R-squared:  0.245, Adjusted R-squared:  0.245 \nF-statistic:  973 on 2 and 5997 DF,  p-value: <2e-16\n\n\nBut what would happen if we ignored the confounder \\(motivation\\) and only regress \\(Y\\) on \\(D\\)? We do not close the backdoor path and consequently, the effect is overestimated.\nWe can see that the coefficient for program participation is higher than expected, i.e. it has an upward bias. This is because, it takes some of the variation that is actually attributable to motivation into the coefficient of program participation. This confirms the need to include an instrument to model causal effects when there is no way to include the confounder.\n\n# Modeling the data without the unobservable variable, i.e. only including \n# program participation in this case, returns a biased coefficient as the \n# relationship between program and the outcome is biased by a collider.\nmodel_biased <- lm(kpi ~ program, data = df)\nsummary(model_biased)\n\n\nCall:\nlm(formula = kpi ~ program, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.631 -0.688 -0.007  0.688  3.658 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4963     0.0185    26.9   <2e-16 ***\nprogram       1.1101     0.0261    42.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5998 degrees of freedom\nMultiple R-squared:  0.231, Adjusted R-squared:  0.231 \nF-statistic: 1.8e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\n\n\n2SLS\nTwo Stage Least Square is the estimation technique for IV and consists, as it names suggest, of two stages. In the first stage, the treatment variable is regressed on the instrument and in the second stage, the estimated values of the first stage are used as a regressors for the outcome. It sounds a little bit confusing, so let’s write it down. Remember, \\(D\\) is our treatment, \\(Z\\) the instrument and \\(Y\\) the outcome.\nFirst stage:\n\\[\nd_i = \\gamma_0 + \\gamma_1z_i + \\nu_i\n\\]\nSecond stage:\n\\[\ny_i = \\beta_0 +\\beta_1\\widehat{d_i} +\\epsilon_i\n\\]\nLower cases indicate single observations, so \\(i\\) indicates for example the row in our data set. What is important to note, is that in the second stage, we do not use \\(d_i\\), but instead \\(\\widehat{d_i}\\). The hat indicates, that these are fitted values from the first stage.\nWe can do 2SLS manually in R, but for reasons I will get to later, it is recommended to use libraries built to run 2SLS. However, for purpose of explanation, we’ll do it also manually here.\nFirst stage: As already discussed, regress treatment variable on instrument and obtain the fitted model. The model coefficient returned by the model summary should be significant, otherwise there is reason to doubt the relevance and validity of the instrument. You can also look at the F-statistic, which should be above 10.\nHere, the instrument is highly significant. The higher the distance, the lower the likelihood of participation.\n!!! log reg?\n\n# First stage\nfirst_stage <- lm(program ~ distance, data = df)\nsummary(first_stage)\n\n\nCall:\nlm(formula = program ~ distance, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2004 -0.3334 -0.0373  0.3509  1.0397 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.4553     0.0192    75.8   <2e-16 ***\ndistance     -1.5999     0.0309   -51.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.42 on 5998 degrees of freedom\nMultiple R-squared:  0.309, Adjusted R-squared:  0.309 \nF-statistic: 2.68e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\nLet’s look at the fitted values from the first stage. The fitted values is what you get when you use the calculated coefficients and for each observation compute what the model predicts as an expected value. So, they are most likely a different from the actual values. How different they are depends on the goodness of fit of your model. This is why it is important that your instrument has a good explanatory value for the treatment variable.\n!!! Plot residuals?\n\nhist(first_stage$fitted.values)\n\n\n\n\n\n\n\n\n\nggplot(tibble(first_stage_fitted = first_stage$fitted.values), \n       aes(x = first_stage_fitted)) + \n  geom_histogram(color = \"white\", alpha = .8)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow we continue to use the fitted values from the first stage and plug it in the second stage to get the local average treatment effect. We see that the coefficient for the effect is close to one as constructed and we were able to eliminate the omitted variable bias.\n\n# Second stage\nsecond_stage <- lm(df$kpi ~ first_stage$fitted.values)\nsummary(second_stage)\n\n\nCall:\nlm(formula = df$kpi ~ first_stage$fitted.values)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.051 -0.757 -0.009  0.753  4.257 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 0.5669     0.0298    19.0   <2e-16 ***\nfirst_stage$fitted.values   0.9689     0.0521    18.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.1 on 5998 degrees of freedom\nMultiple R-squared:  0.0545,    Adjusted R-squared:  0.0543 \nF-statistic:  345 on 1 and 5998 DF,  p-value: <2e-16\n\n\nHowever, it is recommended to use functions, like e.g. iv_robust() from the estimatr package, as it yields correct standard errors. You see that the coefficient is the same but the standard errors slightly differ.\n\n# Using our instrument (distance to training location), we try to eliminate the\n# bias induced by the omitted variable. If all assumptions regarding the validity\n# of our instrument are met, the resulting coefficient should be\n# close to what we have defined above.\nlibrary(estimatr)\nmodel_iv <- iv_robust(kpi ~ program | distance, data = df)\nsummary(model_iv)\n\n\nCall:\niv_robust(formula = kpi ~ program | distance, data = df)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper   DF\n(Intercept)    0.567     0.0267    21.2 1.13e-96    0.515    0.619 5998\nprogram        0.969     0.0466    20.8 7.23e-93    0.878    1.060 5998\n\nMultiple R-squared:  0.228 ,    Adjusted R-squared:  0.227 \nF-statistic:  433 on 1 and 5998 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/07_did.html",
    "href": "content/toolbox/07_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "The most popular research design in quantitative and social sciences is the difference-in-differences (DiD) approach. As it name suggests, the method captures differences by observing a treatment and a control group over time to estimate causal average effects.\nDiD provides a nonexperimental technique that, in its simplest form, compares two groups (control and treatment) at two points in time (before treatment and after) by observing if and how different both group’s outcome evolve.\nBy taking two differences, two different kind of biases should be avoided. First, by comparing both groups at both points in time, any external effect that affects the outcome through time should play no role as it affects both groups. Secondly, taking only the difference of change in consideration, selection bias is eliminated and potential outcomes can vary.\nAs can be seen in the table, the difference in outcome for the treatment group before and after treatment is \\(D + T\\), while for the control group it is only \\(T\\). The difference of these two differences then reduces to only \\(D\\), which is the treatment effect we want to estimate.\n\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\nWe can also break it down in our known notation:\n\\[\nATE = \\bigg(E[Y_{D=1}|T=1] - E[Y_{D=1}|T=0] \\bigg)- \\bigg(E[Y_{D=0}|T=1] - E[Y_{D=0}|T=0]\\bigg)\n\\]\nBecause there are a lot of subscripts, it can also help to write down the formula in pseudo-math:\n\\[\nATE = (Y_{Treatment, After} - Y_{Treatment, before}) - (Y_{Control, After} - Y_{Control, Before})\n\\]\nOpposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential otucomes \\(E[Y_0|D=1] = E[Y_0|D=1]\\) are equal. That is a big difference, because do not have to assume that observation units are similar in all their characteristics.\nInstead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but the change of outcomes from before to after. It implies that there is no other factor that has only an impact on just one of the groups. If units differ in characteristics, they are only allowed to have a constant effect. If the effect varies with time, the paralell trends assumption is violated."
  },
  {
    "objectID": "content/toolbox/07_did.html#parallel-trends",
    "href": "content/toolbox/07_did.html#parallel-trends",
    "title": "Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\n\nScenario A\nTo compute an estimated treatment effect, we filter the data to the two periods just around treatment and implement the formulas as in the introduction. Not surprisingly, we get an estimate that is very close to our true treatment effect.\n\n# [1.1.1] (a) Fulfillment ----\n# Scenario (a)\n# Only show last data point before and first data point after treatment.\ndf_zoom_in_a <- df %>% \n  filter(period %in% (P/2):(P/2+1)) %>%\n  mutate(store_discrete = as.factor(store)) %>% \n  rename(sales = sales_a)\n# Manually compute differences\n# Difference between treatment and control group BEFORE treatment\nbefore_control_a <- df_zoom_in_a %>%\n  filter(store == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_a <- df_zoom_in_a %>%\n  filter(store == 1, after == 0) %>% \n  pull(sales)\ndiff_before_a <- before_treatment_a - before_control_a\n# Difference between treatment and control group AFTER treatment\nafter_control_a <- df_zoom_in_a %>%\n  filter(store == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_a <- df_zoom_in_a %>%\n  filter(store == 1, after == 1) %>% \n  pull(sales)\ndiff_after_a <- after_treatment_a - after_control_a\n# Difference-in-differences. Unbiased estimate if parallel trends is correctly\n# assumed and there is no hidden confounding. Estimate may vary from true\n# treatment effect, as we also include some noise in the data generating \n# process.\ndiff_diff_a <- diff_after_a - diff_before_a\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_a, D)\n\n[1] \"Estimate: 1.05, True Effect: 1.00\"\n\n\nLooking at the last period before and the first period after treatment, the impact of treatment can clearly be seen. The dashed line represents the counterfactual value for the treated group, i.e. the value it would have if it had not been treated. This value is not observed, but by the paralell trends assumptions, it would have developed like the value for the untreated group.\n\n# Plot\nggplot(df_zoom_in_a, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name =\"\", \n                   breaks=c(5, 5.5, 6),\n                   labels = c(\"Before Treatment\", \n                              \"Treatment\",\n                              \"After Treatment\")) +\n  scale_y_continuous(name = \"Sales\", labels = scales::number_format(accuracy = 0.1)) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = ggthemr::swatch()[2], size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_a - (diff_diff_a / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.6, y = before_control_a + 1.1*diff_before_a + .1, \n            label = \"Counterfactual\", size = 4, \n           #angle = atan(after_control_a - before_control_a) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nScenario B\nRepeating the steps for scenario B yields an unexpected result. The estimated treatment effect is different from what we would have expected.\n\n# [1.1.2] (b) Violation ----\n# Scenario (b)\n# Only show last data point before and first data point after treatment.\ndf_zoom_in_b <- df %>% \n  filter(period %in% (P/2):(P/2+1)) %>%\n  mutate(store_discrete = as.factor(store)) %>% \n  rename(sales = sales_b)\n# Manually compute differences\n# Difference between treatment and control group BEFORE treatment\nbefore_control_b <- df_zoom_in_b %>%\n  filter(store == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_b <- df_zoom_in_b %>%\n  filter(store == 1, after == 0) %>% \n  pull(sales)\ndiff_before_b <- before_treatment_b - before_control_b\n# Difference between treatment and control group AFTER treatment\nafter_control_b <- df_zoom_in_b %>%\n  filter(store == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_b <- df_zoom_in_b %>%\n  filter(store == 1, after == 1) %>% \n  pull(sales)\ndiff_after_b <- after_treatment_b - after_control_b\n# Difference-in-differences. Unbiased estimate if parallel trends is correctly\n# assumed and there is no hidden confounding. Estimate varies from true\n# treatment effect due to confounding and added noise.\ndiff_diff_b <- diff_after_b - diff_before_b\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_b, D)\n\n[1] \"Estimate: 1.45, True Effect: 1.00\"\n\n\nAgain, the picture is very similar. Having only four data points, treatment before and after and control before and after, there is no way to test the paralell trends assumption which leaves room for doubt. So how can we check whether we made a mistake or the paralell trends assumption is violated?\n\n# Plot\nggplot(df_zoom_in_b, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name =\"\", \n                     breaks=c(5, 5.5, 6),\n                     labels = c(\"Before Treatment\", \n                                \"Treatment\",\n                                \"After Treatment\")) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_b - (diff_diff_b / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.6, y = before_control_b + 1.1*diff_before_b, \n           label = \"Counterfactual\", size = 4, \n           #angle = atan(after_control_b - before_control_b) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")"
  },
  {
    "objectID": "content/toolbox/07_did.html#event-study",
    "href": "content/toolbox/07_did.html#event-study",
    "title": "Difference-in-Differences",
    "section": "Event Study",
    "text": "Event Study\n\nScenario A\nMany researchers therefore try to increase the validity of their results by providing an event study. Not surprisingly (as we created the data ourselves), both groups develop same before the treatment. It cannot rule out all unobserved behavior but an event study lends credibility to the causal interpretation of treatment effects.\n\n# [1.2.1] (a) Fulfillment ----\n# Zoom out and show that parallel trend assumption is fulfilled in scenario (a)\ndf_zoom_out_a <- df %>% \n  mutate(store_discrete = as.factor(store)) %>% \n  filter(period <= (P/2 + 1)) %>% \n  rename(sales = sales_a)\ndiff_control <- after_control_a - before_control_a\nggplot(df_zoom_out_a, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name   = \"Period\", breaks = 1:(P/2+1)) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2),\n           xend = (P/2 + 1),\n           y = before_treatment_a,\n           yend = before_treatment_a + 1*(diff_control),\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.78), y = after_treatment_a - (diff_diff_a / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + .5, y = before_control_a + 1.1*diff_before_a - .1, \n           label = \"Counterfactual\", size = 4, \n           angle = atan(after_control_a - before_control_a) * 180/pi) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")\n\n\n\n\n\n\n\n\n\n\nScenario B\nPerforming the same steps for scenario B, you see the usefulness of an event study. Other than in scenario A, the paralell trends assumption does not seem to hold. It can be seen from the plot, that the estimated treatment effect is larger than the actual treatment effect. This is due to different trends in both groups. The treatment group has a more positive trend even without treatment and the groups would have further diverged after treatment (see green line). Some of the increase in sales after treatment is therefore attributable to this trends but not the treatment effect.\n\n# [1.2.2] (b) Violation----\n# Zoom out and show that parallel trend assumption is violated in scenario (b)\ndf_zoom_out_b <- df %>% \n  mutate(store_discrete = as.factor(store)) %>% \n  #filter(period <= (P/2 + 1)) %>% \n  rename(sales = sales_b)\ntreatment_period_1 <- df_zoom_out_b %>% \n  filter(store == 1, period == 1) %>% \n  pull(sales)\ntreatment_period_5 <- df_zoom_out_b %>% \n  filter(store == 1, period == P/2) %>%\n  pull(sales)\ndiff_treatment <- (treatment_period_5 - treatment_period_1) / (P/2)\nggplot(df_zoom_out_b, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name   = \"Period\", breaks = 1:P) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_b,\n           yend = before_treatment_b + (P/2)*(after_control_b - before_control_b),\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_b,\n           yend = before_treatment_b + (P/2)*(diff_treatment),\n           linetype = \"dashed\", color = \"green\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_b - (diff_diff_b / 2),\n           label = \"Estimated Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 3, y = before_control_b + 1*diff_before_b,\n           label = \"Counterfactual\", size = 4,\n           #angle = atan(after_control_b - before_control_b) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")"
  },
  {
    "objectID": "content/toolbox/07_did.html#modeling",
    "href": "content/toolbox/07_did.html#modeling",
    "title": "Difference-in-Differences",
    "section": "Modeling",
    "text": "Modeling\nA more typical situation is usually that there is more than one unit in the treatment and control group. You could e.g. imagine that you are managing more than two stores and are implementing an ad campaign in a specific region.\nTo simulate such a scenario, we generate data for 3’000 stores that are split evenly into two regions. In one region, the ad campaign will be run (treatment region) and in the other there will be no campaign (control region). The variable relationships as defined in the previous section still hold.\n\n# [1.4] Linear regression ----\n# Now assume that there are more than two stores and treatment is performed\n# e.g. in a specific region which are, depending on scenario (a) and (b) \n# different.\n# Generate a bunch of samples and combine in one table. Here, we choose a higher\n# standard deviation.\nn_stores <- 3e+3\ndf_lm    <- lapply(1:n_stores, function(R) generate_data(sd = 1)) %>% bind_rows()\ndf_lm    <- df_lm %>% filter(period %in% (P/2):(P/2+1))\n\n\nScenario A\nSo how do we compute the average treatment effect? Previously in this chapter, we just used basic math calculations (particularly subtraction). But there is an easier way: we can use regression again. This is because the average treatment effect is the coefficient of the interaction of group and time.\n\\[\ny_i = \\beta_0 + \\beta_1 * Period_i + \\beta_2 * Treatment_i + \\beta_3 * (Time_i * Treatment_i) + \\epsilon_i\n\\]\n\\(Time\\) indicates whether the period is before or after the treatment and \\(Treatment\\) whether an observation was treated or not. Then, the coefficient we are interested in is \\(\\beta_3\\), because its term is only active for the treated group after treatment.\n!!! x1: maybe purchase power in region\nFor scenario A, we can see that there is no need to adjust for the covariate \\(x1\\). If you check the formulas again, your will notice that \\(x1\\) has a constant and time-invariant effect on sales and therefore it does not violate the paralell trends assumption.\nIncluding or leaving out \\(x1\\) in the regression yields the a similar unbiased estimate (close to defined true size) for our variable of interest \\(store:after\\), the parameter of interest.\n\n# [1.4.1] (a) ----\n# (a): Due to the construction of the data set, we expect interaction\n# coefficient to be significant as well as the covariate and period. However, as\n# the covariate does not have a time-varying effect, it is not a confounder and\n# interaction coefficient should be unbiased even if not adjusting for the\n# covariate.\nsummary(lm(sales_a ~ store * after , data = df_lm))\n\n\nCall:\nlm(formula = sales_a ~ store * after, data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.177 -0.952  0.003  0.956  6.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.9620     0.0257 1981.04  < 2e-16 ***\nstore         1.0367     0.0364   28.50  < 2e-16 ***\nafter         0.2637     0.0364    7.25  4.5e-13 ***\nstore:after   0.9816     0.0514   19.08  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 11996 degrees of freedom\nMultiple R-squared:  0.284, Adjusted R-squared:  0.283 \nF-statistic: 1.58e+03 on 3 and 11996 DF,  p-value: <2e-16\n\nsummary(lm(sales_a ~ store * after + x1, data = df_lm))\n\n\nCall:\nlm(formula = sales_a ~ store * after + x1, data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.947 -0.680  0.001  0.673  4.205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.98165    0.01821 2799.89   <2e-16 ***\nstore        0.03882    0.02732    1.42     0.16    \nafter        0.22829    0.02575    8.86   <2e-16 ***\nx1           0.98742    0.00903  109.32   <2e-16 ***\nstore:after  0.97851    0.03642   26.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11995 degrees of freedom\nMultiple R-squared:  0.641, Adjusted R-squared:  0.641 \nF-statistic: 5.36e+03 on 4 and 11995 DF,  p-value: <2e-16\n\n\n\n\nScenario B\nIn scenario B, the effect of x1 is different because it has a time-varying effect. Therefore it violates the parallel trends assumption, leading to a biased estimate if x1 is not included (e.g. because it is unobserved).\nBecause we constructed the data set ourselves, we are able to see that the bias in fact is quite large and the treatment effect seems to include the actual treatment effect plus the effect of x1. Even with including \\(x1\\) and as a main effect and moderator, we cannot fully reconstruct the true treatment effect.\n\n# [1.4.2] (b) ----\n# (b): Due to the construction of the data set, we expect interaction coefficient\n# to be significant and accurate only when adjusting for the time-varying effect\n# of the covariate and main effects for period and covariate.\nsummary(lm(sales_b ~ store*after, data = df))\n\n\nCall:\nlm(formula = sales_b ~ store * after, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.059 -0.419  0.011  0.383  1.050 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   50.591      0.282  179.15  < 2e-16 ***\nstore          2.985      0.399    7.47  1.3e-06 ***\nafter          1.006      0.399    2.52  0.02275 *  \nstore:after    2.683      0.565    4.75  0.00022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.63 on 16 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.945 \nF-statistic:  109 on 3 and 16 DF,  p-value: 7.44e-11\n\nsummary(lm(sales_b ~ store*after + after*x1 + store*x1, data = df_lm))\n\n\nCall:\nlm(formula = sales_b ~ store * after + after * x1 + store * x1, \n    data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.227 -0.684  0.003  0.683  3.646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.99628    0.01833 2782.65  < 2e-16 ***\nstore       -0.00499    0.03024   -0.16     0.87    \nafter        0.19101    0.02592    7.37  1.8e-13 ***\nx1           2.63604    0.01576  167.23  < 2e-16 ***\nstore:after  0.99312    0.04103   24.21  < 2e-16 ***\nafter:x1     0.35500    0.01819   19.52  < 2e-16 ***\nstore:x1     1.03268    0.01819   56.77  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11993 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.943 \nF-statistic: 3.31e+04 on 6 and 11993 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/06_match.html",
    "href": "content/toolbox/06_match.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggdag)\nlibrary(dagitty)"
  },
  {
    "objectID": "content/toolbox/06_match.html#single-matching-variable",
    "href": "content/toolbox/06_match.html#single-matching-variable",
    "title": "Matching and Subclassification",
    "section": "Single Matching Variable",
    "text": "Single Matching Variable\nProbably the most simple method, though rarely applied in practice, is matching on a single variable. Its rare use in practice is due to the fact that matching on a single variable is only applicable if there is only one backdoor path that can be closed by this matching variable.\nBut for ease of explanation, we’ll have a look at it.\n\nconfounder <- 'dag {\nD [exposure,pos=\"0.000,0.000\"]\nY [outcome,pos=\"2.000,0.000\"]\nZ [pos=\"1.000,1.000\"]\nZ -> D\nZ -> Y\nD -> Y\n}'\nggdag(confounder) + \n  annotate(\"text\", x = 0.5, y = 1, label = \"Z fixed at constant level\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n!!! EXAMPLE:\ncredit card holders, monthly bills, repayment status (pay on time, delay payment) April - September\n\nbeing late April <- size of bill in April -> (Y) being late Sept\n\n\n(Coarsened) Exact Matching\n!!! pick one observation. define distance. single or multiple comparables.\nWe are trying to select observations in the control group that are similar to those in the treated group. To do so, we need to define what similar means. Here, we will say that similar observations are observations that share similar values for our matching variable (size of bill in April). By enforcing treatment and control group to have little variation in the matching variable, we close the backdoor. Only when the backdoor variable varies, it can induce changes in treatment and outcome and when we keep it at a certain level, the only effect from treatment to outcome is the direct effect.\nHow many? With our without replacement? Tradeoff between bias and variance (more matches, less variance and more bias) (Matching with replacement less bias because better matches and is not order-dependent)\n\none-to-one matching: selecting the best match\ntop \\(k\\) matching: k-nearest-neighbor matching\nall “acceptable” matches: every match by some metric of acceptability (hard to define).\n\n\n\nMatched weighted sample\nNow we will move away from a an observation being in or out depending on whether it is a best match to some unit. Instead we use an alternate approach that works by looking at all control observations and checking how close they are to treated observations. An observation is neither in our out but receives a weight depending on its similarity to a treated observation. The closer a control observation is to a treatment observation, the higher its weight.\nHow to construct weights?\n\nKernel matching: kernel function takes difference and returns weight. Weight eventually gets to zero. Better matches obtain higher weights than less-good matches, bad matches obtain weight of zero. Different kinds of kernels.\nInverse probability weighting: specifically designed for use with propensity scores. Each observation is weighted by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight.\n\n!!! PLOT: One-to-one matching (color of controls) vs weight matching (size of controls)\nComparison: selecting matches more intuitive and easier to implement but more sensitive. weights to account for quality of matches"
  },
  {
    "objectID": "content/toolbox/06_match.html#application-multiple-matching-variables",
    "href": "content/toolbox/06_match.html#application-multiple-matching-variables",
    "title": "Matching and Subclassification",
    "section": "Application: Multiple Matching Variables",
    "text": "Application: Multiple Matching Variables\nLet us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.\nNow you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age.\nWe can use a DAG to think about the correct identification strategy.\n\n# define DAG\ndag_model <- 'dag {\n  bb=\"0,0,1,1\"\n  \"Health Program\" [exposure,pos=\"0.25,0.2\"]\n  \"Initial Health Status\" [pos=\"0.35,0.25\"]\n  \"Sick Days\" [outcome,pos=\"0.35,0.2\"]\n  Age [pos=\"0.25,0.25\"]\n  \"Initial Health Status\" -> \"Health Program\"\n  \"Initial Health Status\" -> \"Sick Days\"\n  Age -> \"Health Program\"\n  Age -> \"Initial Health Status\"\n  Age -> \"Sick Days\"\n}'\n# Directed Acyclic Graph\nggdag_status(dag_model, text = FALSE, use_labels = \"name\") +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n# Directed Acyclic Graph with adjustment sets\nggdag_adjustment_set(dag_model, shadow = T, use_labels = \"name\", text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag()\n\n\n\n\n\n\n\n\nThere are two backdoor paths that we need to close, initial health status and age.\n\nadjustmentSets(dag_model)\n\n{ Age, Initial Health Status }\n\n\nA naive estimate would be\n\n# Naive estimation (not accounting for backdoors)\nmodel_naive <- lm(sick_days ~ health_program, data = df)\nbroom::tidy(model_naive)\n\n\n\n  \n\n\n\nwhich is different from the !!! TRUE TREATMENT EFFECT.\n\n(Coarsened) Exact Matching\nIt is not difficult to extend the approaches defined above to multiple matching variables. Regarding the “selecting matches” approach, multiple variables are used for the computations instead of a single. Again, in case of exact matching, only observations that share the same values are matched in.\nTo perform Coarsened Exact Matching (CEM) you can use the MatchIt package in R. If you do not specify how to coarsen the data, it will be done automatically based on an algorithm.\n\nlibrary(MatchIt)\n# Without specifying coarsening\n# (1) Matching\ncem <- matchit(health_program ~ age + sick_days_before,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n# Covariate balance\nsummary(cem)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\")\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.36       1.39     0.033    0.183\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.5           0.001       1.00     0.002    0.009            0.10\nsick_days_before           3.8           3.8           0.021       0.97     0.002    0.042            0.22\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    4600    4825\nMatched          4914    5046\nUnmatched           4      36\nDiscarded           0       0\n\n\nWe are already closer to our true treatment effect.\n\ndf_cem <- match.data(cem)\n# (2) Estimation\nmodel_cem <- lm(sick_days ~ health_program, data = df_cem, weights = weights)\nbroom::tidy(model_cem)\n\n\n\n  \n\n\n\nWe can also provide values to coarsen the data in order to control for the number of subsamples. Again, we also check the balance.\n\n# Custom coarsening\n# (1) Matching\ncutpoints <- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))\ncem_coars <- matchit(health_program ~ age + sick_days_before,\n                     data = df, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n# Covariate balance\nsummary(cem_coars)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.36       1.39     0.033    0.183\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.4           0.025        1.0     0.008    0.025            0.48\nsick_days_before           3.9           3.7           0.124        1.1     0.012    0.094            0.58\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    4762    4926\nMatched          4916    5080\nUnmatched           2       2\nDiscarded           0       0\n\n\nWe can also visualize the subsamples and see how data points are weighted.\n\ndf_cem_coars <- match.data(cem_coars)\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sick_days_before,\n                         size = weights, color = as.factor(health_program))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sick_days_before),\n              mapping = aes(intercept = y, slope = 0)) +\n  geom_vline(data.frame(y = cutpoints$age),\n              mapping = aes(xintercept = y)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow, with fewer subsamples, the estimate is worse. It seems, the backdoors are not properly closed.\n\n# (2) Estimation\nmodel_cem_coars <- lm(sick_days ~ health_program, data = df_cem_coars, \n                      weights = weights)\nbroom::tidy(model_cem_coars)\n\n\n\n  \n\n\n\n\n\nNearest-Neighbor matching\nFor nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular techniques used to find so called “nearest neighbors” is the euclidean distance.\nWe just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching.\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn <- matchit(health_program ~ age + sick_days_before,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\n# Covariate Balance\nsummary(nn)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"nearest\", distance = \"mahalanobis\", replace = T)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.33       1.39     0.033    0.183\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       46.4          46.4          -0.001          1     0.001    0.005           0.009\nsick_days_before           4.2           4.2           0.001          1     0.000    0.000           0.002\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    1783    5082\nMatched          2693    5082\nUnmatched        2225       0\nDiscarded           0       0\n\n\nThis method also brings us closer to the true treatment effect.\n\ndf_nn <- match.data(nn)\n# (2) Estimation\nmodel_nn <- lm(sick_days ~ health_program, data = df_nn, weights = weights)\nbroom::tidy(model_nn)\n\n\n\n  \n\n\n\n\n\nCurse of dimensionality\nWith exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don’t find any matches at all.\nRegarding exact matching, consider for example the situation with two covariates with each five different values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates ~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations.\nNearest-neighbor matching is similarly affected by the curse of dimensionality. The more covariates you include, the less likely you are to find a good match. A quick solution, leaving out some covariates, can only be done if you are sure to not throw out any confounders.\n\n\nInverse Probability weighting\nOne way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let’s go into more detail.\nWe start by understanding what “probability” in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as “propensity score”.\nHere, we will use a logistic regression:\n\n# (1) Propensity scores\nmodel_prop <- glm(health_program ~ age + sick_days_before,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nbroom::tidy(model_prop)\n\n\n\n  \n\n\n\n\n# Add propensities to table\ndf_aug <- broom::augment_columns(\n  model_prop, \n  df,\n  type.predict = \"response\") %>% \n  rename(propensity = .fitted)\n\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHaving obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as propensity score matching. However, there are several reasons why this is not a good identification strategy1, mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that same covariate values indeed imply the same propensity score.\nInstead inverse probability has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use propensity score of an observation unit to in- or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\\[\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n\\]\nwhere only one of the terms is always active as \\(D_i\\) is either one or zero. Now we should better understand what “inverse probability weighting” actually means. It weights each observation by its inverse of its treatment probability.\n\ndf_ipw <- df_aug %>% mutate(\n  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw)\n\n\n\n  \n\n\n\nImagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. \\(D_i=1\\). Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let’s assume it was 0.8, for demonstration. Then his/her weight would equal \\(w_i = \\frac{1}{0.8} = 1.25\\).\nCompared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let’s say his/her probability of participating would be 0.2. Then, his/her weight would be \\(w_i = \\frac{1}{0.2} = 5\\). So we see, he/she would obtain a significantly higher weight.\nIn general, IPW weights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group. If you want, you can check it out yourself.\n\n# (2) Estimation\nmodel_ipw <- lm(sick_days ~ health_program,\n                data = df_ipw, \n                weights = ipw)\nbroom::tidy(model_ipw)\n\n\n\n  \n\n\n\nSome propensity values are very extreme and therefore obtain an extreme weight. We should probably filter them out to improve our estimate.\n\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw) %>% \n  arrange(desc(ipw))\n\n\n\n  \n\n\nmodel_ipw_trim <- lm(sick_days ~ health_program,\n                data = df_ipw %>% filter(propensity %>% between(0.15, 0.85)),\n                weights = ipw)\nbroom::tidy(model_ipw_trim)\n\n\n\n  \n\n\n\nOpposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders.\n\n\nComparison\nYou see, that there many ways backdoors can be closed.\n\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                #\"All\"   = model_adj,\n                                \"CEM1\"  = model_cem,\n                                \"CEM2\"  = model_cem_coars,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n\n\n\n \n  \n      \n    Naive \n    CEM1 \n    CEM2 \n    NN \n    IPW1 \n    IPW2 \n  \n \n\n  \n    (Intercept) \n    7.260 \n    7.603 \n    7.517 \n    8.003 \n    7.853 \n    7.633 \n  \n  \n     \n    (0.033) \n    (0.033) \n    (0.033) \n    (0.047) \n    (0.037) \n    (0.032) \n  \n  \n    health_programTRUE \n    1.252 \n    0.522 \n    0.730 \n    0.510 \n    0.308 \n    0.492 \n  \n  \n     \n    (0.046) \n    (0.046) \n    (0.047) \n    (0.058) \n    (0.052) \n    (0.046) \n  \n  \n    Num.Obs. \n    10000 \n    9960 \n    9996 \n    7775 \n    10000 \n    9956 \n  \n  \n    R2 \n    0.068 \n    0.013 \n    0.024 \n    0.010 \n    0.003 \n    0.011 \n  \n  \n    R2 Adj. \n    0.068 \n    0.013 \n    0.024 \n    0.010 \n    0.003 \n    0.011 \n  \n  \n    AIC \n    45249.3 \n    44933.4 \n    45510.1 \n    36480.9 \n    47740.0 \n    44855.3 \n  \n  \n    BIC \n    45270.9 \n    44955.0 \n    45531.8 \n    36501.7 \n    47761.6 \n    44876.9 \n  \n  \n    Log.Lik. \n    −22621.658 \n    −22463.706 \n    −22752.070 \n    −18237.432 \n    −23866.986 \n    −22424.643 \n  \n  \n    RMSE \n    2.32 \n    2.28 \n    2.33 \n    2.42 \n    2.37 \n    2.22"
  },
  {
    "objectID": "content/toolbox/05_rct.html",
    "href": "content/toolbox/05_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Let’s recall the fundamental problem of causal inference: we are not able to observe individual treatment effects. Only one potential outcome can be observed because there is only one state of the world.\nArguably, the most promising way to deal with it is randomization of the observation units and in particular randomized experiments, also known as randomized controlled trials (RCTs). Due to their statistical rigor and simplicity, RCTs are called the gold standard of causal inference.\nRCTs do not solve the fundamental problem of only observing one potential outcome but instead treatment and control group are randomized such that both groups are expected to be very similar. Having similar groups that either received or not received treatment, we can calculate a valid causal estimate, the Average Treatment Effect (ATE). But it is only due to the randomization of observation units (e.g. individuals) that we are able to interpret it causally."
  },
  {
    "objectID": "content/toolbox/05_rct.html#identification",
    "href": "content/toolbox/05_rct.html#identification",
    "title": "Randomized Controlled Trials",
    "section": "Identification",
    "text": "Identification\nTwo assumptions are crucial for the ATE to be interpreted causally.\n\n(1) Independence assumption\nWe have to assume independence between the potential outcomes and the treatment assignment, i.e. treatment assignment to a unit hast nothing to do with the size of treatment effect for a unit:\n\\[\nD_i \\perp (Y_{i0}, Y_{i1})\n\\]\nThis is where we exploit randomization. We can actually ensure that there is no association between potential outcomes and treatment by randomly assigning observation units to control and treatment group.\nThis way, both groups will be very similar on average, both in observed and in unobserved characteristics. They will only differ in their treatment and possibly in the observed outcome, which makes the estimation of a causal effect possible.\nPlease make sure, that you understand the formula correctly. It does not mean that there is no treatment effect. The (potential) outcome under \\(D=0\\) or \\(D=1\\) is not affected by whether a particular observation unit does or does not receive the treatment. However, the observed outcome \\(Y_i\\) might depend on \\(D_i\\), and in fact, that is the effect we are interested in.\nA related way to express it is\n\\[\nE[Y_0|D=0] = E[Y_0|D=1]\n\\]\nRegardless of the treatment value a unit receives, the expected (but not always observed) potential outcome is the same in both treatment (\\(D=1\\)) and control group (\\(D=0\\)). The mean potential outcome is equal for both groups.\nThis does also imply equality of ATE and ATT, as there is no bias and the association we see is equal to the causation.\nWhen is the independence assumption violated?\nAn example, where the independence between treatment and potential outcomes is not given is if the treatment assignment is not randomized but people are able to self-select into on of the groups.\nThen, it could happen that for e.g. more motivated people would choose the treatment and when motivation had an impact on the potential outcome, e.g. more motivated people have a higher outcome for both potential outcomes compared to less motivated people, that are more likely to be in the control group. Under these circumstances, the independence assumption would be violated.\n\n\n(2) SUTVA\nThe second assumption that needs to be fulfilled is the stable unit treatment value assumption (SUTVA).\nIt ensures that there is no interference between units. In other words, one unit’s treatment does not affect outcomes of other units. If unit \\(i\\) received a treatment, than this treatment of unit \\(i\\) should have no effect on another unit.\nImplicitly, the assumption states that there are only two potential outcomes for each unit and they only depend on a unit’s own treatment status.\nWhen is the SUTVA violated?\nIn situations where observation units are somehow clustered like e.g. in classrooms, departments or other kind of groups, violations of SUTVA can occur.\nAs an example, imagine you are running a company and select a few of your employees to participate in a program that teaches them about safety measures. After the program, it is very likely that they share some of the program content with their colleagues in their department, who might not have been selected for participation. Then, there are spillover effects.\nTo deal with violations of SUTVA you could change your selection process or change the level of analysis (analyzing clusters instead of individuals)."
  },
  {
    "objectID": "content/toolbox/05_rct.html#randomization",
    "href": "content/toolbox/05_rct.html#randomization",
    "title": "Randomized Controlled Trials",
    "section": "Randomization",
    "text": "Randomization\nIn practice, randomization is done automatically by software programs but to get an intuition, you could also think of it as e.g. flipping a coin for each observation unit or individual and assigning units that get head to the treatment group, while units that get tail are assigned to the control group (or the other way around).\nIn fact, that is already a special case because the probability of being treated and being not treated is 50% for both cases. But treatment probabilities could also take different values for a variety of reasons, for example because treatment is costly. However, you need to ensure that both groups are large enough to be comparable in order to fulfill the independence assumption.\nLet’s see what that means. We assume that we have a population of 100’000 individuals which we want to learn something. Using runif() and rbinom, we synthetically generate this population with random value for the characteristics \\(age\\) and \\(sex\\).\n\nlibrary(tidyverse)\n# population size\nn <- 1e+5 \n# create population with two characteristics\nX <- tibble(\n  age = runif(n, 18, 65), # draw random values from uniform distribution\n  sex = rbinom(n, 1, 0.5) # draw random values from binomial distribution\n)\n# show first values\nhead(X)\n\n\n\n  \n\n\n\nUntil now, we have not assigned units to treatment and control group and actually, we do not want to assign our whole population to any group. As a matter of fact, in many applications, you are just able to draw a sample from the population and almost never the whole population.\nRemember, randomization of treatment should achieve that we are able to interpret the average treatment effect causally and for that, both groups need to be as similar as possible. The image illustrates the randomization process. Try to think what could happen if you have just very few units in both groups. How likely is is that they are very similar regarding their characteristics? You can probably already sense that this might not be sufficient to make groups comparable.\n\nBut let’s try it out and see how average group characteristics develop when we change the sample size.\nIn R, it is very easy to generate a random vector that we can use for randomization. Here, we wan to have a random vector that contains either 1 (treatment group) or 0 (control group) with a treatment probability of 50%. We can make use of the rbinom function that can randomly generate outcomes of a Bernoulli trial, which you can just imagine as flipping the coin \\(n\\) times.\nAs we have 100’000 people in our population, we will vary sample sizes from 100 to 100’000 to understand the impact sample size has.\n\n# vector of sample sizes\nsss <- c(50, 100, 500, seq(1000, 1e+5, 1000))\n# empty list to store average tables in\navg_tbl_age_lst <- list()\navg_tbl_sex_lst <- list()\ntbl_sampled_lst <- list()\n# for sample size in sample sizes\nfor (ss in sss) {\n  # sample from population\n  X_sampled <- sample_n(X, ss)\n  \n  # perform random assignment\n  D <- rbinom(ss, 1, 0.5)\n  \n  # combine characteristics and assignment in one table\n  tbl_sampled <- X_sampled %>% mutate(treatment = D)\n  \n  # store in list\n  tbl_sampled_lst[[paste(ss)]] <- tbl_sampled\n  \n  # get average characteristics ...\n  # ... for age\n  avg_tbl_age <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_age = mean(age)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"age\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_age) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_age_lst[[paste(ss)]] <- avg_tbl_age\n  \n  # ... for sex\n  avg_tbl_sex <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_sex = mean(sex)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"sex\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_sex) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_sex_lst[[paste(ss)]] <- avg_tbl_sex\n  \n}\n\nAs you can see in the plot, group average characteristics converge with increasing sample size. The more units are assigned to either group, the less differences are between the groups and thus, the independence assumption, stating that groups only differ by their treatment status, is fulfilled. But although you need a minimum amount of units, there is not much improvement after increasing the sample size way beyond that.\n\n# combine tables to one larger table\navg_age <- avg_tbl_age_lst %>% bind_rows()\navg_sex <- avg_tbl_sex_lst %>% bind_rows()\navgs_tbl <- avg_age %>% bind_rows(avg_sex)\n# plot convergence\nggplot(avgs_tbl, aes(x = sample_size, y = delta_abs)) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(x = \"sample size\", y = \"absolute difference\") +\n  ggtitle(\"Absolute difference of characteristics between groups by sample size\")"
  },
  {
    "objectID": "content/toolbox/05_rct.html#average-treatment-effect",
    "href": "content/toolbox/05_rct.html#average-treatment-effect",
    "title": "Randomized Controlled Trials",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nLet’s just use a sample size of 40’000 units. That means there should be about ~20’000 units per group. There are many suggested rules and guidelines to choose the right sample size, but for now, we will disregard it as our data is simulated and therefore, we do not have any data problems.\nSo far, we have just looked at the covariate balance but have not included the outcome variable. Let’s do that now. In the background we simulated the outcome after treatment and added the column outcome to our table.\n\nhead(df_out)\n\n\n\n  \n\n\n\nAs already mentioned, having balanced baseline characteristics between treatment and control group allows us to estimate the average treatment effect.\nBut how do we calculate the average treatment effect? We can just take a simple difference in means to estimate it. By the way, groups can be of different group size. It is only important, that they are comparable in their characteristics.\nLet’s compute the average outcome per group. We see that there seems to be a difference, the average outcome in the treatment group is higher.\n\n# group by treatment group and compute average outcome\ndf_out %>% \n  group_by(treatment) %>% \n  summarise(mean_outcome = mean(outcome))\n\n\n\n  \n\n\n\nGenerally, it is recommendable to use a linear regression to get an estimate of the treatment effect. You don’t have to manually compute the difference and additionally, the output of lm() and summary() yields information regarding its statistical inference. Then, we see that this effect is in fact highly statistically significant. The effect is equal to the difference of the two values just seen above. Check it out!\n\n# compute ATE with linear regression\nlm_ate <- lm(outcome ~ treatment, data = df_out)\nsummary(lm_ate)\n\n\nCall:\nlm(formula = outcome ~ treatment, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0245 -0.2046 -0.0005  0.2077  1.0459 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.000289   0.002682   -0.11     0.91    \ntreatment    1.116174   0.003796  294.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.38 on 39998 degrees of freedom\nMultiple R-squared:  0.684, Adjusted R-squared:  0.684 \nF-statistic: 8.65e+04 on 1 and 39998 DF,  p-value: <2e-16\n\n\nOne way to support your results could be a boxplot that on the one hand shows the difference of regressors by group and on the other hand the difference of outcomes. Here we will show the 95% confidence intervals for our estimates and it can be seen that there is a substantial difference between both groups. However, for our independent variables, age and sex , both groups are very similar.\n\n# plot independent and and depdent difference\ncompare_age <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = age, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Age\", title = \"Difference in age\")\ncompare_sex <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = sex, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Sex\", title = \"Difference in sex\")\ncompare_outcome <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = outcome, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n# plot age, sex and outcome differences for both groups\nggpubr::ggarrange(compare_age, compare_sex, compare_outcome, ncol = 3) \n\n\n\n\n\n\n\n\nBut why did we not include \\(age\\) and \\(sex\\) into our regression? Because they are similarly distributed across both groups it should not change the treatment effect. But still, they might have an impact on the outcome, as well. Although being similarly distributed in both groups, it does not mean that they still vary within each group. So let’s see what happens if we include them.\nBoth regressors turn out to be significant. However, as expected, the treatment effect is almost unchanged.\n\n# include other regressors\nlm_all <- lm(outcome ~ treatment + age + sex, data = df_out)\nsummary(lm_all)\n\n\nCall:\nlm(formula = outcome ~ treatment + age + sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6738 -0.2500  0.0019  0.2510  0.6913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.459642   0.004874   -94.3   <2e-16 ***\ntreatment    1.115773   0.002774   402.3   <2e-16 ***\nage          0.005052   0.000102    49.5   <2e-16 ***\nsex          0.499399   0.002774   180.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.28 on 39996 degrees of freedom\nMultiple R-squared:  0.831, Adjusted R-squared:  0.831 \nF-statistic: 6.56e+04 on 3 and 39996 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/05_rct.html#subgroup-analysis",
    "href": "content/toolbox/05_rct.html#subgroup-analysis",
    "title": "Randomized Controlled Trials",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nThe significance of \\(age\\) and \\(sex\\) could also indicate that there are different treatment effects across different levels of both covariates. Then, a so called interaction/moderation effect would be covered behind the statistical coefficients.\nA moderation effect expresses different strengths of the treatment for different subgroups. For example older women might benefit relatively more and younger males relatively less.\nIn R, we include interaction effects by using either using a product x1*x2 or a colon x1:x2.\n\n# include interaction\nlm_mod <- lm(outcome ~ treatment * age + treatment * sex, data = df_out)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = outcome ~ treatment * age + treatment * sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4436 -0.0666 -0.0003  0.0665  0.4769 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.35e-03   2.37e-03   -0.57     0.57    \ntreatment      2.02e-01   3.34e-03   60.49   <2e-16 ***\nage            2.02e-05   5.17e-05    0.39     0.70    \nsex            4.34e-04   1.40e-03    0.31     0.76    \ntreatment:age  9.97e-03   7.30e-05  136.49   <2e-16 ***\ntreatment:sex  9.99e-01   1.98e-03  503.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.099 on 39994 degrees of freedom\nMultiple R-squared:  0.978, Adjusted R-squared:  0.978 \nF-statistic: 3.62e+05 on 5 and 39994 DF,  p-value: <2e-16\n\n\nNote also that \\(R^2\\) has increased with each addition to the regression.\nAs the data is simulated, we can check what the data-generating process is and based on that evaluate what regression equation provides the best solution. Check for yourself what model should be used.\n\\[\noutcome = 0.2*treatment + 0.01*treatment*age + treatment*sex + \\epsilon\n\\]\nAgain, it shows how crucial theoretical knowledge of the phenomenon you are studying is. Imagine a situation with a high number of regressors. Testing out all potential variables as moderators requires some effort and might even lead to results just due to chance."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Content",
    "section": "",
    "text": "Welcome to “Causal Data Science for Business Analytics”!\nIn this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nAccording to a large global survey conducted among data science practitioners in industry in 2020, 83% of respondents consider causal inference increasingly important for data-driven decision making. Moreover, 44% state that, in their data science projects, causal inference already is important (https://www.causalscience.org/blog/causal-data-science-in-practice/)."
  },
  {
    "objectID": "content/index.html#mattermost",
    "href": "content/index.html#mattermost",
    "title": "Content",
    "section": "Mattermost",
    "text": "Mattermost\nIn the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn  most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you don’t find a solution and feel you are stuck, don’t hesitate to use our Mattermost channel. Follow these steps to join the channel:\nYou have to do the following steps to join our Mattermost channel:\n\n\n\nhttps://communicating.tuhh.de/\n\nClick \"Click here to sign in\"\n\nClick the Button GitLab\n\nYou may need to login to GitLab https://collaborating.tuhh.de/ with your Kerberos/LDAP data (e.g. abc1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\n\nAfter accessing Mattermost, join the team W-11 students via this link:\nSign-up for GitLab/Mattermost\nTo join W-11 students: https://communicating.tuhh.de/signup_user_complete/?id=8f44fef8tifj5rripq8zya4s7w\nJoin Channel !!! https://communicating.tuhh.de/w-11-students/channels/test_intro\n\nThere, and in the classes of course, I will try to help you as much as possible. But in order to keep it efficient and manageable it is necessary that we all follow some basic rules:\nIf you have any questions about the class content, coding problems and other challenges, please ask them first in the Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\n\n \n```r\nx %>% sum()\n```\n\n\n\n**Error:**\nError in x %>% sum() : could not find function \"%>%\"\n\n\n\n\nMinimalistic example of how you could post an error in Mattermost. See how little formatting makes the code very nice to read."
  },
  {
    "objectID": "content/index.html#installing-r-rstudio-ide",
    "href": "content/index.html#installing-r-rstudio-ide",
    "title": "Content",
    "section": "Installing R & RStudio IDE",
    "text": "Installing R & RStudio IDE\nBefore we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nhttps://rstudio-education.github.io/hopr/starting.html\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -> Global options -> Appearance and switch theme in -> Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab."
  },
  {
    "objectID": "content/index.html#introduction-to-r",
    "href": "content/index.html#introduction-to-r",
    "title": "Content",
    "section": "Introduction to R",
    "text": "Introduction to R\nOne last note before you start coding: don’t be intimidated by the errors, warnings and other messages that you (and everyone else) will without doubt receive.\nThere is no reason to panic just because you see red text in your console and in fact, what is returned will help you to reach your goal.\nThere are three different types of texts:\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\nMessages: these are just friendly texts that provide you with useful information.\n\nInteractive Tutorials:\nThe best way to get familiar with R and to code is to just start. \nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console (http://swirlstats.com/scn/title.html).\nFeel free to complete as many tutorials as you want, but for this class, the following tutorials are of particular use:\n\nThe R Programming Environment (Chapter 2-12)\nggplot (!!!) / https://meghan.rbind.io/slides/neair/neair.html#/ggplot2\n\nswirl()does not come with R by default but is an optional package. R packages are extensions of the base functionality that is implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nInstall the package (one). As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nLoad the package (always). By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later.\n\nswirl()\n\n!!! Assignment?\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")"
  },
  {
    "objectID": "content/fundamentals/02_reg.html",
    "href": "content/fundamentals/02_reg.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Statistical inference aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores and a lot of other business-related units.\nIn practice, it is often impossible to collect data about the whole population, which is why we draw (ideally random) samples from the whole population and use statistical inference to draw conclusions about the whole population using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#r",
    "href": "content/fundamentals/02_reg.html#r",
    "title": "Regression and Statistical Inference",
    "section": "R",
    "text": "R\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in R. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\nBefore we start, we load the R package tidyverse, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\nlibrary(tidyverse)\n\nWe create a tibble, which is a table containing data, with two columns, \\(x\\) and \\(y\\). For \\(x\\), we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable \\(y\\), we make dependent from \\(x\\), i.e. for each unit \\(i\\), we create a \\(y_i\\) as\n\\[\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n\\]\nwhere \\(\\epsilon_i\\) is random noise.\nLet’s have a look at how the data is presented. We have a table containing 100 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n# Simluate data\nn <- 10\nlm_dat <- tibble(\n  x = rnorm(n, mean = 3, sd = 1),\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\nlm_dat\n\n\n\n  \n\n\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between \\(X\\) and \\(Y\\). However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\nLet’s have a look at some random lines.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the error is zero.\n\n\n\n\n\n\n\n\n\n!!! should be same lines in both plots\nThe resulting line is highlighted in blue.\nIf we want to mathematically compute the line in R, we have to use the lm() function and provide data and the assumed functional relationship as arguments. lm() is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: lm_mod), which we can interpret best when using summary() to show the resulting coefficients and other statistical information.\n\n\n\n\n\n\nNote\n\n\n\nlm() is a function that fits a linear model. You have to provide data and a regression equation in the form of for example outcome ~ regressor_1 + regressor_2 or outcome ~ ., if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call summary().\n\n\nLooking at the regression summary, we see that the line is modeled by \\(y = -0.11797 + 0.3466*x\\). It means that for the fitted model, an increase of one unit in \\(x\\) is related to an 0.3466 increase in \\(y\\).\n\n# Fit model and print summary\nlm_mod <- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = y ~ x, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18597 -0.05210 -0.00783  0.06584  0.24115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1918     0.1300   -1.48     0.18    \nx             0.3354     0.0443    7.57  6.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 8 degrees of freedom\nMultiple R-squared:  0.878, Adjusted R-squared:  0.862 \nF-statistic: 57.3 on 1 and 8 DF,  p-value: 6.48e-05\n\n\nNow, let’s check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations \\(y_i\\) and predictions \\(\\hat{y_i}\\) for each \\(i\\) or plotting the residuals \\(r_i = y_i - \\hat{y_i}\\) and comparing it to the \\(x\\)-axis.\n\n# Add fitted values and residuals to data\nlm_dat_fit <- lm_dat %>% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")"
  },
  {
    "objectID": "content/fundamentals/02_reg.html#math",
    "href": "content/fundamentals/02_reg.html#math",
    "title": "Regression and Statistical Inference",
    "section": "Math",
    "text": "Math\nMathematically, the best line is found by the ordinary least squares (OLS) method.\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\nGiven \\(n\\) samples of observed pairs of dependent and independent variables \\(\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}\\), we plug any of them into the equation\n\\[\ny_i=\\beta_0+\\beta_1x_i+u_i\n\\]\nand together with our assumptions \\(E(u) = 0\\) and \\(E(u|x)=0\\) we obtain the equations to be solved to retrieve estimates for \\(\\beta_0\\) and \\(\\beta_1\\).\nFrom the independence of \\(x\\) and \\(u\\) and our understanding of probabilities and expectations, we also know that the expected value of the product of \\(x\\) and \\(u\\) has to be zero: \\(E(xu)=0\\). Substituting \\(u\\) with $y-\\beta_0-\\beta_1$, we obtain the two conditions that when being solved give us the optimal estimates for our \\(\\beta\\) parameters.\n\\[\nE(y-\\beta_0-\\beta_1x) =0 \\\\  \nE\\Big(x[y-\\beta_0-\\beta_1x]\\Big)=0\n\\]\nTranslated into its sample counterpart:\n\\[\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\]\nLooking at the sample equations, we know our sample size \\(n\\), our sampled values \\(y_i\\) and $x_i$. The coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\nSkipping a few transformation steps, we obtain\n\\[\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n\\]\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our \\(\\beta_1\\) is defined as the covariance of \\(X\\) and \\(Y\\) divided by the variance of $X$. It also shows that the variance of \\(X\\) has to be greater than zero, which means that not all values of \\(x_i\\) can be the same. You need to observe different values to be able to estimate how \\(y_i\\) reacts to \\(x_i\\).\n\\(\\beta_0\\) follows directly by plugging \\(\\beta_1\\) into \\(\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta}1\\overline{x}\\). A bar above a variable always represents the sample value of that particular variable. Thus,\\(\\beta_0\\) is (as expected) constant and linear in \\(\\beta_1\\).\nKnowing the equation for the regression line, we can compute fitted values \\(y_i\\) for all \\(i\\)\n\\[\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n\\]\nIn almost all cases however, \\(\\widehat{y}_i\\) won’t be equal to \\(y_i\\) but there will be a prediction error, commonly referred to as residual \\(\\widehat{u}_i\\). Make sure that you don’t mix it up with \\(u\\), the error term, which is always unobserved.\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\nInfo: Squares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n\\[\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n\\]\nAgain, most of the residuals won’t be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#interpretation",
    "href": "content/fundamentals/02_reg.html#interpretation",
    "title": "Regression and Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\nSo let’s run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. We will see in later chapters why that might be dangerous.\n\n# Include all potential regressors\nlm_all <- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n\n\nCall:\nlm(formula = expected_cost ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-383.8 -128.6  -25.1  106.3  934.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 456.77687   28.55378   16.00   <2e-16 ***\nregionE      -7.64241   16.30859   -0.47   0.6394    \nregionS     -20.49277   16.60455   -1.23   0.2174    \nregionW       5.73164   16.98095    0.34   0.7358    \nsex           6.03595   11.80362    0.51   0.6092    \nsmoking     185.59358   13.32027   13.93   <2e-16 ***\nage           9.25803    0.52034   17.79   <2e-16 ***\nincome       -0.05043    0.00385  -13.11   <2e-16 ***\nbmi          -2.07466    0.76463   -2.71   0.0068 ** \nchildren    -11.06608   14.59210   -0.76   0.4484    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 186 on 990 degrees of freedom\nMultiple R-squared:  0.395, Adjusted R-squared:  0.39 \nF-statistic: 71.9 on 9 and 990 DF,  p-value: <2e-16\n\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\nThe estimate is what we know as regression coefficient from before, typically denoted by \\(\\widehat{\\beta}_i\\) or other Greek letters. As it is the estimated version, it has a hat. The estimated for \\(\\beta_i\\) It tells you by how much the dependent variable \\(\\widehat{y}\\) varies when a particular variable is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect of \\(x_i\\) ceteris paribus, Latin for “the others equal”. It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as controlling for the other variables."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#statistical-significance",
    "href": "content/fundamentals/02_reg.html#statistical-significance",
    "title": "Regression and Statistical Inference",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate.\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error.\nMost widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data. The null hypothesis states that there is no correlation between the dependent variable \\(y\\) and the independent variable \\(x_1\\). The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis. Here, the alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable.\nStatistical significance can directly be derived from the p-value and an arbitrary significance level $\\alpha$. However, the most widely used level of \\(\\alpha\\) is \\(0.05\\). Less often used are levels of \\(.1\\), \\(.01\\) or \\(.001\\).\nAn estimate with a p-value less than \\(\\alpha\\) is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level \\(\\alpha\\). Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that \\(age\\), \\(income\\), \\(bmi\\), and \\(smoking\\) are statistically significant (at different levels though, indicated by the number of stars).\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A \\((1-\\alpha)\\) confidence interval has a probability of \\((1-\\alpha)*100 \\%\\) to contain the true value of our estimated coefficient. That means, if we would sample \\(100\\) times, \\(\\beta_i\\) would be contained in the sample \\((1-\\alpha)*100\\) times.\n\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n\n              2.5 %  97.5 %\n(Intercept) 400.744 512.810\nregionE     -39.646  24.361\nregionS     -53.077  12.091\nregionW     -27.591  39.054\nsex         -17.127  29.199\nsmoking     159.454 211.733\nage           8.237  10.279\nincome       -0.058  -0.043\nbmi          -3.575  -0.574\nchildren    -39.701  17.569\n\n\n\n# alpha = 0.9\nconfint(lm_all, level = 0.90)\n\n                5 %    95 %\n(Intercept) 409.766 503.788\nregionE     -34.493  19.208\nregionS     -47.830   6.845\nregionW     -22.226  33.689\nsex         -13.397  25.469\nsmoking     163.663 207.524\nage           8.401  10.115\nincome       -0.057  -0.044\nbmi          -3.334  -0.816\nchildren    -35.090  12.958\n\n\nAn estimate whose interval is either completely positive or completely negative is different from zero and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the positive coefficient."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#model-selection",
    "href": "content/fundamentals/02_reg.html#model-selection",
    "title": "Regression and Statistical Inference",
    "section": "Model selection",
    "text": "Model selection\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\nLooking at the spread of residuals (\\(\\widehat{u}_i = y_i - \\widehat{y}_i\\)) we want them to be spread evenly around zero.\n\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n\n\n\n\n\n\n\n\nWe can see that the residuals are in fact almost normally distributed.\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit is \\(R^2\\). It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\\[\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n\\]\nConveniently, \\(R^2\\) is always between 0 and 1 and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high \\(R^2\\) can even point to a biased model while a model with a low \\(R^2\\) can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of \\(R^2\\) does also not change the interpretation of the regression coefficients.\nA problem with \\(R^2\\) is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the \\(\\text{Adjusted} \\, R^2\\). It includes a term for the number of independent variables used.\n\\[\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n\\]\nwhere \\(n\\) is the sample size and \\(p\\) the number of independent variables. This way, you can compare models and account for their scarcity.\nLet’s build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n# Include only significant regressors\nlm_imp <- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + smoking, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-470.8 -138.5  -26.4  109.7  940.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  398.516     27.818    14.3   <2e-16 ***\nage            7.115      0.482    14.8   <2e-16 ***\nbmi           -1.982      0.825    -2.4    0.016 *  \nsmoking      183.495     14.403    12.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201 on 996 degrees of freedom\nMultiple R-squared:  0.287, Adjusted R-squared:  0.285 \nF-statistic:  134 on 3 and 996 DF,  p-value: <2e-16\n\n\nExcept for \\(bmi\\), coefficients are very similar. We’ll look into that in just a second. But first let us compare both models with regard to \\(\\text{(Adjusted)} \\, R^2\\).\nTo elegantly print variables in a specified format, you can use sprintf().\n\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.39\"\n\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.29\"\n\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike’s Information Criteria (AIC) and Bayesian Information Criteria (BIC). Both AIC and BIC penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\n\n[1] \"AIC: 13298.62\"\n\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n\n[1] \"AIC: 13451.17\"\n\n\nFor both criteria, the model with the lowest value is preferred.\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\nFor example, is it correct to assume a linear relationship between \\(bmi\\) and the outcome \\(expected\\text{_}cost\\)? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term “linear”, so what can we do about it?\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. \\(bmi^2\\).\n\n# Include quadratic term for BMI\nlm_sq <- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + I(bmi^2) + smoking, \n    data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-400.3  -82.1   -2.0   86.7  356.4 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1948.6027    41.6106    46.8   <2e-16 ***\nage            7.7799     0.2953    26.4   <2e-16 ***\nbmi         -121.7879     2.9779   -40.9   <2e-16 ***\nI(bmi^2)       2.0683     0.0507    40.8   <2e-16 ***\nsmoking      198.4633     8.8185    22.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 123 on 995 degrees of freedom\nMultiple R-squared:  0.734, Adjusted R-squared:  0.732 \nF-statistic:  685 on 4 and 995 DF,  p-value: <2e-16\n\n\nFrom the summary, we can see that including the square term significantly improves the model fit. We can take from it, that it is extremely important to rely on theoretical considerations when building models."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#assumptions",
    "href": "content/fundamentals/02_reg.html#assumptions",
    "title": "Regression and Statistical Inference",
    "section": "Assumptions",
    "text": "Assumptions\n\nLinearity: Relationship between \\(X\\) and \\(Y\\) is linear.\nHomoscedasticity: Variance of residual is the same for any value of \\(X\\).\nIndependence: Observations are independent of each other. Residuals are independent of each other.\nNormality: For any fixed value of \\(X\\), \\(Y\\) is normally distributed. Residuals of the model are normally distributed."
  }
]